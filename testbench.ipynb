{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 11:20:22.338908: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-21 11:20:22.939708: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-21 11:20:24.236586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to microsoft phi-2 code (put something like this in final piece of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Embedding dimensions and calculations (done)\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Token embedding with dropout.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        hidden_states = self.wte(input_ids)\n",
    "        return hidden_states\n",
    "\n",
    "# testing py\n",
    "py_layer = Embedding(vocab_size=3000, n_embd=768)\n",
    "input = torch.randint(0, 3000, (4,))\n",
    "# print(py_layer(input))\n",
    "# testing tf\n",
    "matrix = py_layer.wte._parameters['weight'].data.numpy()\n",
    "matrix = tf.convert_to_tensor(matrix)\n",
    "tf_layer = bert.Embedding(4, 768, matrix)\n",
    "# print(tf_layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb() (done)\n",
    "def MY_apply_rotary_emb(x, cos, sin): # code mostly taken from original\n",
    "    assert(len(x.shape) == 4)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = tf.split(x_rot, 2, axis=-1) # assumption that tf.split behaves the same as nn.chunk\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    x1, x2, c, s = [tf.cast(t, tf.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = tf.concat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1)\n",
    "    x_rot = tf.cast(x_rot, x.dtype) # might fail due to passing in x.dtype\n",
    "    return tf.concat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    ") -> torch.FloatTensor:\n",
    "    # print(\"x.shape:\", x.shape)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = x_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    x1, x2, c, s = [t.to(dtype=torch.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = torch.cat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1).to(x.dtype)\n",
    "    return torch.cat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "last_dim = 5 # it seems this is neccesary\n",
    "x = torch.randn([2,3,2,last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "# print(_apply_rotary_emb(x, cos, sin))\n",
    "# print(\"-----\")\n",
    "x = tf.constant(x.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "# print(MY_apply_rotary_emb(x, cos, sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_kv (done)\n",
    "def MY_apply_rotary_emb_kv(kv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(kv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    k1, k2, c, s = [tf.cast(t, tf.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, kv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        kv[:, :, 1:2, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_kv(\n",
    "    kv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k= None,\n",
    "    sin_k= None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    k1, k2, c, s = [t.to(dtype=torch.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(kv.dtype)\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            kv[:, :, 1:2, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "kv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "kv = tf.constant(kv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "\n",
    "# print(MY_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_qkv (done)\n",
    "\n",
    "def MY_apply_rotary_emb_qkv(qkv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(qkv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = tf.split(q_rot, 2, axis=-1)\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    q1, q2, k1, k2, c, s = [tf.cast(t, tf.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = tf.concat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1)\n",
    "    q_rot = tf.cast(q_rot, qkv.dtype)\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, qkv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([q_rot, q_pass], axis=-1), axis=2),\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        qkv[:, :, 2:3, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_qkv(\n",
    "    qkv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k=None,\n",
    "    sin_k=None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    q1, q2, k1, k2, c, s = [t.to(dtype=torch.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = torch.cat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).to(qkv.dtype)\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(qkv.dtype)\n",
    "    return torch.cat(\n",
    "        [   torch.cat([q_rot, q_pass], axis=-1).unsqueeze(2),\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            qkv[:, :, 2:3, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "# print(MY_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing rotary embedding layer (done)\n",
    "class MyRotaryEmbedding(tf.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base: float = None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        name=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        self.inv_freq = self._compute_inv_freq()\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        self.scale = (\n",
    "            (tf.range(0, dim, 2, dtype=tf.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None)\n",
    "        self._update_cos_sin_cache(max_position_embeddings, dtype=tf.float32)\n",
    "\n",
    "    def _compute_inv_freq(self):\n",
    "        return 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(self, seqlen:int, dtype):\n",
    "        self._seq_len_cached = seqlen\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = tf.range(seqlen, dtype=tf.float32)\n",
    "            if self.inv_freq.dtype != tf.float32:\n",
    "                inv_freq = self._compute_inv_freq()\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = tf.range(seqlen, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        # tensorflow does not appear to do what the top comment states\n",
    "        freqs = tf.einsum('i,j->ij', t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = tf.cast(tf.cos(freqs), dtype=dtype)\n",
    "            self._sin_cached = tf.cast(tf.sin(freqs), dtype=dtype)\n",
    "        else:\n",
    "            power = (tf.range(seqlen, dtype=self.scale.dtype) - seqlen // 2) / self.scale_base\n",
    "            scale = self.scale ** tf.expand_dims(power, axis=1)\n",
    "            # Force the scale multiplciation to happen in fp32\n",
    "            self._cos_cached = tf.cast((tf.cos(freqs) * scale), dtype=dtype)\n",
    "            self._sin_cached = tf.cast((tf.sin(freqs) * scale), dtype=dtype)\n",
    "            self._cos_k_cached = tf.cast((tf.cos(freqs) / scale), dtype=dtype)\n",
    "            self._sin_k_cached = tf.cast((tf.sin(freqs) / scale), dtype=dtype)\n",
    "    \n",
    "    def __call__(self, qkv, kv, seqlen_offset, **kwargs):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            # or (self.training and self._cos_cached.is_inference()) # look into this\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return MY_apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = MY_apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            kv = MY_apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            return q, kv\n",
    "    ################################\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Rotary positional embedding (RoPE).\n",
    "    Reference:\n",
    "        RoFormer: Enhanced Transformer with Rotary Position Embedding.\n",
    "        https://arxiv.org/pdf/2104.09864.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base= None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        device = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if scale_base is not None:\n",
    "            raise NotImplementedError\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.device = device\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        inv_freq = self._compute_inv_freq(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        scale = (\n",
    "            (torch.arange(0, dim, 2, device=device, dtype=torch.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None\n",
    "        )\n",
    "        self.register_buffer(\"scale\", scale, persistent=False)\n",
    "\n",
    "        # Initialize cached attributes since ONNX can't rely on dynamic initialization\n",
    "        self._update_cos_sin_cache(max_position_embeddings, device=device, dtype=torch.float32)\n",
    "\n",
    "    def _compute_inv_freq(self, device: Optional[str] = None) -> torch.FloatTensor:\n",
    "        return 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> None:\n",
    "        self._seq_len_cached = seqlen\n",
    "\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
    "            if self.inv_freq.dtype != torch.float32:\n",
    "                inv_freq = self._compute_inv_freq(device=device)\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = torch.cos(freqs).to(dtype)\n",
    "            self._sin_cached = torch.sin(freqs).to(dtype)\n",
    "        else:\n",
    "            power = (\n",
    "                torch.arange(seqlen, dtype=self.scale.dtype, device=self.scale.device) - seqlen // 2\n",
    "            ) / self.scale_base\n",
    "            scale = self.scale.to(device=power.device) ** rearrange(power, \"s -> s 1\")\n",
    "\n",
    "            # Force the scale multiplication to happen in fp32\n",
    "            self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
    "            self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
    "            self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
    "            self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.Tensor,\n",
    "        kv: Optional[torch.Tensor] = None,\n",
    "        seqlen_offset: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            or (self.training and self._cos_cached.is_inference())\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, device=qkv.device, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return _apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = _apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "            kv = _apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "\n",
    "            return q, kv\n",
    "\n",
    "\n",
    "dim = 5\n",
    "base = 5000\n",
    "scale_base = None\n",
    "pos_idx_in_fp32 = False\n",
    "max_position_embeddings = 2048\n",
    "device = 'cpu'\n",
    "\n",
    "rot_emb = RotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings, device)\n",
    "tf_rot_emb = MyRotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings)\n",
    "\n",
    "# test with kv being something, and being None\n",
    "\n",
    "# it appears that if kv will be none, then qkv must be 4d rather than 5d\n",
    "\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "kv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "seqlen_offset = 4\n",
    "print(rot_emb(qkv, kv, seqlen_offset))\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "kv = tf.constant(kv.numpy())\n",
    "print(tf_rot_emb(qkv, kv, seqlen_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Linear Layer\n",
    "layer = torch.nn.Linear(3, 5)\n",
    "input = torch.randn([7,3])\n",
    "print(layer(input))\n",
    "weights = tf.cast(torch.detach(layer.weight).numpy(), dtype=tf.float32)\n",
    "bias = tf.cast(torch.detach(layer.bias).numpy(), dtype=tf.float32)\n",
    "linear = bert.Dense(3, 5, weights, is_biased=True, bias=bias)\n",
    "input = tf.cast(input.numpy(), dtype=tf.float32)\n",
    "print(linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NewGELU(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "    def __call__(self, input):\n",
    "        return 0.5 * input * (1.0 + tf.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * tf.pow(input, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6362,  0.2685, -0.0438],\n",
      "        [ 1.4699, -2.7153, -1.5775]])\n",
      "tensor([[-0.0834,  0.1627, -0.0211],\n",
      "        [ 1.3656, -0.0085, -0.0907]])\n",
      "tf.Tensor(\n",
      "[[-0.08344983  0.16265765 -0.02111891]\n",
      " [ 1.3656254  -0.00851566 -0.09065148]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# MLP (need to fill in the weight, bias, and activation) (works)\n",
    "\n",
    "class MyMLP(tf.Module):\n",
    "    def __init__(self, n_inner:int, n_embd:int, name=None): # manually written in act-fn()\n",
    "        super().__init__(name)\n",
    "        self.fc1 = bert.Dense(n_embd, n_inner, weights=None, is_biased=True, bias=None)\n",
    "        self.fc2 = bert.Dense(n_inner, n_embd, weights=None, is_biased=True, bias=None)\n",
    "        self.act = NewGELU\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class NewGELUActivation(nn.Module): # grabbed from hugging face\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "    \n",
    "\n",
    "# input = torch.randn([2,3])\n",
    "# print(input)\n",
    "# layer = NewGELUActivation()\n",
    "# print(layer(input))\n",
    "# input = tf.constant(input.numpy())\n",
    "# layer = NewGELU()\n",
    "# print(layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.8216e-01,  1.6057e+00, -6.2888e-01,  ..., -1.1855e+00,\n",
      "            2.1962e+00,  1.5852e-01],\n",
      "          [ 6.8783e-02,  2.7639e+00, -5.0665e-01,  ..., -2.9511e-01,\n",
      "            7.0020e-02,  2.5522e-01],\n",
      "          [-1.3802e+00,  6.1853e-01,  1.2782e+00,  ..., -7.9474e-02,\n",
      "            1.7819e+00,  9.7527e-02],\n",
      "          [-6.9262e-01, -9.2186e-01, -1.0209e+00,  ..., -8.3433e-01,\n",
      "            8.6667e-02,  1.5283e-01],\n",
      "          [-1.1307e+00, -2.4051e-01,  4.5175e-01,  ..., -4.2752e-01,\n",
      "           -1.2928e+00, -8.6309e-01]],\n",
      "\n",
      "         [[ 7.7936e-02,  2.1183e-01,  3.2225e-02,  ..., -9.1052e-01,\n",
      "            2.5773e-01, -1.0369e+00],\n",
      "          [-1.1282e+00,  1.5443e+00,  2.6632e-01,  ...,  1.1326e-01,\n",
      "            4.3200e-01, -5.7752e-03],\n",
      "          [-1.3473e-01, -1.2908e-01,  3.9490e-01,  ..., -4.3477e-01,\n",
      "            1.3972e+00, -4.1296e-01],\n",
      "          [-8.6295e-01, -3.5388e-01, -8.1409e-01,  ..., -1.1982e+00,\n",
      "            1.8707e-01, -1.1732e-01],\n",
      "          [-4.5815e-01, -5.4771e-01, -4.0005e-01,  ...,  4.1405e-01,\n",
      "           -4.5936e-01, -9.2498e-01]],\n",
      "\n",
      "         [[ 4.1686e-01, -5.4064e-01,  4.8119e-01,  ..., -3.1856e-02,\n",
      "           -6.2636e-02, -7.6405e-01],\n",
      "          [-8.3524e-01,  1.2597e+00, -1.0027e-01,  ...,  1.2487e-01,\n",
      "            3.6230e-01, -2.2284e-01],\n",
      "          [ 1.3996e-01, -3.5245e-01,  7.5957e-01,  ..., -1.1909e-01,\n",
      "            7.3385e-01, -6.9036e-01],\n",
      "          [-4.0465e-01, -1.5300e-01, -5.1265e-01,  ..., -1.0329e+00,\n",
      "           -2.0260e-01,  8.8103e-02],\n",
      "          [ 4.0559e-02, -3.7698e-01, -3.7480e-01,  ...,  2.6058e-01,\n",
      "           -2.1136e-01, -6.2799e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4646e-02, -2.0919e-01, -1.4023e-01,  ...,  7.7140e-02,\n",
      "            4.4713e-03, -7.7709e-02],\n",
      "          [-2.4676e-01,  3.1811e-01,  3.0791e-02,  ...,  2.7265e-01,\n",
      "            1.5869e-01, -1.2609e-01],\n",
      "          [-2.8203e-01,  2.8043e-01, -2.4265e-01,  ...,  1.2657e-01,\n",
      "           -1.3164e-01, -1.2474e-01],\n",
      "          [-1.2543e-01, -1.8962e-01, -1.6414e-01,  ..., -2.0353e-01,\n",
      "           -3.6428e-02,  2.0748e-01],\n",
      "          [-4.1378e-02, -1.1645e-01,  4.9355e-01,  ...,  2.2217e-01,\n",
      "           -3.3710e-01,  1.2862e-01]],\n",
      "\n",
      "         [[ 1.1366e-01, -1.5610e-01, -2.4029e-01,  ...,  4.6020e-02,\n",
      "           -2.2661e-01, -7.2291e-02],\n",
      "          [-2.2354e-01,  8.6170e-02,  7.1894e-02,  ...,  2.2472e-01,\n",
      "            1.7734e-01, -7.4191e-02],\n",
      "          [-2.0202e-01,  3.1298e-01, -3.1887e-01,  ...,  1.1084e-01,\n",
      "           -1.7858e-01, -4.4099e-02],\n",
      "          [-1.0358e-03, -1.2790e-01, -3.5820e-01,  ..., -2.0297e-01,\n",
      "            1.6216e-01,  1.0694e-01],\n",
      "          [-9.9316e-02, -3.2097e-01,  4.4469e-01,  ...,  2.4768e-01,\n",
      "           -1.8079e-01, -7.6504e-02]],\n",
      "\n",
      "         [[ 8.2411e-02, -3.0136e-02, -1.4922e-01,  ..., -5.1191e-02,\n",
      "            6.1926e-02,  3.9442e-02],\n",
      "          [-1.0226e-01,  2.7475e-01,  4.6859e-02,  ...,  2.1153e-01,\n",
      "            1.8160e-01, -1.4450e-01],\n",
      "          [-1.3043e-01,  2.0906e-01, -2.6470e-01,  ...,  4.3357e-01,\n",
      "           -8.5150e-03, -1.2057e-01],\n",
      "          [-1.0239e-01, -1.8741e-01, -1.8620e-01,  ..., -5.9952e-02,\n",
      "           -5.9387e-03,  4.3021e-01],\n",
      "          [-2.2533e-01, -2.3874e-01,  4.7090e-01,  ...,  1.9306e-01,\n",
      "           -7.6711e-02,  5.6832e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.6600e-01,  9.2051e-01, -4.0277e-01,  ...,  6.3090e-01,\n",
      "           -4.4263e-01,  8.1526e-01],\n",
      "          [-6.3766e-01, -1.4028e-01, -2.0426e+00,  ..., -2.1507e+00,\n",
      "           -8.3354e-01,  7.6734e-01],\n",
      "          [-1.1925e-01, -5.7758e-01,  4.2481e-01,  ..., -1.0113e-01,\n",
      "            5.0606e-01,  4.7923e-01],\n",
      "          [-1.5265e+00, -1.5286e+00,  3.6396e-01,  ..., -5.9096e-01,\n",
      "            3.5486e-01,  5.3277e-01],\n",
      "          [ 2.5297e-01,  2.2987e-01,  6.4070e-01,  ..., -1.3879e+00,\n",
      "           -5.1640e-01, -4.3161e-01]],\n",
      "\n",
      "         [[ 8.3382e-04,  6.6573e-02, -6.3007e-02,  ...,  6.1394e-01,\n",
      "            3.2245e-01,  5.0547e-02],\n",
      "          [-9.3102e-01, -6.5555e-01, -1.6676e+00,  ..., -2.5589e-01,\n",
      "           -1.8374e-01,  7.9091e-01],\n",
      "          [ 6.2144e-01,  8.3345e-01,  6.2808e-01,  ..., -2.3245e-01,\n",
      "            7.4983e-01, -7.9347e-01],\n",
      "          [-4.7097e-01, -1.0472e+00, -2.4359e-01,  ..., -7.5793e-01,\n",
      "            5.7529e-01, -2.7169e-01],\n",
      "          [-3.5240e-02,  7.6388e-01, -3.3105e-01,  ..., -1.4326e+00,\n",
      "            1.0243e-01,  9.1301e-01]],\n",
      "\n",
      "         [[-3.5976e-01,  5.1019e-01, -3.6561e-01,  ..., -6.9160e-02,\n",
      "            3.0066e-01,  2.5789e-01],\n",
      "          [ 1.0371e-02, -4.7654e-01, -1.2557e+00,  ..., -7.4367e-01,\n",
      "           -2.3655e-01,  5.7487e-01],\n",
      "          [ 4.9196e-01,  4.1839e-01,  9.5054e-01,  ...,  1.5913e-01,\n",
      "            7.1454e-01, -8.6196e-01],\n",
      "          [-7.0668e-01, -1.9753e+00, -1.7072e-01,  ...,  5.5552e-01,\n",
      "           -7.4892e-01, -9.7994e-01],\n",
      "          [ 6.5630e-02,  6.6063e-01, -9.7698e-03,  ..., -1.1488e+00,\n",
      "            2.1958e-02,  5.6317e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2860e-01, -1.8779e-01,  2.7717e-01,  ..., -6.6412e-02,\n",
      "            8.7920e-03,  3.0038e-01],\n",
      "          [ 3.7116e-01, -3.8064e-01, -4.3904e-01,  ..., -2.1160e-01,\n",
      "           -2.1408e-02,  2.5225e-01],\n",
      "          [-2.3969e-02,  9.3453e-02,  1.0864e-01,  ..., -2.4138e-01,\n",
      "           -3.8912e-03, -3.6429e-01],\n",
      "          [-3.6842e-01, -5.8845e-01, -2.0110e-01,  ...,  4.4735e-01,\n",
      "            3.8401e-02,  2.6627e-02],\n",
      "          [-3.7773e-02, -1.2670e-01, -5.9774e-02,  ..., -1.7744e-01,\n",
      "            5.3984e-02,  1.0864e-01]],\n",
      "\n",
      "         [[-1.9433e-01,  1.7482e-01,  1.1333e-01,  ..., -1.3529e-01,\n",
      "           -2.1771e-01,  1.0886e-02],\n",
      "          [ 1.5891e-02, -4.5771e-01, -4.5839e-01,  ..., -1.4533e-01,\n",
      "           -2.1022e-02,  1.4793e-01],\n",
      "          [ 9.6486e-03,  1.1382e-01,  1.7254e-01,  ..., -1.5093e-01,\n",
      "           -8.9746e-02, -4.1439e-01],\n",
      "          [-2.5848e-01, -1.9158e-01,  6.9545e-02,  ...,  3.4010e-01,\n",
      "            2.1598e-01,  1.8161e-01],\n",
      "          [-1.7452e-02, -3.9360e-01, -7.2023e-02,  ..., -1.8911e-01,\n",
      "           -1.2654e-02,  1.8016e-01]],\n",
      "\n",
      "         [[-1.5459e-01,  2.6848e-01,  1.7869e-01,  ...,  1.3353e-03,\n",
      "           -2.9532e-01, -4.9399e-02],\n",
      "          [ 9.8224e-03, -4.2818e-01, -4.9857e-01,  ..., -3.5546e-01,\n",
      "           -1.4496e-01,  1.9284e-01],\n",
      "          [ 1.8336e-01,  1.0267e-01,  2.1993e-03,  ..., -1.1771e-01,\n",
      "            3.1664e-02, -5.4467e-01],\n",
      "          [-2.0495e-01, -2.4949e-01,  9.4966e-02,  ...,  3.3030e-01,\n",
      "            2.6559e-01, -2.1512e-02],\n",
      "          [ 6.9348e-02, -1.1564e-01,  4.1997e-02,  ..., -4.0886e-02,\n",
      "            3.0608e-02,  1.7852e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.5724e-02, -1.2945e+00,  8.6361e-01,  ...,  9.1644e-02,\n",
      "            2.8711e+00, -2.5095e-02],\n",
      "          [-1.3211e-01,  1.7794e-01,  1.4658e+00,  ..., -4.0553e-02,\n",
      "           -7.5262e-02, -3.3705e-01],\n",
      "          [ 5.1771e-03, -2.7034e+00, -1.8376e+00,  ...,  7.6552e-01,\n",
      "            8.4822e-01, -5.3956e-02],\n",
      "          [ 2.0885e+00, -2.3657e-01, -8.3022e-02,  ..., -6.0953e-01,\n",
      "            7.2785e-01, -1.8974e-01],\n",
      "          [-1.4963e+00, -2.0750e-01, -3.8505e-02,  ..., -6.4031e-01,\n",
      "            1.2860e-01, -5.7447e-01]],\n",
      "\n",
      "         [[ 6.7724e-01, -5.4907e-01,  1.4666e+00,  ..., -1.3273e-01,\n",
      "            1.7143e+00,  1.7205e-01],\n",
      "          [-3.9260e-01,  2.5710e-01,  4.4109e-01,  ..., -2.7458e-01,\n",
      "           -6.4023e-02,  2.9685e-01],\n",
      "          [ 1.8050e-01, -1.4323e+00, -8.3005e-01,  ..., -2.7961e-01,\n",
      "            6.5341e-01,  3.6454e-01],\n",
      "          [ 9.1493e-01, -3.8277e-01,  9.1325e-01,  ...,  2.6915e-01,\n",
      "            1.4916e-01,  1.1827e-01],\n",
      "          [ 1.3044e-01,  2.9521e-01,  3.2569e-01,  ..., -3.9460e-01,\n",
      "           -1.1563e-02,  3.2093e-01]],\n",
      "\n",
      "         [[ 4.9356e-01, -1.3805e-01,  3.9500e-01,  ...,  1.1286e-01,\n",
      "            5.4080e-01,  5.1408e-01],\n",
      "          [ 6.4685e-02, -1.2281e-01, -9.5627e-01,  ..., -3.3165e-01,\n",
      "            6.0546e-01,  5.4414e-01],\n",
      "          [ 2.2957e-01, -1.6039e+00, -5.7585e-01,  ...,  1.3014e-01,\n",
      "            9.3978e-01, -8.0358e-03],\n",
      "          [ 1.1206e+00, -1.0202e-01,  6.0609e-01,  ..., -4.5870e-01,\n",
      "            4.3620e-01,  1.7258e-01],\n",
      "          [ 3.1562e-01,  3.8840e-01, -3.8253e-02,  ..., -3.6192e-01,\n",
      "           -3.1030e-01,  8.7615e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2569e-01, -8.4901e-02,  5.6769e-01,  ..., -9.1985e-04,\n",
      "           -1.5214e-01, -2.3485e-01],\n",
      "          [-9.9095e-04,  2.4087e-01, -4.4110e-01,  ..., -1.3438e-01,\n",
      "           -1.7074e-01, -3.3593e-01],\n",
      "          [-2.8840e-01,  9.4787e-02, -2.3298e-01,  ...,  3.9469e-01,\n",
      "           -2.5860e-02, -3.5304e-02],\n",
      "          [-9.8659e-03,  1.8463e-01,  1.8453e-01,  ...,  3.4216e-01,\n",
      "            1.2937e-01, -6.1024e-02],\n",
      "          [-8.6418e-02, -6.0271e-02,  2.4951e-01,  ...,  2.7976e-03,\n",
      "           -1.0134e-01,  2.3105e-02]],\n",
      "\n",
      "         [[ 3.6507e-02, -2.0860e-01,  3.2317e-01,  ...,  1.8766e-01,\n",
      "           -1.3263e-02, -1.1494e-01],\n",
      "          [ 4.3341e-02, -1.0555e-01, -5.3624e-02,  ..., -8.1893e-02,\n",
      "           -4.7216e-02, -2.3647e-01],\n",
      "          [-2.8705e-01,  1.2313e-02, -2.4387e-01,  ...,  5.1603e-01,\n",
      "           -9.5293e-03, -3.6045e-02],\n",
      "          [-8.4131e-02,  1.2284e-02,  1.8940e-01,  ...,  4.2112e-01,\n",
      "            1.2885e-01,  4.4160e-02],\n",
      "          [-5.3813e-02, -4.3501e-02,  1.2989e-01,  ..., -2.7508e-02,\n",
      "            1.1383e-01,  1.0971e-01]],\n",
      "\n",
      "         [[-2.5180e-04, -2.4862e-01,  2.8890e-01,  ...,  7.9404e-02,\n",
      "            2.4189e-01, -1.8974e-01],\n",
      "          [ 1.1704e-01, -1.9650e-01, -1.9588e-01,  ..., -1.5798e-01,\n",
      "           -6.0128e-02, -1.5734e-01],\n",
      "          [-1.8424e-01,  5.0470e-02, -2.3466e-01,  ...,  4.9542e-01,\n",
      "            1.4594e-02, -1.8738e-01],\n",
      "          [-2.8701e-02,  5.4238e-02,  1.5622e-01,  ...,  1.3913e-01,\n",
      "            1.5660e-01,  6.3200e-02],\n",
      "          [-8.0334e-02, -1.8720e-01,  1.9968e-01,  ...,  4.0353e-02,\n",
      "            8.0796e-02,  5.1745e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.2711e-01,  7.2359e-01, -5.6510e-01,  ...,  1.2208e+00,\n",
      "           -4.1105e-01, -1.1220e+00],\n",
      "          [ 1.1889e+00,  3.7586e-01,  2.5732e-01,  ..., -7.5491e-01,\n",
      "            3.3806e-01, -9.0770e-01],\n",
      "          [-2.6403e-01,  1.0294e+00, -6.6291e-01,  ..., -1.5111e+00,\n",
      "           -1.4690e+00,  3.6196e-01],\n",
      "          [-1.2437e+00,  1.6932e+00, -8.9748e-01,  ...,  4.2104e-03,\n",
      "            3.6746e-01,  1.3104e+00],\n",
      "          [-1.1449e+00, -2.2652e-02, -1.9021e+00,  ...,  8.7119e-01,\n",
      "            7.1476e-01,  9.2662e-01]],\n",
      "\n",
      "         [[ 4.6966e-01,  1.8898e-01, -2.9314e-01,  ...,  9.7604e-03,\n",
      "            1.1269e-01, -9.2136e-01],\n",
      "          [ 8.1802e-01, -4.9038e-01,  8.6433e-01,  ..., -3.4849e-01,\n",
      "           -4.7615e-01, -1.0906e+00],\n",
      "          [ 5.0943e-01, -3.2042e-03, -2.9300e-01,  ..., -1.2387e+00,\n",
      "           -7.5821e-01,  8.0490e-01],\n",
      "          [-9.1185e-01,  5.0056e-02,  1.4378e-01,  ...,  2.6305e-01,\n",
      "            4.2462e-02,  6.1705e-01],\n",
      "          [-7.9661e-01,  4.6553e-01, -9.5784e-01,  ..., -3.0127e-01,\n",
      "           -1.0690e-02,  7.4775e-01]],\n",
      "\n",
      "         [[-1.4796e-01,  4.4345e-01,  1.4046e-01,  ..., -7.6609e-01,\n",
      "            5.9562e-01, -3.4674e-01],\n",
      "          [ 9.5414e-01, -1.2344e-01, -9.0558e-02,  ..., -4.9494e-01,\n",
      "            6.4845e-02, -1.0694e+00],\n",
      "          [ 2.9430e-01,  1.4026e-01, -3.1359e-01,  ..., -1.0117e+00,\n",
      "           -7.8180e-01,  6.4743e-01],\n",
      "          [ 7.9480e-02, -4.1673e-01, -4.1708e-01,  ...,  5.7452e-01,\n",
      "            5.3644e-02,  5.4635e-02],\n",
      "          [-1.1605e+00,  9.1152e-02, -1.7084e+00,  ...,  4.2222e-01,\n",
      "           -3.5378e-01,  4.2385e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.6634e-01, -3.4680e-02,  2.0854e-01,  ...,  3.2697e-02,\n",
      "           -2.2340e-01, -3.8740e-01],\n",
      "          [ 3.8478e-01, -5.4259e-02,  5.5667e-02,  ..., -2.4532e-01,\n",
      "           -4.8466e-01, -2.6708e-01],\n",
      "          [ 1.8896e-01,  9.3984e-02, -1.1252e-02,  ...,  1.4116e-01,\n",
      "           -1.3960e-01, -1.5992e-01],\n",
      "          [ 5.3587e-01, -2.0083e-01,  3.0864e-02,  ..., -1.3036e-01,\n",
      "           -5.3390e-01, -3.0895e-02],\n",
      "          [-2.1157e-01, -4.3585e-01, -3.3968e-01,  ...,  1.9013e-02,\n",
      "           -2.2579e-01, -4.2073e-01]],\n",
      "\n",
      "         [[ 2.8524e-01,  7.0859e-02,  3.2859e-01,  ..., -6.4516e-02,\n",
      "           -2.2590e-01, -4.0807e-01],\n",
      "          [ 6.2715e-01,  8.8069e-03,  2.6432e-02,  ..., -1.7917e-01,\n",
      "           -3.8568e-01, -4.4484e-01],\n",
      "          [ 2.1298e-01,  2.0770e-01,  2.3959e-01,  ...,  5.0049e-01,\n",
      "           -2.6848e-01, -9.9671e-02],\n",
      "          [ 1.5362e-01, -2.2039e-01,  1.4201e-01,  ..., -1.4864e-01,\n",
      "           -4.9730e-01, -8.6626e-03],\n",
      "          [-2.8824e-01, -2.4473e-01, -1.5807e-01,  ..., -2.8457e-04,\n",
      "           -1.0686e-01, -4.3499e-01]],\n",
      "\n",
      "         [[ 2.9028e-01,  4.9697e-02,  3.9528e-01,  ..., -2.3155e-02,\n",
      "           -2.2388e-01, -2.7919e-01],\n",
      "          [ 3.6743e-01,  1.1808e-02,  1.7892e-01,  ..., -6.6086e-02,\n",
      "           -3.5791e-01, -3.5393e-01],\n",
      "          [ 2.1450e-01,  2.6509e-01,  2.1613e-01,  ...,  5.1947e-01,\n",
      "           -1.1866e-01,  4.4194e-02],\n",
      "          [ 2.2652e-01, -3.0458e-01,  4.7772e-02,  ..., -1.4004e-01,\n",
      "           -4.8082e-01, -3.1001e-02],\n",
      "          [-2.0368e-01, -1.3639e-01, -7.5001e-02,  ..., -1.3381e-03,\n",
      "           -1.7311e-01, -3.5438e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1884e-01, -1.1718e+00,  1.1303e+00,  ..., -8.2783e-01,\n",
      "            8.8881e-02, -1.0051e-01],\n",
      "          [ 1.4593e+00, -9.0533e-01,  1.1358e+00,  ..., -5.6290e-01,\n",
      "           -1.5238e-01, -8.1084e-01],\n",
      "          [ 4.6010e-01,  1.0338e+00, -7.5926e-01,  ...,  1.2091e+00,\n",
      "           -5.2657e-01, -2.1298e-01],\n",
      "          [ 1.0358e+00, -2.5602e-01, -3.5331e+00,  ..., -1.6490e+00,\n",
      "            7.5023e-01, -7.1971e-01],\n",
      "          [-4.3261e-01, -3.7738e-01, -1.1829e+00,  ...,  3.7194e-01,\n",
      "           -3.6474e-01, -1.8771e-01]],\n",
      "\n",
      "         [[ 2.2615e-01, -6.7981e-01,  1.2010e+00,  ..., -5.7451e-01,\n",
      "           -2.1694e-01, -1.6300e-01],\n",
      "          [ 8.4920e-01,  3.1301e-02,  1.4800e+00,  ..., -9.0135e-01,\n",
      "            4.3971e-01,  7.1035e-01],\n",
      "          [ 6.3617e-02,  4.8565e-01, -1.6771e-01,  ...,  7.8035e-01,\n",
      "           -5.8337e-01,  9.6485e-02],\n",
      "          [ 8.7044e-01, -1.5341e-01, -1.7804e+00,  ..., -1.5856e+00,\n",
      "            7.5610e-01, -4.9366e-01],\n",
      "          [ 5.2373e-02, -8.8832e-01, -2.1201e-03,  ...,  2.3746e-01,\n",
      "           -3.6368e-01,  3.7533e-01]],\n",
      "\n",
      "         [[ 4.4738e-01, -5.8680e-01,  9.8697e-01,  ..., -8.1961e-01,\n",
      "           -4.4876e-01, -3.0678e-01],\n",
      "          [ 6.9936e-01, -4.5302e-01,  1.2483e+00,  ..., -4.0597e-01,\n",
      "            6.3623e-01,  2.4710e-01],\n",
      "          [ 6.5903e-02,  5.1600e-01, -1.0491e-01,  ...,  8.7050e-01,\n",
      "           -3.5270e-01, -1.1553e-01],\n",
      "          [ 6.9571e-01, -2.7009e-02, -1.2553e+00,  ..., -1.3776e+00,\n",
      "            4.3052e-01, -3.6877e-01],\n",
      "          [-3.2602e-01, -7.6656e-01,  2.0005e-01,  ..., -4.9216e-02,\n",
      "           -2.3510e-01,  3.2903e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1103e-02, -3.2604e-01,  2.3747e-01,  ..., -1.3807e-01,\n",
      "           -1.9141e-01,  3.0538e-01],\n",
      "          [ 3.0367e-01, -1.2013e-01,  6.1322e-01,  ..., -5.5948e-02,\n",
      "            3.1647e-01,  3.3046e-01],\n",
      "          [-3.3199e-01,  1.1989e-01,  1.9928e-01,  ...,  1.4130e-01,\n",
      "           -5.1825e-01,  2.4661e-02],\n",
      "          [ 5.1557e-01,  4.2635e-02, -4.9138e-01,  ...,  9.1064e-02,\n",
      "            6.7654e-01, -3.2002e-01],\n",
      "          [ 5.2452e-02,  1.1125e-02, -3.0161e-01,  ..., -2.0924e-01,\n",
      "           -6.3929e-01,  1.2128e-01]],\n",
      "\n",
      "         [[-1.2707e-01, -4.5576e-01,  4.1662e-01,  ..., -1.5015e-01,\n",
      "           -3.8785e-01,  4.8560e-02],\n",
      "          [ 1.4752e-01,  5.4482e-02,  6.2810e-01,  ..., -6.0888e-02,\n",
      "            1.9156e-01,  1.6036e-01],\n",
      "          [-2.2170e-01,  1.7270e-01,  1.6037e-01,  ...,  8.5922e-02,\n",
      "           -4.9633e-01,  4.8448e-03],\n",
      "          [ 4.0065e-01,  5.4240e-02, -4.3637e-01,  ...,  1.2327e-01,\n",
      "            7.9270e-01, -1.4744e-01],\n",
      "          [-6.2102e-02, -1.6066e-01, -2.2006e-01,  ..., -2.5543e-01,\n",
      "           -5.4349e-01, -1.1033e-01]],\n",
      "\n",
      "         [[-2.5074e-02, -2.1628e-01,  2.8771e-01,  ...,  8.7222e-03,\n",
      "           -3.8360e-02,  2.6234e-01],\n",
      "          [ 3.0732e-01, -4.6311e-02,  4.9792e-01,  ..., -7.0870e-02,\n",
      "            1.6987e-01,  1.6200e-01],\n",
      "          [-3.2164e-01,  1.2678e-01,  2.0556e-01,  ...,  1.0823e-01,\n",
      "           -3.4767e-01,  1.2041e-01],\n",
      "          [ 3.8736e-01,  1.4476e-01, -4.4522e-01,  ...,  2.3229e-02,\n",
      "            6.7660e-01, -2.2735e-01],\n",
      "          [-1.6960e-01, -7.0288e-02, -1.6544e-01,  ..., -1.3098e-01,\n",
      "           -5.7081e-01, -1.5384e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.6468e-02, -1.0442e+00, -7.0903e-01,  ..., -3.7215e-01,\n",
      "            1.4889e+00, -8.0929e-01],\n",
      "          [ 1.1558e+00,  1.6939e-01, -9.1814e-01,  ..., -1.7845e-01,\n",
      "            1.5922e-01,  8.6887e-01],\n",
      "          [ 2.4648e+00, -7.7476e-01, -1.0742e+00,  ..., -9.2686e-01,\n",
      "            1.5699e+00,  3.0691e-01],\n",
      "          [ 6.3623e-02,  5.9391e-01,  1.3064e+00,  ...,  7.7004e-01,\n",
      "            1.2136e-01,  1.8287e-01],\n",
      "          [-3.4343e-01,  1.1026e+00, -6.5702e-01,  ..., -1.3039e+00,\n",
      "            3.7279e-01,  1.1243e+00]],\n",
      "\n",
      "         [[ 1.9244e-01, -3.0307e-01, -5.2796e-01,  ..., -3.4183e-01,\n",
      "            1.1147e+00, -3.8455e-01],\n",
      "          [ 5.8968e-01,  1.3128e-01, -7.9340e-01,  ...,  1.8000e-01,\n",
      "            6.3860e-01,  9.6073e-01],\n",
      "          [ 2.0731e+00, -2.0806e-01, -1.2750e+00,  ..., -4.0335e-01,\n",
      "            1.4787e+00,  1.2421e+00],\n",
      "          [-4.1455e-01,  2.7595e-01,  8.3894e-01,  ...,  7.3333e-01,\n",
      "           -8.7573e-02, -9.4780e-02],\n",
      "          [-1.1332e+00,  3.1599e-01,  1.7387e-01,  ..., -1.0417e+00,\n",
      "            1.2354e+00,  1.2155e+00]],\n",
      "\n",
      "         [[ 7.2338e-01, -1.1438e-02, -1.0579e-01,  ..., -3.2991e-01,\n",
      "            6.4861e-01,  2.8388e-02],\n",
      "          [ 2.6506e-01,  1.8068e-01, -5.8356e-01,  ...,  9.8829e-01,\n",
      "           -2.2653e-02,  7.1690e-01],\n",
      "          [ 1.0444e+00, -2.1409e-01, -1.1339e+00,  ..., -6.4090e-02,\n",
      "            1.1988e+00,  1.3982e+00],\n",
      "          [-5.1291e-01, -6.9788e-02,  5.1204e-01,  ...,  5.2444e-01,\n",
      "            2.8732e-01,  3.7417e-01],\n",
      "          [-6.2329e-01,  3.8729e-01,  2.2118e-01,  ..., -4.6720e-01,\n",
      "            8.4812e-01,  1.2845e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8738e-01,  1.6224e-02, -3.2298e-01,  ..., -2.1295e-01,\n",
      "            2.6854e-01,  2.8616e-01],\n",
      "          [ 8.8870e-02,  1.5629e-01, -1.4358e-01,  ...,  1.8226e-02,\n",
      "            4.1500e-01, -1.4429e-01],\n",
      "          [ 2.8965e-01, -2.3981e-01, -1.2353e-01,  ...,  6.7949e-02,\n",
      "            2.8854e-01,  6.0755e-01],\n",
      "          [-3.0364e-01, -2.0944e-01, -1.5232e-01,  ...,  5.0342e-02,\n",
      "           -1.0660e-01, -1.1187e-03],\n",
      "          [ 3.7910e-01, -7.6706e-02,  2.8968e-01,  ..., -1.4103e-01,\n",
      "            2.0144e-01, -3.4687e-01]],\n",
      "\n",
      "         [[ 2.7797e-01,  8.1699e-02, -4.0255e-01,  ..., -2.1225e-01,\n",
      "            1.6773e-01,  1.9171e-01],\n",
      "          [ 3.0603e-01,  1.4314e-01, -8.9078e-02,  ...,  2.2122e-02,\n",
      "            3.7274e-01, -1.0481e-01],\n",
      "          [ 7.8185e-02, -2.9302e-01, -2.0020e-01,  ...,  1.2234e-01,\n",
      "            4.1864e-01,  7.6558e-01],\n",
      "          [-3.6304e-01, -1.3401e-01, -4.1733e-02,  ...,  7.9610e-02,\n",
      "           -2.6727e-01, -2.5017e-01],\n",
      "          [ 3.4522e-01, -1.3801e-02,  2.6792e-01,  ..., -1.4812e-01,\n",
      "            2.4303e-01, -2.9908e-01]],\n",
      "\n",
      "         [[ 3.2873e-01, -1.3515e-02, -2.2135e-01,  ..., -1.8585e-01,\n",
      "            1.9386e-01,  2.4522e-01],\n",
      "          [ 5.0686e-01,  2.2542e-01, -1.0469e-01,  ...,  2.5099e-01,\n",
      "            2.8187e-01, -1.1294e-01],\n",
      "          [ 2.2283e-01,  4.1376e-02, -2.3943e-01,  ...,  1.7779e-01,\n",
      "            4.6983e-01,  6.2398e-01],\n",
      "          [-3.0295e-01,  1.6057e-01,  2.8309e-02,  ..., -9.2126e-02,\n",
      "           -3.8124e-02,  3.9804e-02],\n",
      "          [ 7.1766e-02, -8.9766e-02,  2.5065e-01,  ..., -1.7973e-01,\n",
      "            3.1177e-01, -2.9066e-01]]]])\n",
      "-----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 17, 5, 11), dtype=float32, numpy=\n",
       "array([[[[ 7.82163501e-01,  1.60567415e+00, -6.28879726e-01, ...,\n",
       "          -1.18546379e+00,  2.19618988e+00,  1.58521876e-01],\n",
       "         [ 6.87834620e-02,  2.76394773e+00, -5.06650627e-01, ...,\n",
       "          -2.95114070e-01,  7.00201616e-02,  2.55223244e-01],\n",
       "         [-1.38016903e+00,  6.18527055e-01,  1.27824712e+00, ...,\n",
       "          -7.94743299e-02,  1.78190470e+00,  9.75266472e-02],\n",
       "         [-6.92622185e-01, -9.21856761e-01, -1.02088904e+00, ...,\n",
       "          -8.34326744e-01,  8.66672173e-02,  1.52826503e-01],\n",
       "         [-1.13069391e+00, -2.40513384e-01,  4.51752931e-01, ...,\n",
       "          -4.27516937e-01, -1.29279351e+00, -8.63091290e-01]],\n",
       "\n",
       "        [[ 7.79363215e-02,  2.11826324e-01,  3.22249830e-02, ...,\n",
       "          -9.10517752e-01,  2.57727563e-01, -1.03686702e+00],\n",
       "         [-1.12815976e+00,  1.54428232e+00,  2.66317725e-01, ...,\n",
       "           1.13263369e-01,  4.31996763e-01, -5.77515364e-03],\n",
       "         [-1.34734511e-01, -1.29076302e-01,  3.94897610e-01, ...,\n",
       "          -4.34773356e-01,  1.39722228e+00, -4.12957966e-01],\n",
       "         [-8.62949610e-01, -3.53879780e-01, -8.14093649e-01, ...,\n",
       "          -1.19815707e+00,  1.87068880e-01, -1.17323004e-01],\n",
       "         [-4.58150238e-01, -5.47708154e-01, -4.00053173e-01, ...,\n",
       "           4.14054155e-01, -4.59358424e-01, -9.24978375e-01]],\n",
       "\n",
       "        [[ 4.16857719e-01, -5.40644646e-01,  4.81188446e-01, ...,\n",
       "          -3.18559706e-02, -6.26358986e-02, -7.64046490e-01],\n",
       "         [-8.35238338e-01,  1.25970078e+00, -1.00269049e-01, ...,\n",
       "           1.24869645e-01,  3.62297058e-01, -2.22840667e-01],\n",
       "         [ 1.39955670e-01, -3.52445781e-01,  7.59568810e-01, ...,\n",
       "          -1.19089857e-01,  7.33846664e-01, -6.90358400e-01],\n",
       "         [-4.04649734e-01, -1.52999908e-01, -5.12646914e-01, ...,\n",
       "          -1.03288364e+00, -2.02597648e-01,  8.81029963e-02],\n",
       "         [ 4.05588150e-02, -3.76983851e-01, -3.74802947e-01, ...,\n",
       "           2.60576129e-01, -2.11357087e-01, -6.27990067e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.46461663e-02, -2.09189817e-01, -1.40232831e-01, ...,\n",
       "           7.71396458e-02,  4.47128713e-03, -7.77092129e-02],\n",
       "         [-2.46756911e-01,  3.18105191e-01,  3.07907388e-02, ...,\n",
       "           2.72649884e-01,  1.58685654e-01, -1.26091823e-01],\n",
       "         [-2.82032728e-01,  2.80427366e-01, -2.42653877e-01, ...,\n",
       "           1.26567438e-01, -1.31639004e-01, -1.24739572e-01],\n",
       "         [-1.25433415e-01, -1.89620003e-01, -1.64141178e-01, ...,\n",
       "          -2.03525871e-01, -3.64283249e-02,  2.07481116e-01],\n",
       "         [-4.13775519e-02, -1.16453350e-01,  4.93552983e-01, ...,\n",
       "           2.22167045e-01, -3.37098897e-01,  1.28622934e-01]],\n",
       "\n",
       "        [[ 1.13660507e-01, -1.56104416e-01, -2.40285173e-01, ...,\n",
       "           4.60204668e-02, -2.26609081e-01, -7.22908974e-02],\n",
       "         [-2.23542452e-01,  8.61699060e-02,  7.18935877e-02, ...,\n",
       "           2.24723399e-01,  1.77340880e-01, -7.41909295e-02],\n",
       "         [-2.02019751e-01,  3.12982291e-01, -3.18868041e-01, ...,\n",
       "           1.10843681e-01, -1.78575158e-01, -4.40994054e-02],\n",
       "         [-1.03582442e-03, -1.27895892e-01, -3.58199537e-01, ...,\n",
       "          -2.02972502e-01,  1.62160486e-01,  1.06940724e-01],\n",
       "         [-9.93164778e-02, -3.20973814e-01,  4.44694191e-01, ...,\n",
       "           2.47680172e-01, -1.80787534e-01, -7.65044987e-02]],\n",
       "\n",
       "        [[ 8.24114457e-02, -3.01357098e-02, -1.49216518e-01, ...,\n",
       "          -5.11907041e-02,  6.19258732e-02,  3.94420773e-02],\n",
       "         [-1.02255873e-01,  2.74746865e-01,  4.68588471e-02, ...,\n",
       "           2.11527348e-01,  1.81602180e-01, -1.44504786e-01],\n",
       "         [-1.30428761e-01,  2.09063813e-01, -2.64697731e-01, ...,\n",
       "           4.33571845e-01, -8.51497054e-03, -1.20574296e-01],\n",
       "         [-1.02386944e-01, -1.87409148e-01, -1.86195523e-01, ...,\n",
       "          -5.99523485e-02, -5.93870878e-03,  4.30213839e-01],\n",
       "         [-2.25329831e-01, -2.38741815e-01,  4.70895976e-01, ...,\n",
       "           1.93057090e-01, -7.67114311e-02,  5.68318069e-02]]],\n",
       "\n",
       "\n",
       "       [[[-9.65997458e-01,  9.20506954e-01, -4.02766973e-01, ...,\n",
       "           6.30897641e-01, -4.42630589e-01,  8.15256774e-01],\n",
       "         [-6.37662768e-01, -1.40282378e-01, -2.04258394e+00, ...,\n",
       "          -2.15070796e+00, -8.33541393e-01,  7.67337024e-01],\n",
       "         [-1.19251534e-01, -5.77580333e-01,  4.24806744e-01, ...,\n",
       "          -1.01132765e-01,  5.06062388e-01,  4.79227304e-01],\n",
       "         [-1.52648807e+00, -1.52858281e+00,  3.63956422e-01, ...,\n",
       "          -5.90959191e-01,  3.54864120e-01,  5.32769561e-01],\n",
       "         [ 2.52973497e-01,  2.29867160e-01,  6.40697837e-01, ...,\n",
       "          -1.38792503e+00, -5.16400099e-01, -4.31609482e-01]],\n",
       "\n",
       "        [[ 8.33809376e-04,  6.65729642e-02, -6.30066693e-02, ...,\n",
       "           6.13941789e-01,  3.22445333e-01,  5.05473018e-02],\n",
       "         [-9.31022406e-01, -6.55548275e-01, -1.66760755e+00, ...,\n",
       "          -2.55891085e-01, -1.83742270e-01,  7.90910542e-01],\n",
       "         [ 6.21436417e-01,  8.33452165e-01,  6.28075957e-01, ...,\n",
       "          -2.32446462e-01,  7.49834180e-01, -7.93465912e-01],\n",
       "         [-4.70966250e-01, -1.04724538e+00, -2.43585989e-01, ...,\n",
       "          -7.57931173e-01,  5.75286806e-01, -2.71687001e-01],\n",
       "         [-3.52398306e-02,  7.63875067e-01, -3.31046104e-01, ...,\n",
       "          -1.43258452e+00,  1.02431655e-01,  9.13009167e-01]],\n",
       "\n",
       "        [[-3.59758288e-01,  5.10190845e-01, -3.65608931e-01, ...,\n",
       "          -6.91600144e-02,  3.00659835e-01,  2.57889748e-01],\n",
       "         [ 1.03709698e-02, -4.76537049e-01, -1.25569081e+00, ...,\n",
       "          -7.43666768e-01, -2.36548632e-01,  5.74873805e-01],\n",
       "         [ 4.91958261e-01,  4.18394566e-01,  9.50539231e-01, ...,\n",
       "           1.59125715e-01,  7.14536250e-01, -8.61955523e-01],\n",
       "         [-7.06684709e-01, -1.97526670e+00, -1.70720518e-01, ...,\n",
       "           5.55517435e-01, -7.48922408e-01, -9.79935408e-01],\n",
       "         [ 6.56301826e-02,  6.60632491e-01, -9.76979733e-03, ...,\n",
       "          -1.14881539e+00,  2.19579935e-02,  5.63169837e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.28598496e-01, -1.87785178e-01,  2.77169824e-01, ...,\n",
       "          -6.64116144e-02,  8.79196823e-03,  3.00378501e-01],\n",
       "         [ 3.71161401e-01, -3.80638003e-01, -4.39044267e-01, ...,\n",
       "          -2.11597607e-01, -2.14081779e-02,  2.52248108e-01],\n",
       "         [-2.39687227e-02,  9.34533477e-02,  1.08637981e-01, ...,\n",
       "          -2.41384193e-01, -3.89115512e-03, -3.64288479e-01],\n",
       "         [-3.68420005e-01, -5.88454604e-01, -2.01098382e-01, ...,\n",
       "           4.47348535e-01,  3.84005606e-02,  2.66266093e-02],\n",
       "         [-3.77729386e-02, -1.26700982e-01, -5.97736537e-02, ...,\n",
       "          -1.77438259e-01,  5.39840236e-02,  1.08636320e-01]],\n",
       "\n",
       "        [[-1.94326088e-01,  1.74820393e-01,  1.13330968e-01, ...,\n",
       "          -1.35294795e-01, -2.17711091e-01,  1.08864475e-02],\n",
       "         [ 1.58905387e-02, -4.57709610e-01, -4.58394110e-01, ...,\n",
       "          -1.45328477e-01, -2.10216492e-02,  1.47931665e-01],\n",
       "         [ 9.64853540e-03,  1.13819674e-01,  1.72539294e-01, ...,\n",
       "          -1.50932848e-01, -8.97455364e-02, -4.14387077e-01],\n",
       "         [-2.58480728e-01, -1.91578895e-01,  6.95449859e-02, ...,\n",
       "           3.40096891e-01,  2.15981603e-01,  1.81611925e-01],\n",
       "         [-1.74524151e-02, -3.93599153e-01, -7.20226467e-02, ...,\n",
       "          -1.89106584e-01, -1.26542561e-02,  1.80163592e-01]],\n",
       "\n",
       "        [[-1.54588550e-01,  2.68478394e-01,  1.78687066e-01, ...,\n",
       "           1.33530423e-03, -2.95320839e-01, -4.93988618e-02],\n",
       "         [ 9.82238352e-03, -4.28183794e-01, -4.98565167e-01, ...,\n",
       "          -3.55457723e-01, -1.44959807e-01,  1.92842677e-01],\n",
       "         [ 1.83362678e-01,  1.02668166e-01,  2.19926611e-03, ...,\n",
       "          -1.17708609e-01,  3.16640623e-02, -5.44668496e-01],\n",
       "         [-2.04948410e-01, -2.49494553e-01,  9.49664563e-02, ...,\n",
       "           3.30298930e-01,  2.65594542e-01, -2.15122700e-02],\n",
       "         [ 6.93483502e-02, -1.15643509e-01,  4.19968963e-02, ...,\n",
       "          -4.08859923e-02,  3.06080133e-02,  1.78523749e-01]]],\n",
       "\n",
       "\n",
       "       [[[-5.57235666e-02, -1.29445505e+00,  8.63606393e-01, ...,\n",
       "           9.16443840e-02,  2.87114358e+00, -2.50953007e-02],\n",
       "         [-1.32106379e-01,  1.77939892e-01,  1.46578455e+00, ...,\n",
       "          -4.05531190e-02, -7.52615705e-02, -3.37053806e-01],\n",
       "         [ 5.17712068e-03, -2.70338392e+00, -1.83760273e+00, ...,\n",
       "           7.65515566e-01,  8.48216832e-01, -5.39563894e-02],\n",
       "         [ 2.08845639e+00, -2.36571610e-01, -8.30223262e-02, ...,\n",
       "          -6.09527528e-01,  7.27847874e-01, -1.89742401e-01],\n",
       "         [-1.49629176e+00, -2.07501367e-01, -3.85054536e-02, ...,\n",
       "          -6.40311301e-01,  1.28601596e-01, -5.74472249e-01]],\n",
       "\n",
       "        [[ 6.77243829e-01, -5.49074411e-01,  1.46660793e+00, ...,\n",
       "          -1.32728577e-01,  1.71429658e+00,  1.72052279e-01],\n",
       "         [-3.92600805e-01,  2.57098734e-01,  4.41087544e-01, ...,\n",
       "          -2.74584413e-01, -6.40232787e-02,  2.96853483e-01],\n",
       "         [ 1.80504188e-01, -1.43234086e+00, -8.30054462e-01, ...,\n",
       "          -2.79608101e-01,  6.53412223e-01,  3.64544362e-01],\n",
       "         [ 9.14927363e-01, -3.82772058e-01,  9.13250864e-01, ...,\n",
       "           2.69148678e-01,  1.49160668e-01,  1.18267685e-01],\n",
       "         [ 1.30442888e-01,  2.95206010e-01,  3.25687259e-01, ...,\n",
       "          -3.94599557e-01, -1.15633868e-02,  3.20931107e-01]],\n",
       "\n",
       "        [[ 4.93559539e-01, -1.38051942e-01,  3.94997001e-01, ...,\n",
       "           1.12861067e-01,  5.40800691e-01,  5.14081478e-01],\n",
       "         [ 6.46853745e-02, -1.22809753e-01, -9.56266999e-01, ...,\n",
       "          -3.31646860e-01,  6.05458915e-01,  5.44139206e-01],\n",
       "         [ 2.29573518e-01, -1.60390043e+00, -5.75848341e-01, ...,\n",
       "           1.30140662e-01,  9.39780295e-01, -8.03577900e-03],\n",
       "         [ 1.12058401e+00, -1.02021463e-01,  6.06094301e-01, ...,\n",
       "          -4.58701313e-01,  4.36202824e-01,  1.72578856e-01],\n",
       "         [ 3.15622419e-01,  3.88401747e-01, -3.82533222e-02, ...,\n",
       "          -3.61920416e-01, -3.10302973e-01,  8.76145959e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.25688225e-01, -8.49011540e-02,  5.67690849e-01, ...,\n",
       "          -9.19908285e-04, -1.52142167e-01, -2.34851211e-01],\n",
       "         [-9.90934670e-04,  2.40874052e-01, -4.41098750e-01, ...,\n",
       "          -1.34380698e-01, -1.70739919e-01, -3.35927963e-01],\n",
       "         [-2.88397282e-01,  9.47869420e-02, -2.32983276e-01, ...,\n",
       "           3.94690990e-01, -2.58602202e-02, -3.53036374e-02],\n",
       "         [-9.86583531e-03,  1.84627488e-01,  1.84530869e-01, ...,\n",
       "           3.42164218e-01,  1.29368216e-01, -6.10239357e-02],\n",
       "         [-8.64179358e-02, -6.02712221e-02,  2.49508157e-01, ...,\n",
       "           2.79758126e-03, -1.01340711e-01,  2.31053606e-02]],\n",
       "\n",
       "        [[ 3.65069062e-02, -2.08598763e-01,  3.23166043e-01, ...,\n",
       "           1.87662065e-01, -1.32633299e-02, -1.14941336e-01],\n",
       "         [ 4.33410592e-02, -1.05551094e-01, -5.36241531e-02, ...,\n",
       "          -8.18925276e-02, -4.72161472e-02, -2.36469612e-01],\n",
       "         [-2.87049234e-01,  1.23128109e-02, -2.43865818e-01, ...,\n",
       "           5.16033590e-01, -9.52927768e-03, -3.60448062e-02],\n",
       "         [-8.41306299e-02,  1.22836009e-02,  1.89396441e-01, ...,\n",
       "           4.21120048e-01,  1.28847167e-01,  4.41601425e-02],\n",
       "         [-5.38126007e-02, -4.35012653e-02,  1.29886061e-01, ...,\n",
       "          -2.75079980e-02,  1.13827139e-01,  1.09712556e-01]],\n",
       "\n",
       "        [[-2.51807272e-04, -2.48624563e-01,  2.88901329e-01, ...,\n",
       "           7.94041380e-02,  2.41892830e-01, -1.89739570e-01],\n",
       "         [ 1.17039219e-01, -1.96503103e-01, -1.95881233e-01, ...,\n",
       "          -1.57980561e-01, -6.01279773e-02, -1.57339558e-01],\n",
       "         [-1.84237659e-01,  5.04698344e-02, -2.34657213e-01, ...,\n",
       "           4.95418429e-01,  1.45938545e-02, -1.87382460e-01],\n",
       "         [-2.87013277e-02,  5.42379990e-02,  1.56215221e-01, ...,\n",
       "           1.39133573e-01,  1.56599432e-01,  6.32003769e-02],\n",
       "         [-8.03343430e-02, -1.87200159e-01,  1.99679911e-01, ...,\n",
       "           4.03532572e-02,  8.07964206e-02,  5.17450869e-02]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 7.27106094e-01,  7.23592639e-01, -5.65101445e-01, ...,\n",
       "           1.22081506e+00, -4.11048830e-01, -1.12195265e+00],\n",
       "         [ 1.18893087e+00,  3.75862151e-01,  2.57315457e-01, ...,\n",
       "          -7.54914761e-01,  3.38061899e-01, -9.07701075e-01],\n",
       "         [-2.64034629e-01,  1.02936959e+00, -6.62908077e-01, ...,\n",
       "          -1.51105082e+00, -1.46904194e+00,  3.61955047e-01],\n",
       "         [-1.24367976e+00,  1.69321811e+00, -8.97484541e-01, ...,\n",
       "           4.21043998e-03,  3.67464781e-01,  1.31036854e+00],\n",
       "         [-1.14491951e+00, -2.26522163e-02, -1.90205860e+00, ...,\n",
       "           8.71188700e-01,  7.14758575e-01,  9.26617682e-01]],\n",
       "\n",
       "        [[ 4.69664633e-01,  1.88980073e-01, -2.93137252e-01, ...,\n",
       "           9.76037979e-03,  1.12691551e-01, -9.21357155e-01],\n",
       "         [ 8.18020225e-01, -4.90376920e-01,  8.64330232e-01, ...,\n",
       "          -3.48485798e-01, -4.76154387e-01, -1.09062850e+00],\n",
       "         [ 5.09425581e-01, -3.20422649e-03, -2.93001652e-01, ...,\n",
       "          -1.23871768e+00, -7.58205712e-01,  8.04896891e-01],\n",
       "         [-9.11848903e-01,  5.00559807e-02,  1.43777311e-01, ...,\n",
       "           2.63052911e-01,  4.24624532e-02,  6.17048323e-01],\n",
       "         [-7.96609461e-01,  4.65527415e-01, -9.57843125e-01, ...,\n",
       "          -3.01271051e-01, -1.06900930e-02,  7.47745216e-01]],\n",
       "\n",
       "        [[-1.47960126e-01,  4.43448216e-01,  1.40458673e-01, ...,\n",
       "          -7.66091466e-01,  5.95624208e-01, -3.46737802e-01],\n",
       "         [ 9.54141378e-01, -1.23442501e-01, -9.05583799e-02, ...,\n",
       "          -4.94939208e-01,  6.48444891e-02, -1.06944966e+00],\n",
       "         [ 2.94299215e-01,  1.40257031e-01, -3.13591033e-01, ...,\n",
       "          -1.01168644e+00, -7.81798542e-01,  6.47431791e-01],\n",
       "         [ 7.94800371e-02, -4.16727781e-01, -4.17076170e-01, ...,\n",
       "           5.74517190e-01,  5.36435694e-02,  5.46349399e-02],\n",
       "         [-1.16053319e+00,  9.11521763e-02, -1.70840061e+00, ...,\n",
       "           4.22221333e-01, -3.53780717e-01,  4.23851132e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.66335368e-01, -3.46800163e-02,  2.08542854e-01, ...,\n",
       "           3.26966792e-02, -2.23399907e-01, -3.87399614e-01],\n",
       "         [ 3.84779274e-01, -5.42591363e-02,  5.56672812e-02, ...,\n",
       "          -2.45324597e-01, -4.84664589e-01, -2.67076254e-01],\n",
       "         [ 1.88961968e-01,  9.39839557e-02, -1.12516433e-02, ...,\n",
       "           1.41155884e-01, -1.39595509e-01, -1.59920722e-01],\n",
       "         [ 5.35865366e-01, -2.00828969e-01,  3.08643319e-02, ...,\n",
       "          -1.30357355e-01, -5.33896327e-01, -3.08949947e-02],\n",
       "         [-2.11567014e-01, -4.35845375e-01, -3.39681536e-01, ...,\n",
       "           1.90126300e-02, -2.25789607e-01, -4.20729637e-01]],\n",
       "\n",
       "        [[ 2.85235822e-01,  7.08593130e-02,  3.28588814e-01, ...,\n",
       "          -6.45162389e-02, -2.25901693e-01, -4.08070385e-01],\n",
       "         [ 6.27145767e-01,  8.80687963e-03,  2.64317393e-02, ...,\n",
       "          -1.79168165e-01, -3.85678470e-01, -4.44840193e-01],\n",
       "         [ 2.12978855e-01,  2.07704887e-01,  2.39592358e-01, ...,\n",
       "           5.00494361e-01, -2.68478364e-01, -9.96710509e-02],\n",
       "         [ 1.53621644e-01, -2.20393389e-01,  1.42011106e-01, ...,\n",
       "          -1.48640007e-01, -4.97302473e-01, -8.66260752e-03],\n",
       "         [-2.88243324e-01, -2.44730264e-01, -1.58065349e-01, ...,\n",
       "          -2.84560025e-04, -1.06856085e-01, -4.34987634e-01]],\n",
       "\n",
       "        [[ 2.90277809e-01,  4.96966690e-02,  3.95278215e-01, ...,\n",
       "          -2.31549405e-02, -2.23881736e-01, -2.79189259e-01],\n",
       "         [ 3.67430776e-01,  1.18075758e-02,  1.78924650e-01, ...,\n",
       "          -6.60859793e-02, -3.57908159e-01, -3.53926480e-01],\n",
       "         [ 2.14500099e-01,  2.65086412e-01,  2.16129690e-01, ...,\n",
       "           5.19465148e-01, -1.18659884e-01,  4.41940576e-02],\n",
       "         [ 2.26520002e-01, -3.04583371e-01,  4.77717519e-02, ...,\n",
       "          -1.40040174e-01, -4.80819523e-01, -3.10007669e-02],\n",
       "         [-2.03678966e-01, -1.36394724e-01, -7.50008151e-02, ...,\n",
       "          -1.33809820e-03, -1.73107386e-01, -3.54379594e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 4.18842494e-01, -1.17178929e+00,  1.13025999e+00, ...,\n",
       "          -8.27826142e-01,  8.88807625e-02, -1.00514323e-01],\n",
       "         [ 1.45932043e+00, -9.05333519e-01,  1.13583982e+00, ...,\n",
       "          -5.62896311e-01, -1.52376026e-01, -8.10835063e-01],\n",
       "         [ 4.60104495e-01,  1.03376901e+00, -7.59262204e-01, ...,\n",
       "           1.20914769e+00, -5.26570380e-01, -2.12983102e-01],\n",
       "         [ 1.03584766e+00, -2.56018668e-01, -3.53309178e+00, ...,\n",
       "          -1.64902008e+00,  7.50226557e-01, -7.19714940e-01],\n",
       "         [-4.32609975e-01, -3.77382815e-01, -1.18292773e+00, ...,\n",
       "           3.71936053e-01, -3.64735067e-01, -1.87707081e-01]],\n",
       "\n",
       "        [[ 2.26152942e-01, -6.79813862e-01,  1.20095778e+00, ...,\n",
       "          -5.74514329e-01, -2.16938436e-01, -1.63001642e-01],\n",
       "         [ 8.49200487e-01,  3.13012898e-02,  1.48002660e+00, ...,\n",
       "          -9.01350021e-01,  4.39708531e-01,  7.10350871e-01],\n",
       "         [ 6.36167079e-02,  4.85646963e-01, -1.67710155e-01, ...,\n",
       "           7.80350089e-01, -5.83370328e-01,  9.64852422e-02],\n",
       "         [ 8.70435655e-01, -1.53407499e-01, -1.78044844e+00, ...,\n",
       "          -1.58563662e+00,  7.56102443e-01, -4.93658900e-01],\n",
       "         [ 5.23727387e-02, -8.88321519e-01, -2.12001801e-03, ...,\n",
       "           2.37461776e-01, -3.63676727e-01,  3.75332326e-01]],\n",
       "\n",
       "        [[ 4.47376341e-01, -5.86796343e-01,  9.86965418e-01, ...,\n",
       "          -8.19614589e-01, -4.48764324e-01, -3.06777656e-01],\n",
       "         [ 6.99359059e-01, -4.53018248e-01,  1.24826980e+00, ...,\n",
       "          -4.05971885e-01,  6.36227667e-01,  2.47095406e-01],\n",
       "         [ 6.59028590e-02,  5.15998304e-01, -1.04906559e-01, ...,\n",
       "           8.70502472e-01, -3.52699220e-01, -1.15534008e-01],\n",
       "         [ 6.95706010e-01, -2.70090792e-02, -1.25527394e+00, ...,\n",
       "          -1.37755048e+00,  4.30524617e-01, -3.68766487e-01],\n",
       "         [-3.26024383e-01, -7.66563296e-01,  2.00048178e-01, ...,\n",
       "          -4.92164418e-02, -2.35095561e-01,  3.29027891e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.11025786e-02, -3.26036960e-01,  2.37468034e-01, ...,\n",
       "          -1.38073355e-01, -1.91407204e-01,  3.05380225e-01],\n",
       "         [ 3.03665459e-01, -1.20131791e-01,  6.13215923e-01, ...,\n",
       "          -5.59484214e-02,  3.16469014e-01,  3.30456376e-01],\n",
       "         [-3.31985384e-01,  1.19889170e-01,  1.99283630e-01, ...,\n",
       "           1.41300380e-01, -5.18255115e-01,  2.46613175e-02],\n",
       "         [ 5.15572727e-01,  4.26352769e-02, -4.91377532e-01, ...,\n",
       "           9.10644829e-02,  6.76544309e-01, -3.20018560e-01],\n",
       "         [ 5.24524450e-02,  1.11251473e-02, -3.01611483e-01, ...,\n",
       "          -2.09237531e-01, -6.39292121e-01,  1.21279739e-01]],\n",
       "\n",
       "        [[-1.27070606e-01, -4.55759287e-01,  4.16624367e-01, ...,\n",
       "          -1.50147855e-01, -3.87851506e-01,  4.85600010e-02],\n",
       "         [ 1.47524133e-01,  5.44821993e-02,  6.28100753e-01, ...,\n",
       "          -6.08884767e-02,  1.91557139e-01,  1.60359293e-01],\n",
       "         [-2.21704200e-01,  1.72696114e-01,  1.60371989e-01, ...,\n",
       "           8.59221891e-02, -4.96326864e-01,  4.84474748e-03],\n",
       "         [ 4.00653630e-01,  5.42399138e-02, -4.36372876e-01, ...,\n",
       "           1.23267204e-01,  7.92703331e-01, -1.47435471e-01],\n",
       "         [-6.21022806e-02, -1.60662085e-01, -2.20062211e-01, ...,\n",
       "          -2.55432308e-01, -5.43489218e-01, -1.10328130e-01]],\n",
       "\n",
       "        [[-2.50736810e-02, -2.16280267e-01,  2.87706017e-01, ...,\n",
       "           8.72218981e-03, -3.83596048e-02,  2.62336791e-01],\n",
       "         [ 3.07317019e-01, -4.63112928e-02,  4.97919738e-01, ...,\n",
       "          -7.08696246e-02,  1.69866517e-01,  1.62001923e-01],\n",
       "         [-3.21640879e-01,  1.26775578e-01,  2.05559105e-01, ...,\n",
       "           1.08230434e-01, -3.47669482e-01,  1.20408058e-01],\n",
       "         [ 3.87363672e-01,  1.44764587e-01, -4.45222020e-01, ...,\n",
       "           2.32285429e-02,  6.76599443e-01, -2.27351546e-01],\n",
       "         [-1.69597566e-01, -7.02878386e-02, -1.65442467e-01, ...,\n",
       "          -1.30980164e-01, -5.70808113e-01, -1.53843276e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 4.64683920e-02, -1.04420042e+00, -7.09033608e-01, ...,\n",
       "          -3.72145772e-01,  1.48885190e+00, -8.09291601e-01],\n",
       "         [ 1.15577912e+00,  1.69385746e-01, -9.18141782e-01, ...,\n",
       "          -1.78449616e-01,  1.59224331e-01,  8.68869841e-01],\n",
       "         [ 2.46477842e+00, -7.74758518e-01, -1.07422793e+00, ...,\n",
       "          -9.26858723e-01,  1.56993675e+00,  3.06909323e-01],\n",
       "         [ 6.36230782e-02,  5.93913734e-01,  1.30635500e+00, ...,\n",
       "           7.70037532e-01,  1.21355191e-01,  1.82869151e-01],\n",
       "         [-3.43433291e-01,  1.10261214e+00, -6.57017648e-01, ...,\n",
       "          -1.30394018e+00,  3.72789174e-01,  1.12433290e+00]],\n",
       "\n",
       "        [[ 1.92440361e-01, -3.03074419e-01, -5.27962685e-01, ...,\n",
       "          -3.41830343e-01,  1.11465716e+00, -3.84554982e-01],\n",
       "         [ 5.89676321e-01,  1.31283313e-01, -7.93399334e-01, ...,\n",
       "           1.80001646e-01,  6.38602376e-01,  9.60733056e-01],\n",
       "         [ 2.07308030e+00, -2.08059177e-01, -1.27503133e+00, ...,\n",
       "          -4.03348386e-01,  1.47869718e+00,  1.24210882e+00],\n",
       "         [-4.14546549e-01,  2.75954604e-01,  8.38942289e-01, ...,\n",
       "           7.33329892e-01, -8.75725150e-02, -9.47804153e-02],\n",
       "         [-1.13320422e+00,  3.15985739e-01,  1.73874766e-01, ...,\n",
       "          -1.04169631e+00,  1.23542178e+00,  1.21545982e+00]],\n",
       "\n",
       "        [[ 7.23376632e-01, -1.14376843e-02, -1.05788402e-01, ...,\n",
       "          -3.29912841e-01,  6.48612797e-01,  2.83885077e-02],\n",
       "         [ 2.65060067e-01,  1.80676460e-01, -5.83558440e-01, ...,\n",
       "           9.88293767e-01, -2.26531029e-02,  7.16902733e-01],\n",
       "         [ 1.04437971e+00, -2.14092195e-01, -1.13388991e+00, ...,\n",
       "          -6.40896708e-02,  1.19878900e+00,  1.39816988e+00],\n",
       "         [-5.12914419e-01, -6.97882921e-02,  5.12035668e-01, ...,\n",
       "           5.24444282e-01,  2.87324756e-01,  3.74169707e-01],\n",
       "         [-6.23293579e-01,  3.87288511e-01,  2.21182033e-01, ...,\n",
       "          -4.67202604e-01,  8.48120153e-01,  1.28454661e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.87377322e-01,  1.62241645e-02, -3.22977364e-01, ...,\n",
       "          -2.12945193e-01,  2.68544078e-01,  2.86164224e-01],\n",
       "         [ 8.88703316e-02,  1.56290591e-01, -1.43581808e-01, ...,\n",
       "           1.82260871e-02,  4.14999068e-01, -1.44291505e-01],\n",
       "         [ 2.89650112e-01, -2.39814460e-01, -1.23534121e-01, ...,\n",
       "           6.79494366e-02,  2.88539916e-01,  6.07554376e-01],\n",
       "         [-3.03641349e-01, -2.09435940e-01, -1.52317226e-01, ...,\n",
       "           5.03417477e-02, -1.06596932e-01, -1.11868978e-03],\n",
       "         [ 3.79103482e-01, -7.67059624e-02,  2.89684623e-01, ...,\n",
       "          -1.41032398e-01,  2.01440722e-01, -3.46867263e-01]],\n",
       "\n",
       "        [[ 2.77966380e-01,  8.16990584e-02, -4.02546346e-01, ...,\n",
       "          -2.12252408e-01,  1.67726040e-01,  1.91714540e-01],\n",
       "         [ 3.06034178e-01,  1.43142745e-01, -8.90778676e-02, ...,\n",
       "           2.21223123e-02,  3.72742474e-01, -1.04812711e-01],\n",
       "         [ 7.81853497e-02, -2.93022990e-01, -2.00202167e-01, ...,\n",
       "           1.22338504e-01,  4.18639600e-01,  7.65581012e-01],\n",
       "         [-3.63038331e-01, -1.34014741e-01, -4.17326689e-02, ...,\n",
       "           7.96104521e-02, -2.67265290e-01, -2.50168771e-01],\n",
       "         [ 3.45216870e-01, -1.38009265e-02,  2.67920792e-01, ...,\n",
       "          -1.48120731e-01,  2.43025884e-01, -2.99080282e-01]],\n",
       "\n",
       "        [[ 3.28732789e-01, -1.35151297e-02, -2.21350133e-01, ...,\n",
       "          -1.85851187e-01,  1.93860203e-01,  2.45224923e-01],\n",
       "         [ 5.06863594e-01,  2.25420088e-01, -1.04692638e-01, ...,\n",
       "           2.50992835e-01,  2.81874686e-01, -1.12941042e-01],\n",
       "         [ 2.22831368e-01,  4.13761586e-02, -2.39433601e-01, ...,\n",
       "           1.77794412e-01,  4.69834596e-01,  6.23980761e-01],\n",
       "         [-3.02948952e-01,  1.60568982e-01,  2.83091068e-02, ...,\n",
       "          -9.21261460e-02, -3.81238014e-02,  3.98035012e-02],\n",
       "         [ 7.17657804e-02, -8.97659585e-02,  2.50653207e-01, ...,\n",
       "          -1.79733470e-01,  3.11769068e-01, -2.90658087e-01]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the non-masking parts look correct\n",
    "\n",
    "# note: I dont know the notation of the bool mask (does \"true\" mean to mask or not)\n",
    "# what is probably true: 1 (or true) means to keep\n",
    "\n",
    "# assumption that must be checked: assume that True means to replace --> *** FIX THIS\n",
    "def mask_fill(matrix, mask, value):\n",
    "    negmask = 1 - mask\n",
    "    return (matrix * mask) + (negmask * value)\n",
    "\n",
    "class MySelfAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale:float, name=None):\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, qkv, key_padding_mask=None):\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = tf.unstack(qkv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        softmax_scale = self.softmax_scale\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen), -10000.0, dtype=scores.dtype)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 0.0)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # i am assuming this to be causal\n",
    "        causal_mask = tf.experimental.numpy.triu(tf.fill((seqlen, seqlen), -10000.0), 1) # might need to be replaced\n",
    "        scores = scores + tf.cast(causal_mask, dtype=scores.dtype)\n",
    "        attention = tf.cast(tf.nn.softmax(scores, axis=-1), dtype=v.dtype)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        causal: bool = True,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attention_dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.drop = nn.Dropout(attention_dropout)\n",
    "    @torch.autocast(\"cpu\", enabled=False)\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.FloatTensor,\n",
    "        causal: bool = None,\n",
    "        key_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.FloatTensor:\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.to(torch.float32)\n",
    "        k = k.to(torch.float32)\n",
    "        causal = self.causal if causal is None else causal\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        # using float16, which might lead to overflow\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype, device=scores.device)\n",
    "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
    "            scores = scores + rearrange(padding_mask, \"b s -> b 1 1 s\")\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "        attention = torch.softmax(scores, dim=-1).to(v.dtype)\n",
    "        attention = self.drop(attention)\n",
    "        output = torch.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "my_self_atten = MySelfAttention(softmax_scale=0.1)\n",
    "self_atten = SelfAttention(softmax_scale=0.1)\n",
    "\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "# qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "qkv = torch.randn([7, 17, 3, 5, 11])\n",
    "print(self_atten(qkv))\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "my_self_atten(qkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCrossAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale, name=None): # assume to be causal\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, q, kv, key_padding_mask):\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        if kv.shape[3] != kv.shape[2]:\n",
    "            kv = tf.repeat(kv, repeats=q.shape[2] // kv.shape[3], axis=-2)\n",
    "        k, v = tf.unstack(kv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * self.softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen_k), -10000.0, dtype=scores.dtype)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 0.0)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # causal stuff\n",
    "        rows = tf.expand_dims(tf.range(seqlen_q, dtype=tf.long), axis=1) # if this fails, do axis=-1\n",
    "        cols = tf.range(seqlen_k, dtype=tf.long)\n",
    "        causal_mask = cols > rows + seqlen_k - seqlen_q\n",
    "        scores = mask_fill(scores, causal_mask, -10000.0)\n",
    "        # end of causal stuff\n",
    "        attention = tf.softmax(scores, axis=-1, dtype=v.dtype)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MY_update_kv_cache(kv, inference_params, layer_idx:int):\n",
    "    num_heads, head_dim = kv.shape[-2:]\n",
    "    if layer_idx not in inference_params.key_value_memory_dict:\n",
    "        # during the UDO desgin process, could use uninitalized memory rather than zeros\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.zeros(\n",
    "            [\n",
    "                inference_params.max_batch_size,\n",
    "                inference_params.max_seqlen,\n",
    "                2,\n",
    "                num_heads,\n",
    "                head_dim\n",
    "            ],\n",
    "            dtype=kv.dtype\n",
    "        )\n",
    "    batch_start = inference_params.batch_size_offset\n",
    "    batch_end = batch_start + kv.shape[0]\n",
    "    sequence_start = inference_params.seqlen_offset\n",
    "    sequence_end = sequence_start + kv.shape[1]\n",
    "    # When the current sequence length is equal to or larger than the maximum sequence length,\n",
    "    # we need to concatenate the current `kv` with the cached `kv` to expand its length\n",
    "    if sequence_end >= inference_params.max_seqlen:\n",
    "        # the line below might fail due to the tuple\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.concat((inference_params.key_value_memory_dict[layer_idx], kv), axis=1)\n",
    "    inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, sequence_start:sequence_end, ...] = kv\n",
    "    kv = inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, :sequence_end, ...]\n",
    "    return kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2711594939.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    kwargs= **rotary_kwargs,\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MyMHA(tf.Module):\n",
    "    def __init__(self, dtype, rotary_dim, rotary_base, rotary_scale_base,\n",
    "                 n_head, n_head_kv, head_dim, bias:bool, softmax_scale:float,\n",
    "                 layer_idx:int, return_residual:bool,\n",
    "                 flash_rot_emb:bool, n_positions, n_embd,\n",
    "                 fused_dense:bool,\n",
    "                 flash_attn:bool,\n",
    "                 name=None):\n",
    "        super().__init__(name)\n",
    "        # Rotary embedding\n",
    "        self.rotary_dim = rotary_dim\n",
    "        if self.rotary_dim > 0:\n",
    "            rotary_cls = FlashRotaryEmbedding if flash_rot_emb else MyRotaryEmbedding\n",
    "            rotary_kwargs = {}\n",
    "            if rotary_cls is RotaryEmbedding:\n",
    "                rotary_kwargs[\"max_position_embeddings\"] = n_positions\n",
    "            self.rotary_emb =  rotary_cls(\n",
    "                self.rotary_dim,\n",
    "                base=rotary_base,\n",
    "                scale_base=rotary_scale_base,\n",
    "                kwargs= **rotary_kwargs,\n",
    "            )\n",
    "        # MLP\n",
    "        self.n_head = n_head\n",
    "        self.n_head_kv = n_head_kv\n",
    "        self.head_dim = head_dim\n",
    "        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n",
    "        hidden_size = n_embd\n",
    "        linear_cls = FusedDense if fused_dense else bert.Dense\n",
    "        self.Wqkv = linear_cls(hidden_size, op_size, weights=None,  \n",
    "                               is_biased=True, bias=None)\n",
    "        self.out_proj = linear_cls(hidden_size, hidden_size, weights=None,  \n",
    "                               is_biased=True, bias=None)\n",
    "        # Attention\n",
    "        attn_cls = FlashSelfAttention if flash_attn else MySelfAttention\n",
    "        cross_attn_cls = FlashCrossAttention if flash_attn else MyCrossAttention\n",
    "        self.inner_attn = attn_cls(softmax_scale=softmax_scale)\n",
    "        self.inner_cross_attn = cross_attn_cls(softmax_scale=softmax_scale)\n",
    "        self.flash_attn = flash_attn and attn_cls is FlashSelfAttention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.return_residual = return_residual\n",
    "    \n",
    "    def _forward_self_attn(self, x, key_padding_mask):\n",
    "        qkv = self.Wqkv(x)\n",
    "        h = qkv.shape[-1] // (3 * self.head_dim)\n",
    "        assert(h * 3 * self.head_dim == qkv.shape[-1])\n",
    "        qkv = tf.reshape(qkv, qkv.shape[:-1] + [3, h, self.head_dim])\n",
    "        if self.rotary_dim > 0:\n",
    "            qkv = self.rotary_emb(qkv)\n",
    "        if self.flash_attn:\n",
    "            batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "            cu_seqlens, max_seqlen = None, None\n",
    "            if key_padding_mask is not None:\n",
    "                # If `key_padding_mask` is supplied, we need to unpad the input and retrieve\n",
    "                # the `cu_seqlens` and `max_seqlen` to be used by `flash-attn`\n",
    "                qkv, indices, cu_seqlens, max_seqlen = unpad_input(qkv, key_padding_mask)\n",
    "            attn_output = self.inner_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
    "            # If `key_padding_mask` is supplied, we need to pad the output back to the original shape\n",
    "            return pad_input(attn_output, indices, batch_size, seqlen)\n",
    "    \n",
    "    def _forward_cross_attn(self, x, past_key_values, key_padding_mask):\n",
    "        batch_size = x.shape[0]\n",
    "        qkv = self.Wqkv(x)\n",
    "        q = qkv[..., : self.n_head * self.head_dim]\n",
    "        h = q.shape[-1] // self.head_dim\n",
    "        assert(h * self.head_dim == q.shape[-1])\n",
    "        q = tf.reshape(q, q.shape[:-1] + [h, self.head_dim])\n",
    "        kv = qkv[..., self.n_head * self.head_dim :]\n",
    "        hkv = kv.shape[-1] // (2 * self.head_dim)\n",
    "        assert(hkv * 2 * self.head_dim == kv.shape[-1])\n",
    "        kv = tf.reshape(kv, kv.shape[:-1] + [2, hkv, self.head_dim])\n",
    "        seqlen_offset = past_key_values.seqlen_offset if past_key_values is not None else 0\n",
    "        causal = None if seqlen_offset == 0 else False\n",
    "        if self.rotary_dim > 0:\n",
    "            q, kv = self.rotary_emb(q, kv=kv, seqlen_offset=seqlen_offset)\n",
    "        if past_key_values is not None:\n",
    "            kv = MY_update_kv_cache(kv, past_key_values, self.layer_idx)\n",
    "        assert(self.flash_attn is not None)\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k = (\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "        if key_padding_mask is not None:\n",
    "            kv, _, cu_seqlens_k, max_seqlen_k = unpad_input(kv, key_padding_mask)\n",
    "            if seqlen_q == 1:\n",
    "                key_padding_mask = tf.ones((batch_size, 1))\n",
    "            elif seqlen_q != seqlen_k:\n",
    "                key_padding_mask = key_padding_mask[:, -seqlen_q:]\n",
    "            q, indices_q, cu_seqlens_q, max_seqlen_q = unpad_input(q, key_padding_mask)\n",
    "        attn_output = self.inner_cross_attn(\n",
    "            q,\n",
    "            kv,\n",
    "            causal=causal,\n",
    "            cu_seqlens=cu_seqlens_q,\n",
    "            max_seqlen=max_seqlen_q,\n",
    "            cu_seqlens_k=cu_seqlens_k,\n",
    "            max_seqlen_k=max_seqlen_k,\n",
    "        )\n",
    "        return (\n",
    "        pad_input(attn_output, indices_q, batch_size, max_seqlen_q)\n",
    "        if key_padding_mask is not None\n",
    "        else attn_output\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, past_key_values, attention_mask):\n",
    "        assert(attention_mask is not None)\n",
    "        attention_mask = tf.cast(attention_mask, dtype=tf.bool)\n",
    "        # MHA\n",
    "        if self.n_head == self.n_head_kv:\n",
    "            assert(past_key_values is None)\n",
    "            # If `past_key_values` are not supplied, we run self-attention\n",
    "            attn_output = self._forward_self_attn(x, attention_mask)\n",
    "        # MQA / GQA\n",
    "        else:\n",
    "            # Regardless of `past_key_values` being supplied or not, it always use cross-attention\n",
    "            # because `q` and `kv` lengths might be different\n",
    "            attn_output = self._forward_cross_attn(x, past_key_values, attention_mask\n",
    "        output = tf.reshape(attn_output, attn.shape[:-2] + [attn_output.shape[-1] * attn_output.shape[-2]])\n",
    "        output = self.out_proj(output)\n",
    "        return output if not self.return_residual else (output, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyParallelBlock(tf.Module):\n",
    "    def __init__(self, block_idx, name=None):\n",
    "        super().__init__(name)\n",
    "        self.ln = bert.LayerNorm(weights=None, biases=None, eps=None)\n",
    "        self.block_idx = block_idx\n",
    "        self.mixer = MyMHA()\n",
    "        self.mlp = MyMLP()\n",
    "    \n",
    "    def __call__(self, hidden_states, past_key_values, attention_mask):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        attn_outputs = self.mixer(\n",
    "            hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        if isinstance(attn_outputs, tuple):\n",
    "            attn_outputs = attn_outputs[0]\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCausalLMHead(tf.Module):\n",
    "    def __init__(self, n_embd, layer_norm_eps, vocab_size, name=None):\n",
    "        super().__init__(name)\n",
    "        self.ln = nn.LayerNorm(n_embd, eps=layer_norm_eps)\n",
    "        self.linear = bert.Dense(n_embd, vocab_size)\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        logits = tf.cast(self.linear(hidden_states), dtype=tf.float32)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyPhiPreTrainedModel(tf.Module):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
