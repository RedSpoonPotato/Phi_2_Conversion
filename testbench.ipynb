{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 11:59:23.454102: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-01 11:59:24.054279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 11:59:25.622222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to microsoft phi-2 code (put something like this in final piece of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 11:59:28.622281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 11:59:28.622675: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# testing Embedding dimensions and calculations (done)\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Token embedding with dropout.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        hidden_states = self.wte(input_ids)\n",
    "        return hidden_states\n",
    "\n",
    "# testing py\n",
    "py_layer = Embedding(vocab_size=3000, n_embd=768)\n",
    "input = torch.randint(0, 3000, (4,))\n",
    "# print(py_layer(input))\n",
    "# testing tf\n",
    "matrix = py_layer.wte._parameters['weight'].data.numpy()\n",
    "matrix = tf.convert_to_tensor(matrix)\n",
    "tf_layer = bert.Embedding(4, 768, matrix)\n",
    "# print(tf_layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb() (done)\n",
    "def MY_apply_rotary_emb(x, cos, sin): # code mostly taken from original\n",
    "    print(\"x.shape:\", x.shape)\n",
    "    assert(len(x.shape) == 4)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = tf.split(x_rot, 2, axis=-1) # assumption that tf.split behaves the same as nn.chunk\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    x1, x2, c, s = [tf.cast(t, tf.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = tf.concat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1)\n",
    "    x_rot = tf.cast(x_rot, x.dtype) # might fail due to passing in x.dtype\n",
    "    return tf.concat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    ") -> torch.FloatTensor:\n",
    "    print(\"x.shape:\", x.shape)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = x_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    x1, x2, c, s = [t.to(dtype=torch.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = torch.cat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1).to(x.dtype)\n",
    "    return torch.cat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "last_dim = 5 # it seems this is neccesary\n",
    "x = torch.randn([2,3,2,last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "# print(_apply_rotary_emb(x, cos, sin))\n",
    "# print(\"-----\")\n",
    "x = tf.constant(x.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "# print(MY_apply_rotary_emb(x, cos, sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_kv (done)\n",
    "def MY_apply_rotary_emb_kv(kv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(kv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    k1, k2, c, s = [tf.cast(t, tf.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, kv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        kv[:, :, 1:2, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_kv(\n",
    "    kv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k= None,\n",
    "    sin_k= None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    k1, k2, c, s = [t.to(dtype=torch.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(kv.dtype)\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            kv[:, :, 1:2, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "kv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "kv = tf.constant(kv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "\n",
    "# print(MY_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_qkv (done)\n",
    "\n",
    "def MY_apply_rotary_emb_qkv(qkv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(qkv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = tf.split(q_rot, 2, axis=-1)\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    q1, q2, k1, k2, c, s = [tf.cast(t, tf.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = tf.concat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1)\n",
    "    q_rot = tf.cast(q_rot, qkv.dtype)\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, qkv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([q_rot, q_pass], axis=-1), axis=2),\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        qkv[:, :, 2:3, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_qkv(\n",
    "    qkv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k=None,\n",
    "    sin_k=None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    q1, q2, k1, k2, c, s = [t.to(dtype=torch.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = torch.cat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).to(qkv.dtype)\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(qkv.dtype)\n",
    "    return torch.cat(\n",
    "        [   torch.cat([q_rot, q_pass], axis=-1).unsqueeze(2),\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            qkv[:, :, 2:3, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "# print(MY_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 2\n",
      "x.shape: torch.Size([2, 3, 8, 10])\n",
      "torch.Size([2, 3, 8, 3])\n",
      "torch.Size([2, 3, 8, 3])\n",
      "torch.Size([3, 1, 3])\n",
      "torch.Size([3, 1, 3])\n",
      "(tensor([[[[-1.8938e+00, -4.1029e-01, -8.8421e-01, -1.9304e-01, -1.2140e-01,\n",
      "           -1.0539e+00, -1.4824e-02, -1.5790e-02,  6.0957e-01, -1.3694e+00],\n",
      "          [ 8.0937e-01,  1.1165e+00, -1.2808e-01,  1.9952e+00,  5.9032e-01,\n",
      "           -9.3814e-01, -5.7851e-02,  1.3259e+00,  1.0572e-01, -6.2015e-01],\n",
      "          [ 6.2096e-01,  5.4155e-01, -2.1747e-01, -4.6848e-01, -4.1188e-02,\n",
      "            9.9107e-01, -1.8650e+00,  3.3280e-01, -1.6502e+00, -2.0197e+00],\n",
      "          [ 2.3909e-01,  6.3765e-01, -7.6045e-02,  5.6865e-01, -2.5666e+00,\n",
      "            1.5488e-01,  3.1994e-01, -3.4196e-01, -1.0873e-01, -6.4962e-01],\n",
      "          [-4.8921e-01, -2.8636e-01,  3.6575e-01,  1.3399e+00,  2.5697e+00,\n",
      "            3.5761e-01, -3.1746e-01,  1.0271e+00, -5.4583e-01, -7.2060e-01],\n",
      "          [ 1.1611e+00,  7.6173e-01, -6.9356e-01,  2.6104e-02, -1.0691e+00,\n",
      "            1.6318e+00, -2.1081e-01, -2.5194e+00, -5.9818e-01,  8.9844e-01],\n",
      "          [-3.0419e+00, -4.9471e-01, -1.2881e+00, -2.2113e-01,  1.2074e-01,\n",
      "           -2.9413e-01,  1.5914e+00,  1.2493e+00,  7.9932e-01,  8.8000e-02],\n",
      "          [-1.4243e-01, -2.1552e-01, -1.8313e+00,  1.9361e-01, -1.4876e+00,\n",
      "            7.2517e-01,  1.6299e+00, -1.3873e-02, -1.0584e+00,  3.8128e-01]],\n",
      "\n",
      "         [[ 1.5151e+00, -1.0449e+00,  2.7594e-01,  3.5283e-03, -2.8024e-01,\n",
      "            7.6097e-01,  1.1745e-01,  9.3817e-01,  1.7238e+00, -9.4952e-01],\n",
      "          [ 7.9809e-01,  4.3932e-01,  1.2234e+00, -8.3913e-01,  1.0010e+00,\n",
      "            5.0341e-01,  1.0257e+00,  1.8867e-01, -3.2138e+00,  8.9081e-01],\n",
      "          [ 2.2877e-01,  1.4987e+00,  2.5069e-01, -8.6068e-01,  4.1341e-01,\n",
      "            7.9282e-01, -2.8243e-01,  8.2120e-01, -5.4806e-01,  6.0257e-01],\n",
      "          [ 6.8532e-01, -1.4095e+00,  7.2754e-01, -2.0656e+00,  6.7501e-01,\n",
      "           -7.2148e-01,  1.6716e-02,  9.6575e-01, -3.1726e-01, -2.9407e-01],\n",
      "          [-5.8347e-01,  1.7697e-01, -4.0804e-01,  5.9728e-01,  1.4215e+00,\n",
      "            2.5619e+00,  5.6832e-01,  2.6508e-01, -8.0327e-01, -7.1649e-02],\n",
      "          [-6.7638e-01, -2.2443e-01,  7.1073e-01, -9.0426e-01,  7.5416e-01,\n",
      "           -2.1715e-01, -2.2427e-01,  2.7627e+00,  2.0735e-01,  1.3852e-01],\n",
      "          [-5.5934e-01, -7.2236e-01, -5.2721e-01,  1.6941e-01, -6.6353e-01,\n",
      "           -7.1384e-01, -4.2123e-01,  1.6647e+00,  1.1550e+00, -7.8998e-01],\n",
      "          [-2.4336e-01,  5.6459e-01,  1.3020e+00, -1.8067e+00,  1.5036e+00,\n",
      "            1.2603e+00, -1.6753e+00,  6.8144e-01,  3.7590e-01, -3.6858e-01]],\n",
      "\n",
      "         [[-5.7133e-02, -7.8391e-01, -6.7565e-01,  3.6893e-01, -1.5935e+00,\n",
      "           -1.0632e+00,  2.2761e-02,  1.0975e+00, -5.3950e-01,  5.5317e-01],\n",
      "          [ 1.7054e-01, -1.4475e+00,  8.1970e-01,  5.6866e-01,  5.2043e-01,\n",
      "           -9.4337e-01,  3.2491e-01,  9.5332e-02,  1.9535e-01,  4.7094e-01],\n",
      "          [-5.4772e-01, -2.7574e+00, -8.0933e-01,  4.1492e-01,  6.3724e-01,\n",
      "           -4.8102e-01, -2.5236e-02, -1.5277e+00,  6.3482e-01, -6.3325e-01],\n",
      "          [ 8.7920e-01, -2.0710e+00,  7.9932e-01, -6.7280e-01,  1.0933e+00,\n",
      "            2.3772e+00,  8.4523e-02, -4.7193e-01, -2.2510e-01, -4.2846e-01],\n",
      "          [-5.2441e-01,  3.4448e-01,  1.1435e+00, -1.3574e+00, -1.8927e+00,\n",
      "            1.6143e+00, -6.4722e-01,  4.2009e-01, -6.2852e-01, -1.0523e+00],\n",
      "          [-1.3357e+00, -5.0477e-01,  6.0148e-01, -4.2558e-01,  6.1032e-01,\n",
      "           -6.6559e-01, -8.8760e-02,  1.3120e+00,  2.4307e-01,  1.9149e+00],\n",
      "          [-1.2483e-01, -4.2378e-01, -2.5322e-02,  3.3606e-01,  5.9717e-01,\n",
      "           -1.7549e-01, -6.2680e-01, -4.9626e-01, -2.0450e-01,  3.1016e-01],\n",
      "          [-2.1343e+00, -1.4973e-01,  1.6162e+00,  8.0887e-01,  1.7872e+00,\n",
      "            1.5459e+00,  2.7549e+00,  7.9589e-01, -1.1320e+00,  2.3798e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.4223e-01, -3.7779e-01,  8.0045e-01,  1.2963e+00, -5.4016e-01,\n",
      "           -2.6637e-01, -3.0841e-01, -1.1315e-01, -8.3665e-01,  7.3082e-01],\n",
      "          [ 4.6773e-01,  1.5867e-01,  8.1345e-01, -1.0678e+00,  2.3595e-01,\n",
      "           -2.1873e-01,  5.5032e-01,  1.4911e-01,  4.5032e-02,  4.7314e-01],\n",
      "          [ 1.4845e+00,  3.1432e-01, -1.4516e+00,  4.1872e-01,  2.5656e-02,\n",
      "            4.3786e-01,  9.1509e-03, -4.8457e-01, -1.9223e+00, -3.5816e-01],\n",
      "          [-3.3392e-01,  4.2820e-01, -8.6207e-01, -9.2914e-01, -1.3454e+00,\n",
      "           -2.4726e-01,  2.6606e-01, -2.8372e-01, -2.9777e-01, -1.2462e-01],\n",
      "          [ 7.8578e-01, -3.5027e-01, -1.3947e+00,  1.2900e+00,  5.9795e-02,\n",
      "           -2.6756e-01, -5.0997e-01,  3.4320e-01,  6.7359e-01, -1.1709e+00],\n",
      "          [-4.6354e-01, -3.5726e-01,  2.3241e-01,  2.7589e-01, -8.2812e-01,\n",
      "            2.9619e-01, -4.2467e-02,  1.2713e+00, -5.2280e-01,  1.8249e+00],\n",
      "          [ 3.9827e-01,  1.3684e-01, -4.2960e-01,  7.6773e-01, -1.5036e+00,\n",
      "           -1.5253e+00,  5.3395e-01, -1.3730e+00,  1.4114e+00,  4.5294e-01],\n",
      "          [ 1.0159e+00,  2.0662e+00,  1.3616e+00, -1.2856e-01, -3.9458e-02,\n",
      "            4.7141e-01,  1.0565e+00, -9.1752e-01, -2.5644e+00, -4.2384e-01]],\n",
      "\n",
      "         [[ 1.2131e+00, -1.0828e+00,  1.5966e+00,  8.6882e-01, -1.1150e+00,\n",
      "            1.8155e-01, -1.4870e-01, -7.4792e-01, -1.6967e+00,  1.5568e+00],\n",
      "          [-8.6378e-01, -9.6712e-01, -1.0450e+00, -1.0732e+00,  5.9485e-01,\n",
      "            1.8470e+00, -2.4489e-01, -1.0656e+00, -4.5546e-01,  1.2891e+00],\n",
      "          [ 1.0273e+00, -1.0032e+00,  1.1722e-01, -1.2443e+00,  7.6400e-01,\n",
      "           -1.2293e+00,  7.4594e-01,  8.5020e-01, -9.5707e-01,  2.2183e-01],\n",
      "          [-7.9178e-01, -3.9499e-01,  5.0978e-01, -5.8898e-01, -9.5525e-01,\n",
      "            2.9413e-02, -4.8209e-01,  2.7984e+00, -2.5365e-01, -1.7560e+00],\n",
      "          [-4.9133e-01, -4.3963e-01,  1.0095e+00,  5.1066e-01,  7.0106e-01,\n",
      "            2.7865e-01, -6.6767e-04,  1.3480e+00, -1.7513e+00, -1.6277e+00],\n",
      "          [-7.8468e-01,  1.6126e-01,  4.8972e-02,  7.0553e-01, -7.3181e-01,\n",
      "            3.5645e-01, -1.3779e+00, -1.0603e-01, -3.9783e-01,  8.6330e-01],\n",
      "          [ 1.4642e+00, -4.6952e-01,  1.7388e+00,  4.7799e-02,  5.3425e-02,\n",
      "           -4.4371e-01, -8.2609e-01,  5.7906e-01, -5.5100e-01, -6.2916e-02],\n",
      "          [-1.5814e+00,  6.7060e-01, -1.2270e+00,  5.9083e-01,  8.6641e-01,\n",
      "            3.0252e-01,  1.3201e+00,  3.9990e-01, -1.8944e+00,  6.2473e-01]],\n",
      "\n",
      "         [[-2.9063e-01, -2.7426e-01,  2.5092e+00,  1.6084e+00, -6.0558e-01,\n",
      "            1.3565e+00,  1.2666e+00,  5.1240e-02,  5.3060e-01,  2.0702e+00],\n",
      "          [ 1.8929e+00,  1.3487e+00,  1.2811e+00, -9.9127e-01, -1.0392e+00,\n",
      "            2.0950e-01, -1.6913e-01,  3.8978e-01, -1.4108e+00, -1.0922e+00],\n",
      "          [-8.1935e-02, -6.8265e-01, -1.8538e-01, -8.0794e-01,  1.1514e+00,\n",
      "            6.8059e-01,  3.1649e-01, -7.5252e-01,  2.9111e-01,  1.1630e+00],\n",
      "          [ 5.5220e-01,  6.0785e-01, -1.4269e+00, -6.8248e-01,  1.6161e+00,\n",
      "            5.4890e-01,  2.6557e-01, -1.5948e+00, -1.8116e+00,  2.5996e+00],\n",
      "          [-4.6918e-01,  5.3876e-01,  1.0505e+00, -3.3144e-01,  1.4343e+00,\n",
      "           -1.8226e+00,  8.7598e-01, -8.4355e-02, -7.3112e-01, -8.2565e-03],\n",
      "          [ 7.4671e-02,  2.0504e-01, -1.1351e+00,  9.2695e-01,  8.7382e-01,\n",
      "           -7.5705e-01,  3.4552e-01, -1.0719e+00, -4.8541e-01,  2.9636e-01],\n",
      "          [-9.1097e-01, -1.0465e+00,  3.1389e-01, -7.4815e-01, -6.5178e-01,\n",
      "            3.0405e-01,  2.2107e+00, -4.9369e-01, -7.8466e-01,  3.6343e-01],\n",
      "          [-1.3736e+00,  1.6014e+00,  1.4837e-01, -1.1962e+00,  1.0519e+00,\n",
      "           -8.2719e-01,  1.0049e+00,  1.1443e+00, -2.1030e+00,  2.8138e-01]]]]), tensor([[[[[ 3.7349e-01, -4.4557e-01, -7.3741e-01,  6.4713e-02,  4.7125e-01,\n",
      "             1.2197e-01, -5.7792e-01,  1.8606e-01, -3.9727e-01,  1.0313e+00],\n",
      "           [ 1.2780e+00,  1.2660e+00,  2.1897e-01,  4.8771e-01, -1.1221e+00,\n",
      "            -6.3617e-01, -1.0951e+00,  2.2460e-01, -1.8984e+00, -1.9205e-01],\n",
      "           [-1.6978e+00,  1.2801e+00, -9.0272e-01, -1.1262e+00, -5.1757e-01,\n",
      "             7.2192e-01, -2.0901e-01,  8.4134e-01, -3.4566e-01,  1.3265e+00],\n",
      "           [-8.1135e-01,  1.4191e-01,  6.8654e-01,  1.0282e+00, -1.2215e+00,\n",
      "            -5.4647e-01,  7.1155e-01, -3.0461e+00,  5.8498e-01,  1.1101e+00],\n",
      "           [ 2.8919e-01,  8.7093e-01,  3.8489e-01,  9.3462e-02,  1.2283e+00,\n",
      "            -5.9841e-01,  1.0284e+00, -7.0598e-01, -2.3404e-01, -4.8586e-01],\n",
      "           [-1.8171e+00, -2.5543e+00, -3.6013e-01,  5.2305e-01, -1.0733e+00,\n",
      "             1.7607e+00,  7.2762e-01, -2.1016e-01,  1.6916e+00,  1.8796e-01],\n",
      "           [ 1.5571e+00,  4.4529e-01, -6.4810e-01, -3.5690e-01,  1.4198e+00,\n",
      "             1.1786e+00, -9.5598e-02,  9.1083e-01, -4.0572e-01, -7.4477e-01],\n",
      "           [ 2.4208e+00,  1.1836e-01, -2.1928e+00,  1.3369e+00, -6.4571e-01,\n",
      "            -3.5330e-01, -8.7507e-01,  1.2675e+00, -1.7725e+00,  8.5072e-01]],\n",
      "\n",
      "          [[-1.3246e+00, -6.6746e-01, -4.3654e-01,  6.1968e-01, -6.8081e-01,\n",
      "            -6.6096e-01,  1.0008e+00,  8.9480e-01,  3.3196e-02,  2.5473e-01],\n",
      "           [ 1.6565e+00,  8.8695e-02, -5.5112e-01,  7.8145e-01,  1.7372e+00,\n",
      "             5.5380e-01,  4.1281e-01,  1.7134e-01,  7.4919e-01,  6.8188e-01],\n",
      "           [-9.9953e-01, -7.6207e-01, -1.8918e+00, -5.5618e-01,  1.1414e+00,\n",
      "            -5.7385e-01, -9.6935e-01, -3.3052e-01, -1.3557e+00,  6.6680e-02],\n",
      "           [-2.1196e-01, -2.2527e+00, -4.8659e-01, -5.8973e-02,  1.8539e+00,\n",
      "             1.7399e-01,  4.8281e-01,  8.7823e-01, -2.1491e+00,  1.3882e+00],\n",
      "           [ 3.6674e-01,  8.3696e-01, -1.1936e+00, -7.4614e-02,  7.2006e-01,\n",
      "             3.2477e-01, -2.5121e-01, -2.6451e-01,  1.2931e+00, -6.0575e-01],\n",
      "           [ 1.2845e+00, -1.8261e-01, -1.5155e+00,  3.1066e-01,  1.4477e+00,\n",
      "             3.0233e-01,  1.4707e-01,  7.6862e-01, -5.5063e-01,  1.2463e+00],\n",
      "           [ 5.2499e-02,  1.1351e-02,  7.5683e-01,  7.1753e-01,  6.4402e-01,\n",
      "             6.7828e-01,  1.4600e-01,  1.2056e+00,  7.0340e-01, -1.1971e+00],\n",
      "           [ 1.3299e+00, -1.9647e-01, -7.3509e-02,  5.1983e-01,  1.4229e-01,\n",
      "            -1.3363e+00,  1.3377e-01,  1.3446e+00, -1.9757e+00, -1.1552e+00]]],\n",
      "\n",
      "\n",
      "         [[[-4.5626e-01, -1.0551e+00,  6.0665e-01,  5.6149e-01,  1.0153e+00,\n",
      "             1.7172e+00,  1.5277e+00,  4.9633e-01,  1.2547e-01,  7.6909e-01],\n",
      "           [-2.3136e+00, -8.4046e-02,  1.6352e+00,  4.1975e-01, -6.5709e-02,\n",
      "             7.1443e-01, -9.4015e-02, -4.9963e-01, -1.9273e-01,  1.1849e-01],\n",
      "           [ 1.8586e+00,  1.2937e+00, -1.0211e+00,  1.7765e+00, -2.9161e-01,\n",
      "             7.4759e-01,  1.2440e+00,  2.4464e-01,  8.4464e-01,  5.7271e-01],\n",
      "           [ 3.3678e+00,  8.3522e-01, -8.6784e-01,  1.8851e+00,  1.9595e+00,\n",
      "            -2.2429e+00, -6.1363e-01,  6.0984e-02, -1.0715e+00, -1.6556e+00],\n",
      "           [ 3.4502e-01,  1.0585e+00, -4.0265e-01, -2.3445e+00, -3.3589e-01,\n",
      "            -1.2542e-01, -3.1802e-01, -5.3302e-01, -2.5139e+00, -1.7272e-01],\n",
      "           [ 4.4646e-01,  1.0396e-01,  1.8033e-01, -4.0881e-01,  5.7981e-01,\n",
      "             8.8872e-01, -7.1871e-01,  9.0646e-01, -1.0993e+00,  2.8732e-01],\n",
      "           [ 8.9061e-01,  1.3335e+00, -1.0785e-01,  1.6658e+00,  2.8518e-01,\n",
      "            -5.5235e-01, -2.5891e+00, -5.6767e-02, -1.3242e-01,  1.4363e-01],\n",
      "           [-1.1481e+00,  3.5671e-01, -7.4208e-01, -9.0344e-01,  1.9477e-01,\n",
      "            -9.0989e-02, -3.0497e-01, -1.1987e+00,  1.5868e-01, -5.2052e-01]],\n",
      "\n",
      "          [[-5.0092e-01, -1.0636e+00, -4.7443e-01,  1.8622e+00,  3.8223e-01,\n",
      "            -9.6457e-01,  3.0535e-01, -9.5080e-01,  3.3825e+00,  7.7563e-01],\n",
      "           [ 1.5355e-01,  2.4459e+00, -4.6707e-01, -2.2704e+00, -5.6358e-01,\n",
      "            -2.2278e+00,  2.1304e-01,  1.1220e+00, -1.0448e+00,  1.2461e+00],\n",
      "           [ 8.0058e-01,  7.0255e-01, -5.5253e-01,  1.2801e-01,  9.5636e-01,\n",
      "             1.4050e+00,  4.0787e-01,  6.0691e-01, -2.5627e-01,  1.0854e-01],\n",
      "           [ 9.8410e-02, -8.1039e-01,  9.5053e-01,  1.2417e-01, -2.1522e-01,\n",
      "            -1.2576e+00, -7.2318e-01, -9.6567e-01,  1.3955e+00,  4.2122e-01],\n",
      "           [ 9.2080e-01,  1.3752e+00, -1.2042e-01,  3.1351e-01,  1.1581e+00,\n",
      "            -1.4073e+00,  1.3677e+00, -1.1750e-01,  1.7527e-01, -6.6062e-01],\n",
      "           [ 3.6620e-01, -2.2756e-01,  2.9968e-01,  1.2259e+00,  6.3794e-01,\n",
      "             2.0929e+00,  1.4404e+00, -1.7188e-01, -1.1023e+00,  2.7225e+00],\n",
      "           [ 5.7363e-01,  3.1734e-01, -1.4161e-01,  5.0543e-01,  6.7748e-01,\n",
      "             6.9826e-01,  1.4219e-01, -1.7961e+00,  9.7755e-01, -4.4165e-01],\n",
      "           [-8.6433e-01,  3.4707e-01,  2.1140e+00,  8.9582e-01, -1.0211e+00,\n",
      "             1.2485e-01,  5.4277e-01, -8.3908e-01,  1.5419e+00,  1.0797e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 2.1623e+00, -1.2119e+00, -1.0678e-01,  8.1100e-01,  6.9367e-02,\n",
      "             2.4451e+00, -5.6697e-01, -2.9384e-01,  9.4926e-03, -2.8926e+00],\n",
      "           [-8.9127e-01, -1.4264e+00, -1.6317e+00, -1.2916e+00, -1.0804e-01,\n",
      "             1.9080e-01, -4.8974e-01, -5.0943e-01,  1.2014e+00, -6.6851e-01],\n",
      "           [ 4.4779e-01,  3.4171e-01, -1.0325e+00,  1.7003e+00, -1.2952e+00,\n",
      "             7.1683e-02,  9.0602e-01, -1.3359e-01,  6.4948e-01, -3.2345e-01],\n",
      "           [-1.3828e-01, -9.6416e-01,  4.4108e-01,  5.3136e-01, -5.3071e-03,\n",
      "            -1.6483e+00, -9.8758e-03, -2.6214e-01,  1.1966e+00,  8.9261e-01],\n",
      "           [ 8.0850e-01,  9.2652e-02,  8.2905e-01,  8.5779e-01,  3.3537e-01,\n",
      "            -3.6625e-01,  5.2573e-01,  7.6012e-01, -5.5303e-01,  4.6499e-01],\n",
      "           [ 2.6987e-01, -8.6436e-01, -3.9810e-03, -7.7834e-01,  1.4278e+00,\n",
      "             4.6438e-01,  1.4670e+00, -8.1965e-02,  1.3494e+00, -5.6719e-02],\n",
      "           [-1.0442e+00,  8.0556e-01, -8.5251e-01, -2.1086e+00, -9.0231e-01,\n",
      "             2.2859e-01,  1.2553e-01, -1.2743e-01, -1.4558e+00,  8.3512e-01],\n",
      "           [ 1.1660e-01,  5.7177e-01, -1.1823e+00, -4.2832e-01, -1.5356e+00,\n",
      "            -4.7473e-01, -3.6535e-01,  1.7113e-01, -1.4585e+00,  4.7909e-02]],\n",
      "\n",
      "          [[-1.2971e+00, -7.1298e-01,  4.2676e-01,  7.6616e-01, -4.1008e-01,\n",
      "             2.7177e-01, -3.8115e-01,  1.1023e+00,  2.0656e-01, -7.1776e-01],\n",
      "           [ 2.2487e+00, -5.7498e-01, -2.0355e+00, -1.0139e+00,  5.2553e-01,\n",
      "            -1.7078e+00, -6.7653e-01,  4.5942e-01,  7.1840e-01, -1.1195e-02],\n",
      "           [ 3.0838e-01, -3.3321e-01, -5.1502e-01,  1.6506e-01,  2.1446e-01,\n",
      "            -1.2311e+00, -2.1184e+00, -1.2082e+00, -6.7935e-01,  3.3924e-01],\n",
      "           [ 4.5568e-01,  1.8040e+00, -2.4347e+00, -1.9434e-01, -1.2862e+00,\n",
      "            -1.0508e+00,  9.6088e-01,  1.4329e-01,  1.3035e+00,  7.5938e-01],\n",
      "           [-1.0038e+00,  2.4172e+00, -8.2505e-01,  1.5939e+00, -4.8262e-01,\n",
      "             8.7404e-02, -1.3861e+00,  6.7300e-01, -9.7816e-01,  7.8547e-01],\n",
      "           [-6.9721e-02,  3.7343e-01, -4.4460e-01,  1.3504e+00,  1.7615e+00,\n",
      "            -1.1563e-01,  2.2578e-01, -3.0732e-01, -2.4746e+00,  7.7672e-01],\n",
      "           [ 4.9252e-01,  7.2883e-01,  8.9131e-01, -1.5451e+00, -4.3001e-01,\n",
      "            -1.3132e+00, -7.0943e-01,  3.1252e-01,  1.3704e+00, -2.8638e-02],\n",
      "           [-1.4189e-01, -2.6601e-02,  1.6035e-01,  7.4743e-01, -1.2210e+00,\n",
      "            -3.7197e-01,  8.4531e-01,  7.4499e-01,  9.2444e-01,  9.6459e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.0659e+00, -4.9811e-01, -1.8228e-02, -1.0474e+00, -2.4198e-01,\n",
      "             9.1734e-01, -1.2854e+00,  6.1097e-01,  3.6098e-01, -7.4825e-01],\n",
      "           [ 5.7782e-01,  3.2471e-01, -6.5524e-01, -6.2836e-01,  2.1146e+00,\n",
      "            -1.4384e+00, -3.1632e-01, -1.6592e-02, -7.6946e-01, -5.6411e-02],\n",
      "           [ 2.4770e+00, -5.7238e-01,  5.5372e-01, -1.7275e+00,  8.0800e-01,\n",
      "             3.0200e-01,  2.3044e-01, -2.2055e+00,  2.7947e-02,  9.9119e-01],\n",
      "           [ 1.2375e+00, -3.8173e-01, -6.8725e-01, -3.8795e-01,  6.2958e-01,\n",
      "            -4.5196e-02, -4.5320e-01, -1.2012e+00, -5.2673e-01,  1.7431e-01],\n",
      "           [-1.2894e+00,  9.7506e-02, -2.5092e-01, -3.7552e-01, -4.4429e-01,\n",
      "             2.3598e-01, -1.1307e+00,  1.5688e+00, -2.0188e+00,  1.1644e+00],\n",
      "           [ 4.4415e-01, -1.6378e-01,  8.0927e-01,  1.9659e-01,  3.7176e-01,\n",
      "            -9.6795e-01, -3.8001e-01,  1.7975e+00, -3.5754e-01, -1.7508e+00],\n",
      "           [-1.5674e+00,  8.9660e-02,  7.4230e-01,  9.0849e-01,  4.2585e-01,\n",
      "            -3.3110e-01, -9.3222e-01, -2.9560e-01,  1.4780e-03,  1.8149e-01],\n",
      "           [-2.2707e-01,  6.3243e-01, -5.4466e-01,  1.2422e-01,  1.1184e+00,\n",
      "             1.6206e-02, -2.6323e-01, -1.2564e+00,  1.3303e+00,  7.5098e-01]],\n",
      "\n",
      "          [[-1.3047e+00, -5.1675e-01, -1.5866e+00,  2.3112e+00,  3.8405e-01,\n",
      "             1.7155e-01,  2.1392e-01,  3.0985e-01, -1.7499e+00,  9.1463e-01],\n",
      "           [-1.7998e-01, -4.3397e-01, -1.0544e+00, -9.6166e-01,  5.2026e-02,\n",
      "            -1.5351e+00,  1.2239e+00, -7.2293e-01, -1.7664e+00,  1.2737e+00],\n",
      "           [-8.9913e-01, -6.8019e-01,  1.8753e-01,  1.6096e+00,  2.0385e+00,\n",
      "             3.3596e-01, -1.7843e+00, -6.8336e-01,  2.3292e-01,  9.5584e-02],\n",
      "           [-1.4682e+00, -6.7262e-01, -3.8149e-01,  4.3683e-01, -1.7727e+00,\n",
      "             1.9916e-01, -3.8810e-01,  4.6811e-01, -3.0797e-01, -1.2306e+00],\n",
      "           [ 5.0314e-01,  8.0165e-01,  8.3775e-01, -2.7057e-01, -6.2376e-01,\n",
      "             2.5933e+00, -2.6015e-01,  1.1675e+00, -1.7194e+00, -4.3247e-01],\n",
      "           [ 6.9233e-01,  1.0403e+00,  4.9663e-01, -3.5531e-01,  4.7269e-01,\n",
      "            -2.3912e+00, -2.0241e-01,  4.9756e-02,  4.0402e-01,  6.1235e-01],\n",
      "           [ 7.2895e-01,  7.2588e-01,  4.9210e-01,  7.8180e-01,  1.0545e+00,\n",
      "            -1.1894e+00, -2.6261e+00, -1.2145e-01, -2.1856e+00,  1.9874e+00],\n",
      "           [-3.2502e-01,  2.0562e-01, -7.8485e-01, -3.6451e-01,  5.9963e-01,\n",
      "            -3.9945e-01, -1.3619e+00,  4.7574e-01,  4.2311e-01,  1.0862e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 2.6117e+00,  7.7322e-01, -9.6780e-01, -9.3376e-01,  2.0288e+00,\n",
      "             5.7425e-01,  8.4799e-01, -7.5456e-02,  2.4858e-01, -1.2127e+00],\n",
      "           [-1.2660e+00, -7.0692e-01,  3.9956e-01,  1.1760e-01,  1.1392e+00,\n",
      "            -4.3989e-01, -2.2073e-01, -1.4558e-01,  2.7766e-01, -9.8439e-01],\n",
      "           [ 3.8919e-01,  2.1157e+00,  1.0144e+00, -1.8980e-01,  9.8890e-01,\n",
      "            -2.0005e+00, -3.0907e-01,  1.1394e+00, -1.4198e+00,  2.6497e-02],\n",
      "           [-1.8743e+00, -3.8502e-01,  1.2880e+00,  1.1719e+00,  3.0350e-01,\n",
      "             9.5982e-01, -2.0876e+00, -3.6652e-01, -3.8200e-01,  1.7907e+00],\n",
      "           [-1.7691e-01, -1.3783e+00,  6.6604e-02,  6.4003e-01,  4.2798e-01,\n",
      "             2.3030e+00,  1.2111e+00, -1.1599e+00,  1.7125e-01, -3.1849e-01],\n",
      "           [ 2.6220e+00,  7.7046e-01, -1.3689e+00,  3.9706e-01,  2.0507e+00,\n",
      "             3.0597e+00,  6.6251e-01,  1.3459e+00,  5.5303e-01,  1.3568e-01],\n",
      "           [ 2.1643e+00,  1.8202e+00,  1.8057e+00,  4.0620e-01, -1.5070e-01,\n",
      "             1.8392e+00,  2.4204e-01, -1.8340e+00, -8.1682e-01,  3.0772e-01],\n",
      "           [-2.1256e-01, -4.2113e-01, -5.6289e-01,  7.5485e-01, -7.9843e-01,\n",
      "            -6.9458e-01,  3.1044e-01, -6.3307e-01,  4.9788e-01, -1.1856e+00]],\n",
      "\n",
      "          [[-1.1527e+00,  1.4407e+00,  1.3798e+00, -4.2144e-01,  3.5425e-01,\n",
      "             3.2301e-01,  1.0679e+00,  2.3044e-01,  9.1832e-01,  5.0247e-01],\n",
      "           [ 1.6276e+00,  7.8384e-01,  1.6974e+00,  1.7871e+00,  1.2511e+00,\n",
      "            -4.9467e-02, -3.9888e-01,  1.1200e+00, -1.0775e+00, -7.0879e-01],\n",
      "           [ 4.4270e-01, -1.9154e-01, -3.5654e-01, -1.5000e+00, -6.2318e-02,\n",
      "             4.7171e-01,  7.5786e-01,  1.7258e-01,  1.9460e-01, -3.7177e-01],\n",
      "           [ 7.2385e-01,  7.1054e-02, -1.6552e-01,  5.6950e-01,  2.3623e+00,\n",
      "             1.6979e+00,  1.2810e+00,  7.1481e-01, -1.5503e+00,  1.2196e+00],\n",
      "           [ 2.3551e+00, -1.0987e+00,  1.0211e+00,  1.4561e-01,  6.8418e-01,\n",
      "             1.2531e+00,  5.9381e-01, -4.2816e-01,  8.9466e-01,  9.4043e-01],\n",
      "           [ 7.0115e-01,  8.8397e-01,  2.2499e+00,  1.3411e+00,  5.7726e-01,\n",
      "            -2.4119e+00,  4.4347e-01,  7.1758e-01,  1.2807e+00, -2.9569e+00],\n",
      "           [-8.4300e-01,  2.6469e-02, -1.1090e+00, -1.8489e+00, -1.2594e+00,\n",
      "            -2.5498e-01, -4.9330e-01,  4.6608e-01,  1.2950e+00, -1.9702e-01],\n",
      "           [ 9.2274e-02,  5.8116e-01, -1.4246e+00,  1.0167e+00, -4.6983e-01,\n",
      "            -1.9479e+00,  1.4021e+00, -3.4657e-01,  7.8676e-01,  1.1532e+00]]],\n",
      "\n",
      "\n",
      "         [[[ 5.9593e-01, -1.2719e+00,  2.0284e-01,  4.8720e-01, -9.0565e-02,\n",
      "             8.3780e-01, -1.2149e+00,  1.5122e+00, -5.0957e-02, -6.0528e-01],\n",
      "           [ 1.1725e+00,  1.4964e+00,  4.6633e-01, -2.8616e-01,  8.3302e-01,\n",
      "             1.8390e+00, -3.6981e-02,  7.6969e-02, -1.7477e+00, -5.9216e-01],\n",
      "           [-5.9228e-01, -1.3773e+00,  1.6588e+00, -4.6207e-01, -7.1827e-01,\n",
      "            -2.2943e-01,  2.7561e-01,  1.5160e+00, -2.0066e-01, -6.1942e-01],\n",
      "           [ 7.9148e-01,  1.6991e+00, -6.2765e-01,  2.2201e-01, -1.5694e-01,\n",
      "             1.3882e+00, -1.7632e+00,  4.9741e-01,  1.4233e-01, -1.0047e-01],\n",
      "           [ 7.2092e-01, -5.6314e-02,  9.0547e-01, -2.0511e-02,  2.9248e-01,\n",
      "            -1.7269e+00, -1.2919e+00, -1.6610e+00,  1.6291e+00, -6.4545e-01],\n",
      "           [ 1.4884e+00, -1.7333e-02,  1.4894e-01, -4.8060e-01, -4.0515e-01,\n",
      "             1.4978e-01,  1.1897e+00, -1.9695e+00, -2.1159e-01,  8.7227e-01],\n",
      "           [ 7.7702e-01, -2.1860e-01,  2.0425e+00, -1.3200e-01, -1.5580e-01,\n",
      "             1.0954e+00, -2.4898e-01, -4.8119e-01,  3.5016e-01, -8.2236e-01],\n",
      "           [ 3.7257e-01, -3.1210e-01,  1.3177e+00,  1.6629e+00, -2.4588e+00,\n",
      "             9.8168e-02,  1.5477e+00,  1.4497e+00,  5.3947e-01, -1.0303e+00]],\n",
      "\n",
      "          [[ 2.3920e-01, -4.7171e-01, -2.7702e-01,  1.6098e+00,  7.9546e-01,\n",
      "            -5.9967e-01, -2.7175e+00,  2.2654e+00, -5.0596e-01,  5.6519e-01],\n",
      "           [ 5.6013e-01,  1.0482e+00, -7.4022e-01, -1.0899e-01, -8.4715e-01,\n",
      "            -2.5134e-02, -1.1212e+00, -1.0617e+00, -3.5753e-01, -5.7875e-01],\n",
      "           [ 2.2244e-01, -1.5390e-01,  3.4053e-01,  1.1534e+00,  2.3695e+00,\n",
      "            -2.0675e+00, -6.7995e-01, -4.8299e-01, -1.2507e+00, -6.5151e-01],\n",
      "           [-1.0089e-01,  8.2805e-01, -2.4395e+00, -8.7834e-01, -2.1008e-01,\n",
      "             6.9324e-01, -2.5948e-01, -3.8028e-01, -2.0346e+00,  8.0077e-01],\n",
      "           [-1.0926e+00,  6.0097e-01, -2.1212e-01,  9.8800e-02,  1.2240e+00,\n",
      "            -7.2123e-01,  1.2400e+00, -2.2163e+00,  5.2217e-01, -1.2527e+00],\n",
      "           [-1.5192e+00,  8.7499e-01,  1.5090e+00, -1.8473e-01,  2.8272e-01,\n",
      "            -3.8110e-01,  1.7345e-01,  4.4088e-02,  7.2310e-01,  1.9719e+00],\n",
      "           [-1.7972e+00, -2.1262e+00, -1.8391e+00, -3.9606e-01, -8.4972e-01,\n",
      "             8.0497e-01,  2.7831e-01, -2.8136e-01,  1.3115e-01,  1.4980e+00],\n",
      "           [ 1.1510e-01, -7.8747e-01, -2.4430e+00, -3.7224e+00,  1.1557e-01,\n",
      "            -4.2160e-01, -1.0026e-01, -3.6730e-01, -1.4687e+00,  1.7687e-01]]]]]))\n",
      "-----\n",
      "case 2\n",
      "x.shape: (2, 3, 8, 10)\n",
      "(2, 3, 8, 3)\n",
      "(2, 3, 8, 3)\n",
      "(3, 1, 3)\n",
      "(3, 1, 3)\n",
      "(<tf.Tensor: shape=(2, 3, 8, 10), dtype=float32, numpy=\n",
      "array([[[[-1.89375913e+00, -4.10285443e-01, -8.84207368e-01,\n",
      "          -1.93039179e-01, -1.21399969e-01, -1.05391490e+00,\n",
      "          -1.48243317e-02, -1.57895889e-02,  6.09565079e-01,\n",
      "          -1.36937284e+00],\n",
      "         [ 8.09371710e-01,  1.11645365e+00, -1.28075853e-01,\n",
      "           1.99521852e+00,  5.90323269e-01, -9.38135386e-01,\n",
      "          -5.78513071e-02,  1.32585561e+00,  1.05718531e-01,\n",
      "          -6.20149016e-01],\n",
      "         [ 6.20958924e-01,  5.41548491e-01, -2.17470780e-01,\n",
      "          -4.68481600e-01, -4.11883816e-02,  9.91073012e-01,\n",
      "          -1.86496985e+00,  3.32804292e-01, -1.65016127e+00,\n",
      "          -2.01974940e+00],\n",
      "         [ 2.39093736e-01,  6.37647390e-01, -7.60452002e-02,\n",
      "           5.68654954e-01, -2.56657362e+00,  1.54877290e-01,\n",
      "           3.19943696e-01, -3.41964304e-01, -1.08728014e-01,\n",
      "          -6.49622738e-01],\n",
      "         [-4.89213258e-01, -2.86360621e-01,  3.65752578e-01,\n",
      "           1.33992910e+00,  2.56970263e+00,  3.57614905e-01,\n",
      "          -3.17458242e-01,  1.02713025e+00, -5.45833409e-01,\n",
      "          -7.20600009e-01],\n",
      "         [ 1.16106606e+00,  7.61733770e-01, -6.93562925e-01,\n",
      "           2.61043310e-02, -1.06908286e+00,  1.63176525e+00,\n",
      "          -2.10808486e-01, -2.51939225e+00, -5.98178148e-01,\n",
      "           8.98438990e-01],\n",
      "         [-3.04188108e+00, -4.94711936e-01, -1.28806901e+00,\n",
      "          -2.21128583e-01,  1.20740697e-01, -2.94125229e-01,\n",
      "           1.59137881e+00,  1.24933267e+00,  7.99323440e-01,\n",
      "           8.80000517e-02],\n",
      "         [-1.42425597e-01, -2.15519086e-01, -1.83125234e+00,\n",
      "           1.93608314e-01, -1.48759568e+00,  7.25169599e-01,\n",
      "           1.62985480e+00, -1.38730127e-02, -1.05837774e+00,\n",
      "           3.81278217e-01]],\n",
      "\n",
      "        [[ 1.51513040e+00, -1.04489720e+00,  2.75939494e-01,\n",
      "           3.52829695e-03, -2.80239999e-01,  7.60973990e-01,\n",
      "           1.17449120e-01,  9.38170493e-01,  1.72381806e+00,\n",
      "          -9.49522853e-01],\n",
      "         [ 7.98091769e-01,  4.39316511e-01,  1.22340667e+00,\n",
      "          -8.39128673e-01,  1.00099945e+00,  5.03411829e-01,\n",
      "           1.02566266e+00,  1.88669577e-01, -3.21382499e+00,\n",
      "           8.90814364e-01],\n",
      "         [ 2.28765920e-01,  1.49874520e+00,  2.50690669e-01,\n",
      "          -8.60679686e-01,  4.13411379e-01,  7.92822242e-01,\n",
      "          -2.82426655e-01,  8.21204782e-01, -5.48059464e-01,\n",
      "           6.02571189e-01],\n",
      "         [ 6.85317636e-01, -1.40947199e+00,  7.27538407e-01,\n",
      "          -2.06562448e+00,  6.75014257e-01, -7.21480131e-01,\n",
      "           1.67156085e-02,  9.65754151e-01, -3.17259371e-01,\n",
      "          -2.94065505e-01],\n",
      "         [-5.83471358e-01,  1.76971629e-01, -4.08036947e-01,\n",
      "           5.97282708e-01,  1.42148852e+00,  2.56189156e+00,\n",
      "           5.68322480e-01,  2.65082270e-01, -8.03267062e-01,\n",
      "          -7.16491044e-02],\n",
      "         [-6.76383495e-01, -2.24430919e-01,  7.10728586e-01,\n",
      "          -9.04264092e-01,  7.54158497e-01, -2.17147395e-01,\n",
      "          -2.24271566e-01,  2.76269794e+00,  2.07349122e-01,\n",
      "           1.38520375e-01],\n",
      "         [-5.59342384e-01, -7.22361088e-01, -5.27205288e-01,\n",
      "           1.69412822e-01, -6.63533449e-01, -7.13843882e-01,\n",
      "          -4.21230704e-01,  1.66469181e+00,  1.15497828e+00,\n",
      "          -7.89977729e-01],\n",
      "         [-2.43355155e-01,  5.64586580e-01,  1.30197680e+00,\n",
      "          -1.80673862e+00,  1.50359273e+00,  1.26029348e+00,\n",
      "          -1.67527235e+00,  6.81435227e-01,  3.75900924e-01,\n",
      "          -3.68575722e-01]],\n",
      "\n",
      "        [[-5.71327806e-02, -7.83907533e-01, -6.75645411e-01,\n",
      "           3.68925989e-01, -1.59350240e+00, -1.06320429e+00,\n",
      "           2.27613337e-02,  1.09754384e+00, -5.39502382e-01,\n",
      "           5.53167880e-01],\n",
      "         [ 1.70543864e-01, -1.44746649e+00,  8.19703639e-01,\n",
      "           5.68664670e-01,  5.20434260e-01, -9.43373501e-01,\n",
      "           3.24906379e-01,  9.53324065e-02,  1.95347041e-01,\n",
      "           4.70941126e-01],\n",
      "         [-5.47720194e-01, -2.75742316e+00, -8.09326708e-01,\n",
      "           4.14924026e-01,  6.37242854e-01, -4.81016725e-01,\n",
      "          -2.52357405e-02, -1.52769601e+00,  6.34823024e-01,\n",
      "          -6.33253217e-01],\n",
      "         [ 8.79199088e-01, -2.07104397e+00,  7.99317598e-01,\n",
      "          -6.72798157e-01,  1.09328675e+00,  2.37717962e+00,\n",
      "           8.45229030e-02, -4.71930087e-01, -2.25097060e-01,\n",
      "          -4.28464472e-01],\n",
      "         [-5.24409354e-01,  3.44481856e-01,  1.14354146e+00,\n",
      "          -1.35744655e+00, -1.89271247e+00,  1.61427951e+00,\n",
      "          -6.47220731e-01,  4.20086324e-01, -6.28522038e-01,\n",
      "          -1.05231977e+00],\n",
      "         [-1.33570123e+00, -5.04766583e-01,  6.01476014e-01,\n",
      "          -4.25576985e-01,  6.10315144e-01, -6.65592253e-01,\n",
      "          -8.87598619e-02,  1.31197131e+00,  2.43073285e-01,\n",
      "           1.91488683e+00],\n",
      "         [-1.24831788e-01, -4.23780382e-01, -2.53218841e-02,\n",
      "           3.36059481e-01,  5.97170591e-01, -1.75486967e-01,\n",
      "          -6.26803279e-01, -4.96256262e-01, -2.04495385e-01,\n",
      "           3.10163856e-01],\n",
      "         [-2.13432360e+00, -1.49725914e-01,  1.61619282e+00,\n",
      "           8.08873534e-01,  1.78720605e+00,  1.54587376e+00,\n",
      "           2.75486231e+00,  7.95891941e-01, -1.13201070e+00,\n",
      "           2.37983927e-01]]],\n",
      "\n",
      "\n",
      "       [[[-8.42225373e-01, -3.77789289e-01,  8.00446987e-01,\n",
      "           1.29630995e+00, -5.40164649e-01, -2.66368121e-01,\n",
      "          -3.08414489e-01, -1.13147832e-01, -8.36650670e-01,\n",
      "           7.30824828e-01],\n",
      "         [ 4.67734039e-01,  1.58670604e-01,  8.13449919e-01,\n",
      "          -1.06780756e+00,  2.35948935e-01, -2.18731850e-01,\n",
      "           5.50318658e-01,  1.49106011e-01,  4.50317375e-02,\n",
      "           4.73144203e-01],\n",
      "         [ 1.48449683e+00,  3.14320654e-01, -1.45155942e+00,\n",
      "           4.18719232e-01,  2.56559718e-02,  4.37856615e-01,\n",
      "           9.15085524e-03, -4.84571397e-01, -1.92234504e+00,\n",
      "          -3.58163267e-01],\n",
      "         [-3.33922595e-01,  4.28199291e-01, -8.62071931e-01,\n",
      "          -9.29139256e-01, -1.34539342e+00, -2.47256041e-01,\n",
      "           2.66056657e-01, -2.83720881e-01, -2.97772646e-01,\n",
      "          -1.24624759e-01],\n",
      "         [ 7.85781741e-01, -3.50265026e-01, -1.39474916e+00,\n",
      "           1.29001796e+00,  5.97953610e-02, -2.67561495e-01,\n",
      "          -5.09974718e-01,  3.43199521e-01,  6.73587501e-01,\n",
      "          -1.17092347e+00],\n",
      "         [-4.63540643e-01, -3.57264996e-01,  2.32411072e-01,\n",
      "           2.75893867e-01, -8.28116298e-01,  2.96189278e-01,\n",
      "          -4.24667932e-02,  1.27127862e+00, -5.22800922e-01,\n",
      "           1.82491970e+00],\n",
      "         [ 3.98265839e-01,  1.36840612e-01, -4.29597586e-01,\n",
      "           7.67730653e-01, -1.50359523e+00, -1.52528453e+00,\n",
      "           5.33945441e-01, -1.37300003e+00,  1.41136646e+00,\n",
      "           4.52942908e-01],\n",
      "         [ 1.01585793e+00,  2.06624866e+00,  1.36162865e+00,\n",
      "          -1.28557563e-01, -3.94581258e-02,  4.71406192e-01,\n",
      "           1.05647004e+00, -9.17523086e-01, -2.56437874e+00,\n",
      "          -4.23837245e-01]],\n",
      "\n",
      "        [[ 1.21310365e+00, -1.08280766e+00,  1.59657383e+00,\n",
      "           8.68818879e-01, -1.11503196e+00,  1.81547076e-01,\n",
      "          -1.48696855e-01, -7.47922361e-01, -1.69674671e+00,\n",
      "           1.55681956e+00],\n",
      "         [-8.63777280e-01, -9.67118382e-01, -1.04503679e+00,\n",
      "          -1.07318377e+00,  5.94852507e-01,  1.84700489e+00,\n",
      "          -2.44891465e-01, -1.06559932e+00, -4.55456376e-01,\n",
      "           1.28911602e+00],\n",
      "         [ 1.02726042e+00, -1.00315130e+00,  1.17216818e-01,\n",
      "          -1.24433005e+00,  7.64003396e-01, -1.22925794e+00,\n",
      "           7.45936930e-01,  8.50203931e-01, -9.57067728e-01,\n",
      "           2.21831813e-01],\n",
      "         [-7.91776419e-01, -3.94985020e-01,  5.09779572e-01,\n",
      "          -5.88981509e-01, -9.55254793e-01,  2.94125900e-02,\n",
      "          -4.82094914e-01,  2.79844046e+00, -2.53650278e-01,\n",
      "          -1.75601029e+00],\n",
      "         [-4.91334498e-01, -4.39633250e-01,  1.00954366e+00,\n",
      "           5.10661006e-01,  7.01062858e-01,  2.78652370e-01,\n",
      "          -6.67666958e-04,  1.34798646e+00, -1.75127172e+00,\n",
      "          -1.62767601e+00],\n",
      "         [-7.84682870e-01,  1.61256045e-01,  4.89716008e-02,\n",
      "           7.05526769e-01, -7.31809020e-01,  3.56447786e-01,\n",
      "          -1.37790716e+00, -1.06030658e-01, -3.97828758e-01,\n",
      "           8.63301277e-01],\n",
      "         [ 1.46419621e+00, -4.69517142e-01,  1.73882234e+00,\n",
      "           4.77992892e-02,  5.34248129e-02, -4.43710208e-01,\n",
      "          -8.26089203e-01,  5.79063833e-01, -5.50998986e-01,\n",
      "          -6.29159138e-02],\n",
      "         [-1.58143568e+00,  6.70597076e-01, -1.22695029e+00,\n",
      "           5.90826154e-01,  8.66412759e-01,  3.02523434e-01,\n",
      "           1.32005334e+00,  3.99899870e-01, -1.89440513e+00,\n",
      "           6.24733210e-01]],\n",
      "\n",
      "        [[-2.90633321e-01, -2.74263710e-01,  2.50919199e+00,\n",
      "           1.60839438e+00, -6.05580211e-01,  1.35654497e+00,\n",
      "           1.26662695e+00,  5.12400791e-02,  5.30604362e-01,\n",
      "           2.07023072e+00],\n",
      "         [ 1.89285553e+00,  1.34873712e+00,  1.28113902e+00,\n",
      "          -9.91267800e-01, -1.03918302e+00,  2.09497690e-01,\n",
      "          -1.69125378e-01,  3.89782906e-01, -1.41076851e+00,\n",
      "          -1.09216547e+00],\n",
      "         [-8.19348693e-02, -6.82654798e-01, -1.85377181e-01,\n",
      "          -8.07943106e-01,  1.15139830e+00,  6.80587053e-01,\n",
      "           3.16492081e-01, -7.52518833e-01,  2.91114300e-01,\n",
      "           1.16299069e+00],\n",
      "         [ 5.52202284e-01,  6.07847333e-01, -1.42693973e+00,\n",
      "          -6.82483613e-01,  1.61610973e+00,  5.48901379e-01,\n",
      "           2.65570164e-01, -1.59482563e+00, -1.81156027e+00,\n",
      "           2.59957695e+00],\n",
      "         [-4.69175905e-01,  5.38761258e-01,  1.05053473e+00,\n",
      "          -3.31440985e-01,  1.43434644e+00, -1.82258010e+00,\n",
      "           8.75983179e-01, -8.43549371e-02, -7.31122196e-01,\n",
      "          -8.25648475e-03],\n",
      "         [ 7.46705532e-02,  2.05035731e-01, -1.13511491e+00,\n",
      "           9.26950097e-01,  8.73815775e-01, -7.57049799e-01,\n",
      "           3.45522672e-01, -1.07192397e+00, -4.85410869e-01,\n",
      "           2.96362340e-01],\n",
      "         [-9.10965621e-01, -1.04645348e+00,  3.13889384e-01,\n",
      "          -7.48145700e-01, -6.51775479e-01,  3.04052591e-01,\n",
      "           2.21068048e+00, -4.93689924e-01, -7.84657538e-01,\n",
      "           3.63433123e-01],\n",
      "         [-1.37362707e+00,  1.60141563e+00,  1.48371592e-01,\n",
      "          -1.19622767e+00,  1.05193770e+00, -8.27190101e-01,\n",
      "           1.00491631e+00,  1.14434004e+00, -2.10302567e+00,\n",
      "           2.81379402e-01]]]], dtype=float32)>, <tf.Tensor: shape=(2, 3, 2, 8, 10), dtype=float32, numpy=\n",
      "array([[[[[ 3.73490125e-01, -4.45566684e-01, -7.37408817e-01,\n",
      "            6.47134483e-02,  4.71245080e-01,  1.21966057e-01,\n",
      "           -5.77918708e-01,  1.86057553e-01, -3.97267759e-01,\n",
      "            1.03125644e+00],\n",
      "          [ 1.27802587e+00,  1.26601768e+00,  2.18973607e-01,\n",
      "            4.87705857e-01, -1.12214720e+00, -6.36173725e-01,\n",
      "           -1.09513056e+00,  2.24595204e-01, -1.89844787e+00,\n",
      "           -1.92053750e-01],\n",
      "          [-1.69783974e+00,  1.28009093e+00, -9.02721226e-01,\n",
      "           -1.12615812e+00, -5.17565727e-01,  7.21916258e-01,\n",
      "           -2.09014982e-01,  8.41341197e-01, -3.45656216e-01,\n",
      "            1.32645214e+00],\n",
      "          [-8.11351061e-01,  1.41911313e-01,  6.86539114e-01,\n",
      "            1.02823007e+00, -1.22150791e+00, -5.46465158e-01,\n",
      "            7.11550772e-01, -3.04608893e+00,  5.84980845e-01,\n",
      "            1.11008573e+00],\n",
      "          [ 2.89187133e-01,  8.70926619e-01,  3.84894818e-01,\n",
      "            9.34618935e-02,  1.22830033e+00, -5.98413110e-01,\n",
      "            1.02843761e+00, -7.05981195e-01, -2.34044522e-01,\n",
      "           -4.85863835e-01],\n",
      "          [-1.81705236e+00, -2.55428362e+00, -3.60130429e-01,\n",
      "            5.23053706e-01, -1.07330155e+00,  1.76071846e+00,\n",
      "            7.27622271e-01, -2.10160047e-01,  1.69160926e+00,\n",
      "            1.87957019e-01],\n",
      "          [ 1.55714929e+00,  4.45293814e-01, -6.48103237e-01,\n",
      "           -3.56900215e-01,  1.41978812e+00,  1.17856634e+00,\n",
      "           -9.55984145e-02,  9.10829425e-01, -4.05717760e-01,\n",
      "           -7.44770169e-01],\n",
      "          [ 2.42076302e+00,  1.18355513e-01, -2.19275498e+00,\n",
      "            1.33693480e+00, -6.45712674e-01, -3.53295505e-01,\n",
      "           -8.75073075e-01,  1.26753116e+00, -1.77250087e+00,\n",
      "            8.50718439e-01]],\n",
      "\n",
      "         [[-1.32464898e+00, -6.67459071e-01, -4.36544389e-01,\n",
      "            6.19681656e-01, -6.80814147e-01, -6.60964370e-01,\n",
      "            1.00080526e+00,  8.94797266e-01,  3.31958234e-02,\n",
      "            2.54728049e-01],\n",
      "          [ 1.65648830e+00,  8.86945128e-02, -5.51120579e-01,\n",
      "            7.81454086e-01,  1.73717511e+00,  5.53801596e-01,\n",
      "            4.12810266e-01,  1.71337724e-01,  7.49190569e-01,\n",
      "            6.81875408e-01],\n",
      "          [-9.99533594e-01, -7.62072086e-01, -1.89179432e+00,\n",
      "           -5.56182027e-01,  1.14136314e+00, -5.73854923e-01,\n",
      "           -9.69347239e-01, -3.30524683e-01, -1.35572803e+00,\n",
      "            6.66800961e-02],\n",
      "          [-2.11963117e-01, -2.25268555e+00, -4.86592442e-01,\n",
      "           -5.89730367e-02,  1.85388958e+00,  1.73992038e-01,\n",
      "            4.82807428e-01,  8.78225207e-01, -2.14906836e+00,\n",
      "            1.38816214e+00],\n",
      "          [ 3.66738558e-01,  8.36958647e-01, -1.19362736e+00,\n",
      "           -7.46140778e-02,  7.20057189e-01,  3.24771702e-01,\n",
      "           -2.51206636e-01, -2.64509588e-01,  1.29308665e+00,\n",
      "           -6.05749846e-01],\n",
      "          [ 1.28453004e+00, -1.82606131e-01, -1.51545393e+00,\n",
      "            3.10661465e-01,  1.44768393e+00,  3.02333236e-01,\n",
      "            1.47070125e-01,  7.68623352e-01, -5.50625265e-01,\n",
      "            1.24634051e+00],\n",
      "          [ 5.24993092e-02,  1.13510033e-02,  7.56832778e-01,\n",
      "            7.17529297e-01,  6.44016087e-01,  6.78282917e-01,\n",
      "            1.45997658e-01,  1.20558262e+00,  7.03395963e-01,\n",
      "           -1.19714677e+00],\n",
      "          [ 1.32987416e+00, -1.96473733e-01, -7.35089779e-02,\n",
      "            5.19833624e-01,  1.42287150e-01, -1.33633125e+00,\n",
      "            1.33774996e-01,  1.34463286e+00, -1.97565079e+00,\n",
      "           -1.15523362e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.56256032e-01, -1.05509973e+00,  6.06653988e-01,\n",
      "            5.61487913e-01,  1.01534879e+00,  1.71715522e+00,\n",
      "            1.52773881e+00,  4.96328831e-01,  1.25472009e-01,\n",
      "            7.69089282e-01],\n",
      "          [-2.31361485e+00, -8.40459317e-02,  1.63522685e+00,\n",
      "            4.19750392e-01, -6.57088086e-02,  7.14425206e-01,\n",
      "           -9.40154716e-02, -4.99631554e-01, -1.92726105e-01,\n",
      "            1.18490614e-01],\n",
      "          [ 1.85864544e+00,  1.29372871e+00, -1.02111721e+00,\n",
      "            1.77649033e+00, -2.91610837e-01,  7.47592032e-01,\n",
      "            1.24404943e+00,  2.44639739e-01,  8.44644248e-01,\n",
      "            5.72705209e-01],\n",
      "          [ 3.36784339e+00,  8.35223973e-01, -8.67837369e-01,\n",
      "            1.88514924e+00,  1.95948160e+00, -2.24290395e+00,\n",
      "           -6.13632679e-01,  6.09841533e-02, -1.07145119e+00,\n",
      "           -1.65558553e+00],\n",
      "          [ 3.45015258e-01,  1.05850971e+00, -4.02649760e-01,\n",
      "           -2.34451556e+00, -3.35888743e-01, -1.25423044e-01,\n",
      "           -3.18020701e-01, -5.33021867e-01, -2.51394963e+00,\n",
      "           -1.72724888e-01],\n",
      "          [ 4.46455568e-01,  1.03961997e-01,  1.80325538e-01,\n",
      "           -4.08811748e-01,  5.79810858e-01,  8.88723791e-01,\n",
      "           -7.18713462e-01,  9.06459451e-01, -1.09927762e+00,\n",
      "            2.87315369e-01],\n",
      "          [ 8.90610814e-01,  1.33347821e+00, -1.07846759e-01,\n",
      "            1.66574991e+00,  2.85183638e-01, -5.52349746e-01,\n",
      "           -2.58912492e+00, -5.67668825e-02, -1.32422522e-01,\n",
      "            1.43627316e-01],\n",
      "          [-1.14809966e+00,  3.56708288e-01, -7.42076278e-01,\n",
      "           -9.03444290e-01,  1.94768429e-01, -9.09894183e-02,\n",
      "           -3.04966003e-01, -1.19873810e+00,  1.58684000e-01,\n",
      "           -5.20521104e-01]],\n",
      "\n",
      "         [[-5.00921369e-01, -1.06356597e+00, -4.74429160e-01,\n",
      "            1.86220813e+00,  3.82229507e-01, -9.64568198e-01,\n",
      "            3.05349022e-01, -9.50796425e-01,  3.38250494e+00,\n",
      "            7.75633335e-01],\n",
      "          [ 1.53549477e-01,  2.44590735e+00, -4.67072159e-01,\n",
      "           -2.27036858e+00, -5.63577533e-01, -2.22777438e+00,\n",
      "            2.13038117e-01,  1.12200296e+00, -1.04483545e+00,\n",
      "            1.24605370e+00],\n",
      "          [ 8.00577402e-01,  7.02545285e-01, -5.52534223e-01,\n",
      "            1.28007382e-01,  9.56364095e-01,  1.40504634e+00,\n",
      "            4.07867044e-01,  6.06908917e-01, -2.56272495e-01,\n",
      "            1.08535685e-01],\n",
      "          [ 9.84097868e-02, -8.10387135e-01,  9.50529277e-01,\n",
      "            1.24172002e-01, -2.15215638e-01, -1.25758529e+00,\n",
      "           -7.23181427e-01, -9.65670705e-01,  1.39551854e+00,\n",
      "            4.21217501e-01],\n",
      "          [ 9.20798540e-01,  1.37522018e+00, -1.20421290e-01,\n",
      "            3.13514560e-01,  1.15810227e+00, -1.40726388e+00,\n",
      "            1.36766720e+00, -1.17503248e-01,  1.75274417e-01,\n",
      "           -6.60615265e-01],\n",
      "          [ 3.66199344e-01, -2.27562189e-01,  2.99683660e-01,\n",
      "            1.22589147e+00,  6.37936354e-01,  2.09285474e+00,\n",
      "            1.44039059e+00, -1.71876907e-01, -1.10233724e+00,\n",
      "            2.72254896e+00],\n",
      "          [ 5.73631346e-01,  3.17339361e-01, -1.41608372e-01,\n",
      "            5.05427301e-01,  6.77479148e-01,  6.98256731e-01,\n",
      "            1.42187759e-01, -1.79608905e+00,  9.77547824e-01,\n",
      "           -4.41652358e-01],\n",
      "          [-8.64329338e-01,  3.47067207e-01,  2.11396861e+00,\n",
      "            8.95823717e-01, -1.02114880e+00,  1.24847330e-01,\n",
      "            5.42768538e-01, -8.39075267e-01,  1.54193270e+00,\n",
      "            1.07967365e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.16231346e+00, -1.21194887e+00, -1.06779642e-01,\n",
      "            8.11002851e-01,  6.93668872e-02,  2.44507313e+00,\n",
      "           -5.66974342e-01, -2.93835491e-01,  9.49259289e-03,\n",
      "           -2.89257026e+00],\n",
      "          [-8.91270339e-01, -1.42643225e+00, -1.63170803e+00,\n",
      "           -1.29159033e+00, -1.08038157e-01,  1.90803364e-01,\n",
      "           -4.89743888e-01, -5.09428918e-01,  1.20136642e+00,\n",
      "           -6.68511152e-01],\n",
      "          [ 4.47788388e-01,  3.41705501e-01, -1.03253627e+00,\n",
      "            1.70032561e+00, -1.29516220e+00,  7.16826022e-02,\n",
      "            9.06019270e-01, -1.33590907e-01,  6.49484098e-01,\n",
      "           -3.23451102e-01],\n",
      "          [-1.38279825e-01, -9.64158297e-01,  4.41075057e-01,\n",
      "            5.31355679e-01, -5.30712306e-03, -1.64825082e+00,\n",
      "           -9.87583492e-03, -2.62139291e-01,  1.19656980e+00,\n",
      "            8.92613888e-01],\n",
      "          [ 8.08495402e-01,  9.26518887e-02,  8.29046965e-01,\n",
      "            8.57793987e-01,  3.35368276e-01, -3.66251796e-01,\n",
      "            5.25726140e-01,  7.60118842e-01, -5.53029537e-01,\n",
      "            4.64990407e-01],\n",
      "          [ 2.69866288e-01, -8.64357710e-01, -3.98098538e-03,\n",
      "           -7.78335571e-01,  1.42779696e+00,  4.64379460e-01,\n",
      "            1.46701002e+00, -8.19651037e-02,  1.34936738e+00,\n",
      "           -5.67192696e-02],\n",
      "          [-1.04419684e+00,  8.05560112e-01, -8.52508068e-01,\n",
      "           -2.10863018e+00, -9.02307510e-01,  2.28593528e-01,\n",
      "            1.25530660e-01, -1.27428785e-01, -1.45580554e+00,\n",
      "            8.35120082e-01],\n",
      "          [ 1.16595432e-01,  5.71766973e-01, -1.18233514e+00,\n",
      "           -4.28315818e-01, -1.53564668e+00, -4.74733621e-01,\n",
      "           -3.65346581e-01,  1.71132803e-01, -1.45845795e+00,\n",
      "            4.79093343e-02]],\n",
      "\n",
      "         [[-1.29711306e+00, -7.12977409e-01,  4.26760435e-01,\n",
      "            7.66164660e-01, -4.10077602e-01,  2.71769047e-01,\n",
      "           -3.81146073e-01,  1.10231543e+00,  2.06556305e-01,\n",
      "           -7.17758715e-01],\n",
      "          [ 2.24871063e+00, -5.74982047e-01, -2.03547239e+00,\n",
      "           -1.01386940e+00,  5.25532722e-01, -1.70783448e+00,\n",
      "           -6.76526725e-01,  4.59419489e-01,  7.18397200e-01,\n",
      "           -1.11951362e-02],\n",
      "          [ 3.08384717e-01, -3.33208412e-01, -5.15023232e-01,\n",
      "            1.65062264e-01,  2.14462757e-01, -1.23109424e+00,\n",
      "           -2.11841846e+00, -1.20817518e+00, -6.79348826e-01,\n",
      "            3.39244783e-01],\n",
      "          [ 4.55677360e-01,  1.80395126e+00, -2.43474507e+00,\n",
      "           -1.94340706e-01, -1.28617144e+00, -1.05082536e+00,\n",
      "            9.60879922e-01,  1.43286377e-01,  1.30347228e+00,\n",
      "            7.59381652e-01],\n",
      "          [-1.00377774e+00,  2.41719842e+00, -8.25052321e-01,\n",
      "            1.59389234e+00, -4.82616633e-01,  8.74037594e-02,\n",
      "           -1.38610256e+00,  6.72996879e-01, -9.78158951e-01,\n",
      "            7.85473406e-01],\n",
      "          [-6.97205365e-02,  3.73429030e-01, -4.44599003e-01,\n",
      "            1.35044813e+00,  1.76150358e+00, -1.15632936e-01,\n",
      "            2.25776717e-01, -3.07316691e-01, -2.47458053e+00,\n",
      "            7.76717901e-01],\n",
      "          [ 4.92516339e-01,  7.28830993e-01,  8.91305804e-01,\n",
      "           -1.54512644e+00, -4.30010855e-01, -1.31320477e+00,\n",
      "           -7.09433138e-01,  3.12523633e-01,  1.37041020e+00,\n",
      "           -2.86375955e-02],\n",
      "          [-1.41889095e-01, -2.66005546e-02,  1.60349265e-01,\n",
      "            7.47434437e-01, -1.22101593e+00, -3.71966869e-01,\n",
      "            8.45313072e-01,  7.44994760e-01,  9.24441338e-01,\n",
      "            9.64586437e-01]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[ 1.06591868e+00, -4.98112082e-01, -1.82277355e-02,\n",
      "           -1.04742193e+00, -2.41983443e-01,  9.17336524e-01,\n",
      "           -1.28536844e+00,  6.10969186e-01,  3.60984504e-01,\n",
      "           -7.48251140e-01],\n",
      "          [ 5.77821076e-01,  3.24712425e-01, -6.55238867e-01,\n",
      "           -6.28355026e-01,  2.11463356e+00, -1.43841136e+00,\n",
      "           -3.16321462e-01, -1.65919848e-02, -7.69462228e-01,\n",
      "           -5.64111918e-02],\n",
      "          [ 2.47700334e+00, -5.72381258e-01,  5.53723276e-01,\n",
      "           -1.72752571e+00,  8.08003008e-01,  3.02002788e-01,\n",
      "            2.30442420e-01, -2.20551038e+00,  2.79466640e-02,\n",
      "            9.91192937e-01],\n",
      "          [ 1.23746693e+00, -3.81734729e-01, -6.87249482e-01,\n",
      "           -3.87947112e-01,  6.29578650e-01, -4.51964289e-02,\n",
      "           -4.53199863e-01, -1.20121849e+00, -5.26732624e-01,\n",
      "            1.74310431e-01],\n",
      "          [-1.28941894e+00,  9.75059122e-02, -2.50915766e-01,\n",
      "           -3.75523448e-01, -4.44285870e-01,  2.35976383e-01,\n",
      "           -1.13066995e+00,  1.56876910e+00, -2.01878119e+00,\n",
      "            1.16443884e+00],\n",
      "          [ 4.44145322e-01, -1.63775802e-01,  8.09270084e-01,\n",
      "            1.96587846e-01,  3.71759981e-01, -9.67947841e-01,\n",
      "           -3.80005777e-01,  1.79745936e+00, -3.57539326e-01,\n",
      "           -1.75077474e+00],\n",
      "          [-1.56741464e+00,  8.96601900e-02,  7.42301762e-01,\n",
      "            9.08486187e-01,  4.25845504e-01, -3.31098288e-01,\n",
      "           -9.32218015e-01, -2.95603305e-01,  1.47798040e-03,\n",
      "            1.81488320e-01],\n",
      "          [-2.27065459e-01,  6.32430732e-01, -5.44660807e-01,\n",
      "            1.24215379e-01,  1.11837208e+00,  1.62058324e-02,\n",
      "           -2.63228059e-01, -1.25642025e+00,  1.33033180e+00,\n",
      "            7.50977814e-01]],\n",
      "\n",
      "         [[-1.30474794e+00, -5.16747952e-01, -1.58662605e+00,\n",
      "            2.31122661e+00,  3.84046108e-01,  1.71548054e-01,\n",
      "            2.13918880e-01,  3.09850872e-01, -1.74987197e+00,\n",
      "            9.14632916e-01],\n",
      "          [-1.79982603e-01, -4.33974266e-01, -1.05439329e+00,\n",
      "           -9.61655557e-01,  5.20259999e-02, -1.53508019e+00,\n",
      "            1.22394824e+00, -7.22933590e-01, -1.76644933e+00,\n",
      "            1.27372551e+00],\n",
      "          [-8.99129152e-01, -6.80187881e-01,  1.87530816e-01,\n",
      "            1.60963464e+00,  2.03853393e+00,  3.35962623e-01,\n",
      "           -1.78434753e+00, -6.83358550e-01,  2.32917532e-01,\n",
      "            9.55839679e-02],\n",
      "          [-1.46821237e+00, -6.72623694e-01, -3.81492972e-01,\n",
      "            4.36829835e-01, -1.77269042e+00,  1.99160293e-01,\n",
      "           -3.88098001e-01,  4.68111366e-01, -3.07973742e-01,\n",
      "           -1.23063362e+00],\n",
      "          [ 5.03140569e-01,  8.01652908e-01,  8.37746739e-01,\n",
      "           -2.70572990e-01, -6.23762965e-01,  2.59332013e+00,\n",
      "           -2.60148346e-01,  1.16754937e+00, -1.71944666e+00,\n",
      "           -4.32470411e-01],\n",
      "          [ 6.92328155e-01,  1.04029882e+00,  4.96626496e-01,\n",
      "           -3.55311394e-01,  4.72688168e-01, -2.39116025e+00,\n",
      "           -2.02406600e-01,  4.97555435e-02,  4.04018939e-01,\n",
      "            6.12349987e-01],\n",
      "          [ 7.28949308e-01,  7.25884140e-01,  4.92102772e-01,\n",
      "            7.81798184e-01,  1.05450058e+00, -1.18937409e+00,\n",
      "           -2.62608480e+00, -1.21451847e-01, -2.18558455e+00,\n",
      "            1.98738694e+00],\n",
      "          [-3.25019896e-01,  2.05615193e-01, -7.84846365e-01,\n",
      "           -3.64511251e-01,  5.99629164e-01, -3.99454921e-01,\n",
      "           -1.36194861e+00,  4.75735843e-01,  4.23107922e-01,\n",
      "            1.08618593e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.61168218e+00,  7.73218870e-01, -9.67796922e-01,\n",
      "           -9.33764219e-01,  2.02877045e+00,  5.74249506e-01,\n",
      "            8.47985804e-01, -7.54562840e-02,  2.48575792e-01,\n",
      "           -1.21265495e+00],\n",
      "          [-1.26600790e+00, -7.06917048e-01,  3.99564415e-01,\n",
      "            1.17599905e-01,  1.13924658e+00, -4.39889580e-01,\n",
      "           -2.20734879e-01, -1.45581096e-01,  2.77658552e-01,\n",
      "           -9.84394789e-01],\n",
      "          [ 3.89193207e-01,  2.11568069e+00,  1.01440310e+00,\n",
      "           -1.89797729e-01,  9.88897443e-01, -2.00052357e+00,\n",
      "           -3.09067965e-01,  1.13941014e+00, -1.41977990e+00,\n",
      "            2.64968835e-02],\n",
      "          [-1.87429190e+00, -3.85016322e-01,  1.28798771e+00,\n",
      "            1.17186201e+00,  3.03500533e-01,  9.59820449e-01,\n",
      "           -2.08762264e+00, -3.66518497e-01, -3.82003784e-01,\n",
      "            1.79072917e+00],\n",
      "          [-1.76909104e-01, -1.37831903e+00,  6.66035116e-02,\n",
      "            6.40033185e-01,  4.27979052e-01,  2.30300999e+00,\n",
      "            1.21107399e+00, -1.15986681e+00,  1.71248779e-01,\n",
      "           -3.18493366e-01],\n",
      "          [ 2.62199092e+00,  7.70463705e-01, -1.36886084e+00,\n",
      "            3.97063494e-01,  2.05072141e+00,  3.05971217e+00,\n",
      "            6.62514925e-01,  1.34592783e+00,  5.53025842e-01,\n",
      "            1.35684147e-01],\n",
      "          [ 2.16431308e+00,  1.82017112e+00,  1.80570602e+00,\n",
      "            4.06197608e-01, -1.50699019e-01,  1.83918309e+00,\n",
      "            2.42040813e-01, -1.83399618e+00, -8.16816032e-01,\n",
      "            3.07719022e-01],\n",
      "          [-2.12555960e-01, -4.21127766e-01, -5.62892079e-01,\n",
      "            7.54852355e-01, -7.98431933e-01, -6.94578528e-01,\n",
      "            3.10443074e-01, -6.33068502e-01,  4.97877568e-01,\n",
      "           -1.18564665e+00]],\n",
      "\n",
      "         [[-1.15267801e+00,  1.44074821e+00,  1.37980795e+00,\n",
      "           -4.21441793e-01,  3.54254127e-01,  3.23008269e-01,\n",
      "            1.06787658e+00,  2.30436578e-01,  9.18319523e-01,\n",
      "            5.02472639e-01],\n",
      "          [ 1.62755752e+00,  7.83837557e-01,  1.69739544e+00,\n",
      "            1.78707182e+00,  1.25112331e+00, -4.94670570e-02,\n",
      "           -3.98880750e-01,  1.12001216e+00, -1.07752001e+00,\n",
      "           -7.08785951e-01],\n",
      "          [ 4.42700684e-01, -1.91540927e-01, -3.56543273e-01,\n",
      "           -1.49996972e+00, -6.23179004e-02,  4.71712291e-01,\n",
      "            7.57857203e-01,  1.72584176e-01,  1.94597825e-01,\n",
      "           -3.71774346e-01],\n",
      "          [ 7.23846495e-01,  7.10535273e-02, -1.65522888e-01,\n",
      "            5.69501340e-01,  2.36231709e+00,  1.69791698e+00,\n",
      "            1.28097951e+00,  7.14807451e-01, -1.55034828e+00,\n",
      "            1.21956980e+00],\n",
      "          [ 2.35507631e+00, -1.09874797e+00,  1.02106667e+00,\n",
      "            1.45606041e-01,  6.84179783e-01,  1.25305831e+00,\n",
      "            5.93811572e-01, -4.28158909e-01,  8.94663393e-01,\n",
      "            9.40427244e-01],\n",
      "          [ 7.01145887e-01,  8.83972049e-01,  2.24994540e+00,\n",
      "            1.34107876e+00,  5.77258646e-01, -2.41185832e+00,\n",
      "            4.43469256e-01,  7.17579782e-01,  1.28070474e+00,\n",
      "           -2.95691967e+00],\n",
      "          [-8.43000233e-01,  2.64691543e-02, -1.10897171e+00,\n",
      "           -1.84887898e+00, -1.25935245e+00, -2.54980743e-01,\n",
      "           -4.93296087e-01,  4.66082901e-01,  1.29501700e+00,\n",
      "           -1.97021231e-01],\n",
      "          [ 9.22741368e-02,  5.81160724e-01, -1.42458093e+00,\n",
      "            1.01672494e+00, -4.69826490e-01, -1.94794858e+00,\n",
      "            1.40213001e+00, -3.46566230e-01,  7.86755383e-01,\n",
      "            1.15322661e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.95929146e-01, -1.27186990e+00,  2.02839047e-01,\n",
      "            4.87203598e-01, -9.05653685e-02,  8.37803185e-01,\n",
      "           -1.21485853e+00,  1.51218200e+00, -5.09566441e-02,\n",
      "           -6.05277836e-01],\n",
      "          [ 1.17246938e+00,  1.49635684e+00,  4.66328263e-01,\n",
      "           -2.86159754e-01,  8.33016932e-01,  1.83897185e+00,\n",
      "           -3.69812809e-02,  7.69686699e-02, -1.74770486e+00,\n",
      "           -5.92163026e-01],\n",
      "          [-5.92279732e-01, -1.37731767e+00,  1.65884101e+00,\n",
      "           -4.62067038e-01, -7.18269348e-01, -2.29433119e-01,\n",
      "            2.75609493e-01,  1.51604569e+00, -2.00658053e-01,\n",
      "           -6.19424641e-01],\n",
      "          [ 7.91479170e-01,  1.69913816e+00, -6.27653241e-01,\n",
      "            2.22011819e-01, -1.56938642e-01,  1.38818693e+00,\n",
      "           -1.76320374e+00,  4.97405112e-01,  1.42325997e-01,\n",
      "           -1.00465961e-01],\n",
      "          [ 7.20922232e-01, -5.63141331e-02,  9.05469775e-01,\n",
      "           -2.05106139e-02,  2.92476624e-01, -1.72691298e+00,\n",
      "           -1.29186034e+00, -1.66095257e+00,  1.62914479e+00,\n",
      "           -6.45446181e-01],\n",
      "          [ 1.48839259e+00, -1.73327327e-02,  1.48938909e-01,\n",
      "           -4.80601668e-01, -4.05148536e-01,  1.49780601e-01,\n",
      "            1.18974042e+00, -1.96951532e+00, -2.11591974e-01,\n",
      "            8.72266114e-01],\n",
      "          [ 7.77022362e-01, -2.18596041e-01,  2.04250789e+00,\n",
      "           -1.32003069e-01, -1.55803636e-01,  1.09539270e+00,\n",
      "           -2.48975307e-01, -4.81194824e-01,  3.50157142e-01,\n",
      "           -8.22357655e-01],\n",
      "          [ 3.72569501e-01, -3.12095970e-01,  1.31770468e+00,\n",
      "            1.66288066e+00, -2.45876026e+00,  9.81682241e-02,\n",
      "            1.54770160e+00,  1.44966185e+00,  5.39474130e-01,\n",
      "           -1.03025484e+00]],\n",
      "\n",
      "         [[ 2.39195481e-01, -4.71709877e-01, -2.77022719e-01,\n",
      "            1.60984123e+00,  7.95463979e-01, -5.99672437e-01,\n",
      "           -2.71754575e+00,  2.26543522e+00, -5.05961955e-01,\n",
      "            5.65187871e-01],\n",
      "          [ 5.60128391e-01,  1.04824269e+00, -7.40221918e-01,\n",
      "           -1.08994298e-01, -8.47153425e-01, -2.51335856e-02,\n",
      "           -1.12120414e+00, -1.06166255e+00, -3.57525349e-01,\n",
      "           -5.78753293e-01],\n",
      "          [ 2.22437710e-01, -1.53895423e-01,  3.40528846e-01,\n",
      "            1.15343595e+00,  2.36950111e+00, -2.06750798e+00,\n",
      "           -6.79950893e-01, -4.82994586e-01, -1.25074184e+00,\n",
      "           -6.51509643e-01],\n",
      "          [-1.00889422e-01,  8.28052223e-01, -2.43952060e+00,\n",
      "           -8.78340483e-01, -2.10076392e-01,  6.93236411e-01,\n",
      "           -2.59476423e-01, -3.80277574e-01, -2.03458500e+00,\n",
      "            8.00767124e-01],\n",
      "          [-1.09258795e+00,  6.00974381e-01, -2.12120548e-01,\n",
      "            9.88000706e-02,  1.22403467e+00, -7.21231699e-01,\n",
      "            1.24001861e+00, -2.21634316e+00,  5.22169530e-01,\n",
      "           -1.25265253e+00],\n",
      "          [-1.51920903e+00,  8.74989808e-01,  1.50902236e+00,\n",
      "           -1.84730113e-01,  2.82718390e-01, -3.81096274e-01,\n",
      "            1.73447058e-01,  4.40880097e-02,  7.23098516e-01,\n",
      "            1.97185385e+00],\n",
      "          [-1.79723954e+00, -2.12624669e+00, -1.83913291e+00,\n",
      "           -3.96061808e-01, -8.49718809e-01,  8.04965556e-01,\n",
      "            2.78305441e-01, -2.81362325e-01,  1.31149381e-01,\n",
      "            1.49796319e+00],\n",
      "          [ 1.15097284e-01, -7.87471473e-01, -2.44302440e+00,\n",
      "           -3.72239017e+00,  1.15571074e-01, -4.21596378e-01,\n",
      "           -1.00257702e-01, -3.67303610e-01, -1.46872509e+00,\n",
      "            1.76869944e-01]]]]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# testing rotary embedding layer (done)\n",
    "class MyRotaryEmbedding(tf.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base: float = None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        name=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        if scale_base is not None:\n",
    "            raise NotImplementedError\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        self.inv_freq = self._compute_inv_freq()\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        self.scale = (\n",
    "            (tf.range(0, dim, 2, dtype=tf.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None)\n",
    "        self._update_cos_sin_cache(max_position_embeddings, dtype=tf.float32)\n",
    "\n",
    "    def _compute_inv_freq(self):\n",
    "        return 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(self, seqlen:int, dtype):\n",
    "        self._seq_len_cached = seqlen\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = tf.range(seqlen, dtype=tf.float32)\n",
    "            if self.inv_freq.dtype != tf.float32:\n",
    "                inv_freq = self._compute_inv_freq()\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = tf.range(seqlen, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        # tensorflow does not appear to do what the top comment states\n",
    "        freqs = tf.einsum('i,j->ij', t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = tf.cast(tf.cos(freqs), dtype=dtype)\n",
    "            self._sin_cached = tf.cast(tf.sin(freqs), dtype=dtype)\n",
    "        else:\n",
    "            power = (tf.range(seqlen, dtype=self.scale.dtype) - seqlen // 2) / self.scale_base\n",
    "            scale = self.scale ** tf.expand_dims(power, axis=1)\n",
    "            # Force the scale multiplciation to happen in fp32\n",
    "            self._cos_cached = tf.cast((tf.cos(freqs) * scale), dtype=dtype)\n",
    "            self._sin_cached = tf.cast((tf.sin(freqs) * scale), dtype=dtype)\n",
    "            self._cos_k_cached = tf.cast((tf.cos(freqs) / scale), dtype=dtype)\n",
    "            self._sin_k_cached = tf.cast((tf.sin(freqs) / scale), dtype=dtype)\n",
    "    \n",
    "    def __call__(self, qkv, kv, seqlen_offset, **kwargs):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            # or (self.training and self._cos_cached.is_inference()) # look into this\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return MY_apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = MY_apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            kv = MY_apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            return q, kv\n",
    "    ################################\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base= None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        device = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if scale_base is not None:\n",
    "            raise NotImplementedError\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.device = device\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        inv_freq = self._compute_inv_freq(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        scale = (\n",
    "            (torch.arange(0, dim, 2, device=device, dtype=torch.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None\n",
    "        )\n",
    "        self.register_buffer(\"scale\", scale, persistent=False)\n",
    "\n",
    "        # Initialize cached attributes since ONNX can't rely on dynamic initialization\n",
    "        self._update_cos_sin_cache(max_position_embeddings, device=device, dtype=torch.float32)\n",
    "\n",
    "    def _compute_inv_freq(self, device: Optional[str] = None) -> torch.FloatTensor:\n",
    "        return 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> None:\n",
    "        self._seq_len_cached = seqlen\n",
    "\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
    "            if self.inv_freq.dtype != torch.float32:\n",
    "                inv_freq = self._compute_inv_freq(device=device)\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = torch.cos(freqs).to(dtype)\n",
    "            self._sin_cached = torch.sin(freqs).to(dtype)\n",
    "        else:\n",
    "            power = (\n",
    "                torch.arange(seqlen, dtype=self.scale.dtype, device=self.scale.device) - seqlen // 2\n",
    "            ) / self.scale_base\n",
    "            scale = self.scale.to(device=power.device) ** rearrange(power, \"s -> s 1\")\n",
    "\n",
    "            # Force the scale multiplication to happen in fp32\n",
    "            self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
    "            self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
    "            self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
    "            self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.Tensor,\n",
    "        kv: Optional[torch.Tensor] = None,\n",
    "        seqlen_offset: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            or (self.training and self._cos_cached.is_inference())\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, device=qkv.device, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return _apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = _apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "            kv = _apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "\n",
    "            return q, kv\n",
    "\n",
    "\n",
    "dim = 5\n",
    "base = 5000\n",
    "scale_base = None\n",
    "pos_idx_in_fp32 = False\n",
    "max_position_embeddings = 2048\n",
    "device = 'cpu'\n",
    "\n",
    "rot_emb = RotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings, device)\n",
    "tf_rot_emb = MyRotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings)\n",
    "\n",
    "# test with kv being something, and being None\n",
    "\n",
    "# it appears that if kv will be none, then qkv must be 4d rather than 5d\n",
    "\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "kv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "seqlen_offset = 4\n",
    "print(rot_emb(qkv, kv, seqlen_offset))\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "kv = tf.constant(kv.numpy())\n",
    "print(tf_rot_emb(qkv, kv, seqlen_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3245, -0.0591,  0.6093, -0.1040, -0.7552],\n",
      "        [ 0.7832,  0.7040, -0.8370,  0.0035,  0.8881],\n",
      "        [ 0.0339,  0.8532,  0.2983,  0.0426, -0.2447],\n",
      "        [ 0.5386,  0.1699, -0.0566,  0.0462, -0.1206],\n",
      "        [ 0.0076,  0.7232, -0.5018, -0.1676,  0.6907],\n",
      "        [ 0.3041, -0.0959,  0.1605, -0.0091, -0.3878],\n",
      "        [-0.2141,  0.4867,  0.1948, -0.1034, -0.1485]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tf.Tensor(\n",
      "[[ 0.90998816  0.764223   -0.65371203 -0.20212054 -0.3200668 ]\n",
      " [-0.4556808  -0.11932212  0.36024922 -0.0818251   0.5056948 ]\n",
      " [ 0.49138063  0.12101516  0.58125836 -0.04253865  0.03689446]\n",
      " [ 0.23134461  0.7114701  -0.49200276 -0.4596124  -0.14770705]\n",
      " [-0.06815962 -0.44666833  0.5968875   0.35498527  0.62488246]\n",
      " [ 0.46989405  0.9027568  -0.8334268  -0.4704327  -0.28691515]\n",
      " [ 0.5129371   0.11793047  0.17815518  0.09346697  0.11931678]], shape=(7, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# testing Linear Layer\n",
    "layer = torch.nn.Linear(3, 5)\n",
    "input = torch.randn([7,3])\n",
    "print(layer(input))\n",
    "weights = tf.cast(torch.detach(layer.weight).numpy(), dtype=tf.float32)\n",
    "bias = tf.cast(torch.detach(layer.bias).numpy(), dtype=tf.float32)\n",
    "linear = bert.Dense_v2(3, 5, weights, bias=bias)\n",
    "input = tf.cast(input.numpy(), dtype=tf.float32)\n",
    "print(linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NewGELU(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "    def __call__(self, input):\n",
    "        return 0.5 * input * (1.0 + tf.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * tf.pow(input, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP (need to fill in the weight, bias, and activation) (works)\n",
    "\n",
    "class MyMLP(tf.Module):\n",
    "    def __init__(self, n_inner:int, n_embd:int, name=None): # manually written in act-fn()\n",
    "        super().__init__(name)\n",
    "        self.fc1 = bert.Dense_v2(n_embd, n_inner, weights=None, bias=None)\n",
    "        self.fc2 = bert.Dense_v2(n_inner, n_embd, weights=None, bias=None)\n",
    "        self.act = NewGELU\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class NewGELUActivation(nn.Module): # grabbed from hugging face\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "    \n",
    "\n",
    "# input = torch.randn([2,3])\n",
    "# print(input)\n",
    "# layer = NewGELUActivation()\n",
    "# print(layer(input))\n",
    "# input = tf.constant(input.numpy())\n",
    "# layer = NewGELU()\n",
    "# print(layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Looks good\n",
    "\n",
    "# not sure what key_padding_mask dims are supposed to be in reality\n",
    "\n",
    "def mask_fill(matrix, mask, value):\n",
    "    inv_mask = (-1 * mask) + 1\n",
    "    return inv_mask * matrix + (value * mask)\n",
    "\n",
    "class MySelfAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale:float, name=None):\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, qkv, key_padding_mask=None):\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = tf.unstack(qkv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        softmax_scale = self.softmax_scale\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        # print(scores)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen), -10000.0)\n",
    "            padding_mask = tf.cast(padding_mask, dtype=scores.dtype)\n",
    "            # print(padding_mask)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 3.0)\n",
    "            # print(padding_mask)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # print(scores)\n",
    "        # i am assuming this to be causal\n",
    "        causal_mask = tf.experimental.numpy.triu(tf.fill((seqlen, seqlen), -10000.0), 1) # might need to be replaced\n",
    "        scores = scores + tf.cast(causal_mask, dtype=scores.dtype)\n",
    "        # print(scores)\n",
    "        attention = tf.cast(tf.nn.softmax(scores, axis=-1), dtype=v.dtype)\n",
    "        # print(attention)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        causal: bool = True,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attention_dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.drop = nn.Dropout(attention_dropout)\n",
    "    @torch.autocast(\"cpu\", enabled=False)\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.FloatTensor,\n",
    "        causal: bool = None,\n",
    "        key_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.FloatTensor:\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.to(torch.float32)\n",
    "        k = k.to(torch.float32)\n",
    "        causal = self.causal if causal is None else causal\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        # using float16, which might lead to overflow\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        # print(scores)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype, device=scores.device)\n",
    "            # print(padding_mask)\n",
    "            padding_mask.masked_fill_(key_padding_mask, 3.0)\n",
    "            # print(padding_mask)\n",
    "            scores = scores + rearrange(padding_mask, \"b s -> b 1 1 s\")\n",
    "        # print(scores)\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "        # print(scores)\n",
    "        attention = torch.softmax(scores, dim=-1).to(v.dtype)\n",
    "        # print(attention)\n",
    "        attention = self.drop(attention)\n",
    "        output = torch.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "my_self_atten = MySelfAttention(softmax_scale=0.1)\n",
    "self_atten = SelfAttention(softmax_scale=0.1)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 17\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "# qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "qkv = torch.randn([batch_size, seq_len, 3, 5, 11])\n",
    "key_padding_mask = torch.zeros((batch_size, seq_len), dtype=torch.float32)\n",
    "key_padding_mask[0,0] += 1\n",
    "key_padding_mask[0,1] += 1\n",
    "key_padding_mask = key_padding_mask.to(bool)\n",
    "out = self_atten(qkv, causal=True, key_padding_mask=key_padding_mask)\n",
    "# print(out)\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "key_padding_mask = tf.constant(key_padding_mask.to(torch.float).numpy())\n",
    "out = my_self_atten(qkv, key_padding_mask=key_padding_mask)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 20, 7])\n",
      "torch.Size([1, 11, 2, 20, 7])\n",
      "-----\n",
      "q (1, 17, 20, 7)\n",
      "kv (1, 11, 2, 20, 7)\n",
      "k (1, 11, 20, 7)\n",
      "padding_mask: (1, 11)\n",
      "key_padding_mask: (1, 11)\n"
     ]
    }
   ],
   "source": [
    "# looks good\n",
    "\n",
    "class MyCrossAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale, name=None): # assume to be causal\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, q, kv, key_padding_mask):\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        if kv.shape[3] != kv.shape[2]:\n",
    "            kv = tf.repeat(kv, repeats=q.shape[2] // kv.shape[3], axis=-2)\n",
    "        k, v = tf.unstack(kv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        print(\"q\", q.shape)\n",
    "        print(\"kv\", kv.shape)\n",
    "        print(\"k\", k.shape)\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * self.softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen_k), -10000.0)\n",
    "            padding_mask = tf.cast(padding_mask, dtype=scores.dtype)\n",
    "            print(\"padding_mask:\", padding_mask.shape)\n",
    "            print(\"key_padding_mask:\", key_padding_mask.shape)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 0.0)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # causal stuff\n",
    "        rows = tf.expand_dims(tf.range(seqlen_q, dtype=tf.int64), axis=1) # if this fails, do axis=-1\n",
    "        cols = tf.range(seqlen_k, dtype=tf.int64)\n",
    "        causal_mask = cols > rows + seqlen_k - seqlen_q\n",
    "        causal_mask = tf.cast(causal_mask, dtype=tf.float32)\n",
    "        scores = mask_fill(scores, causal_mask, -10000.0)\n",
    "        # end of causal stuff\n",
    "        attention = tf.nn.softmax(scores, axis=-1)\n",
    "        attention = tf.cast(attention, dtype=v.dtype)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, causal: bool = True, softmax_scale: Optional[float] = None,\n",
    "        attention_dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.drop = nn.Dropout(attention_dropout)\n",
    "    def forward(self, q: torch.FloatTensor, kv: torch.FloatTensor,causal: bool = None,\n",
    "        key_padding_mask: Optional[torch.BoolTensor] = None, **kwargs) -> torch.FloatTensor:\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        if kv.shape[3] != q.shape[2]:\n",
    "            kv = repeat(kv, \"... hkv d -> ... (hkv g) d\", g=q.shape[2] // kv.shape[3])\n",
    "        k, v = kv.unbind(dim=2)\n",
    "        q = q.to(torch.float32)\n",
    "        k = k.to(torch.float32)\n",
    "        causal = self.causal if causal is None else causal\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        # using float16, which might lead to overflow\n",
    "        print(q.shape)\n",
    "        print(kv.shape)\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full(\n",
    "                (batch_size, seqlen_k),\n",
    "                -10000.0,\n",
    "                dtype=scores.dtype,\n",
    "                device=scores.device,\n",
    "            )\n",
    "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
    "            scores = scores + rearrange(padding_mask, \"b s -> b 1 1 s\")\n",
    "        if causal:\n",
    "            rows = rearrange(torch.arange(seqlen_q, device=q.device, dtype=torch.long), \"s -> s 1\")\n",
    "            cols = torch.arange(seqlen_k, device=k.device, dtype=torch.long)\n",
    "            causal_mask = cols > rows + seqlen_k - seqlen_q\n",
    "            scores = scores.masked_fill(causal_mask, -10000.0)\n",
    "        attention = torch.softmax(scores, dim=-1).to(v.dtype)\n",
    "        attention = self.drop(attention)\n",
    "        output = torch.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "cross_atten = CrossAttention(causal=True, softmax_scale=0.1)\n",
    "my_cross_atten = MyCrossAttention(softmax_scale=0.1)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 17\n",
    "seqlen_k = 11\n",
    "two = 2 # must be two\n",
    "q = torch.randn(batch_size, seq_len, 20, 7)\n",
    "kv = torch.randn(batch_size, seqlen_k, two, 5, 7)\n",
    "key_padding_mask = torch.zeros((batch_size, seqlen_k), dtype=torch.float32)\n",
    "key_padding_mask[0,0] += 1\n",
    "key_padding_mask[0,1] += 1\n",
    "key_padding_mask = key_padding_mask.to(bool)\n",
    "out = cross_atten(q, kv, causal=True, key_padding_mask=key_padding_mask)\n",
    "\n",
    "# print(out)\n",
    "print(\"-----\")\n",
    "q = tf.constant(q.numpy())\n",
    "kv = tf.constant(kv.numpy())\n",
    "key_padding_mask = tf.constant(key_padding_mask.to(torch.float).numpy())\n",
    "out = my_cross_atten(q, kv, key_padding_mask)\n",
    "# print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MY_update_kv_cache(kv, inference_params, layer_idx:int):\n",
    "    num_heads, head_dim = kv.shape[-2:]\n",
    "    if layer_idx not in inference_params.key_value_memory_dict:\n",
    "        # during the UDO desgin process, could use uninitalized memory rather than zeros\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.zeros(\n",
    "            [\n",
    "                inference_params.max_batch_size,\n",
    "                inference_params.max_seqlen,\n",
    "                2,\n",
    "                num_heads,\n",
    "                head_dim\n",
    "            ],\n",
    "            dtype=kv.dtype\n",
    "        )\n",
    "    batch_start = inference_params.batch_size_offset\n",
    "    batch_end = batch_start + kv.shape[0]\n",
    "    sequence_start = inference_params.seqlen_offset\n",
    "    sequence_end = sequence_start + kv.shape[1]\n",
    "    # When the current sequence length is equal to or larger than the maximum sequence length,\n",
    "    # we need to concatenate the current `kv` with the cached `kv` to expand its length\n",
    "    if sequence_end >= inference_params.max_seqlen:\n",
    "        # the line below might fail due to the tuple\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.concat((inference_params.key_value_memory_dict[layer_idx], kv), axis=1)\n",
    "    inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, sequence_start:sequence_end, ...] = kv\n",
    "    kv = inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, :sequence_end, ...]\n",
    "    return kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_mha_dims(\n",
    "    config,\n",
    "    n_head: Optional[int] = None,\n",
    "    n_head_kv: Optional[int] = None,\n",
    "    head_dim: Optional[int] = None,\n",
    "):\n",
    "    if n_head is None and head_dim is None:\n",
    "        head_dim = config.n_embd // config.n_head\n",
    "        n_head = config.n_head\n",
    "    elif n_head is None or head_dim is None:\n",
    "        raise ValueError(\"`n_head` and `head_dim` must be both specified or `None`.\")\n",
    "    if n_head_kv is None:\n",
    "        n_head_kv = getattr(config, \"n_head_kv\", None) or n_head\n",
    "    return n_head, n_head_kv, head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not my code\n",
    "\n",
    "pad_input, unpad_input = None, None\n",
    "FlashRotaryEmbedding = None\n",
    "FlashSelfAttention, FlashCrossAttention = None, None\n",
    "FusedDense = None\n",
    "\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \"\"\"Multi-head attention layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: PretrainedConfig,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "        device: Optional[str] = None,\n",
    "        rotary_dim: Optional[int] = None,\n",
    "        rotary_base: float = 10000.0,\n",
    "        rotary_scale_base: Optional[float] = None,\n",
    "        n_head: Optional[int] = None,\n",
    "        n_head_kv: Optional[int] = None,\n",
    "        head_dim: Optional[int] = None,\n",
    "        bias: bool = True,\n",
    "        causal: bool = True,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        layer_idx: Optional[int] = None,\n",
    "        return_residual: bool = False,\n",
    "        checkpointing: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Rotary embedding\n",
    "        self.rotary_dim = rotary_dim if rotary_dim is not None else getattr(config, \"rotary_dim\", 0)\n",
    "        if self.rotary_dim > 0:\n",
    "            rotary_cls = FlashRotaryEmbedding if config.flash_rotary else RotaryEmbedding\n",
    "            if rotary_cls is None:\n",
    "                rotary_cls = RotaryEmbedding\n",
    "\n",
    "            rotary_kwargs = {}\n",
    "            if rotary_cls is RotaryEmbedding:\n",
    "                rotary_kwargs[\"max_position_embeddings\"] = config.n_positions\n",
    "            print(\"rotary cls:\", rotary_cls)\n",
    "            self.rotary_emb = rotary_cls(\n",
    "                self.rotary_dim,\n",
    "                base=rotary_base,\n",
    "                scale_base=rotary_scale_base,\n",
    "                pos_idx_in_fp32=True,\n",
    "                max_position_embeddings=2048,\n",
    "                device=device,)\n",
    "\n",
    "        # MLP\n",
    "        self.n_head, self.n_head_kv, self.head_dim = _find_mha_dims(\n",
    "            config, n_head=n_head, n_head_kv=n_head_kv, head_dim=head_dim\n",
    "        )\n",
    "        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n",
    "        hidden_size = config.n_embd\n",
    "\n",
    "        linear_cls = FusedDense if config.fused_dense else nn.Linear\n",
    "        if linear_cls is None:\n",
    "            linear_cls = nn.Linear\n",
    "\n",
    "        self.Wqkv = linear_cls(hidden_size, op_size, bias=bias, device=device, dtype=dtype)\n",
    "        self.out_proj = linear_cls(hidden_size, hidden_size, bias=bias, device=device, dtype=dtype)\n",
    "\n",
    "        # Attention\n",
    "        attn_cls = FlashSelfAttention if config.flash_attn else SelfAttention\n",
    "        if attn_cls is None:\n",
    "            attn_cls = SelfAttention\n",
    "\n",
    "        cross_attn_cls = FlashCrossAttention if config.flash_attn else CrossAttention\n",
    "        if cross_attn_cls is None:\n",
    "            cross_attn_cls = CrossAttention\n",
    "\n",
    "        self.inner_attn = attn_cls(\n",
    "            causal=causal,\n",
    "            softmax_scale=softmax_scale,\n",
    "            attention_dropout=config.attn_pdrop,\n",
    "        )\n",
    "        self.inner_cross_attn = cross_attn_cls(\n",
    "            causal=causal,\n",
    "            softmax_scale=softmax_scale,\n",
    "            attention_dropout=config.attn_pdrop,\n",
    "        )\n",
    "\n",
    "        self.flash_attn = config.flash_attn and attn_cls is FlashSelfAttention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.return_residual = return_residual\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "    def _forward_self_attn(\n",
    "        self, x: torch.FloatTensor, key_padding_mask: Optional[torch.BoolTensor]\n",
    "    ) -> torch.FloatTensor:\n",
    "        qkv = self.Wqkv(x)\n",
    "        qkv = rearrange(qkv, \"... (three h d) -> ... three h d\", three=3, d=self.head_dim)\n",
    "        if self.rotary_dim > 0:\n",
    "            qkv = self.rotary_emb(qkv)\n",
    "        if self.flash_attn:\n",
    "            batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "            cu_seqlens, max_seqlen = None, None\n",
    "            if key_padding_mask is not None:\n",
    "                # If `key_padding_mask` is supplied, we need to unpad the input and retrieve\n",
    "                # the `cu_seqlens` and `max_seqlen` to be used by `flash-attn`\n",
    "                qkv, indices, cu_seqlens, max_seqlen = unpad_input(qkv, key_padding_mask)\n",
    "            if self.checkpointing:\n",
    "                attn_output = torch.utils.checkpoint.checkpoint(\n",
    "                    self.inner_attn, qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen\n",
    "                )\n",
    "            else:\n",
    "                attn_output = self.inner_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen).to(qkv.device)\n",
    "            # If `key_padding_mask` is supplied, we need to pad the output back to the original shape\n",
    "            return pad_input(attn_output, indices, batch_size, seqlen) if key_padding_mask is not None else attn_output\n",
    "        if self.checkpointing:\n",
    "            return torch.utils.checkpoint.checkpoint(self.inner_attn, qkv, key_padding_mask=key_padding_mask)\n",
    "        return self.inner_attn(qkv, key_padding_mask=key_padding_mask)\n",
    "\n",
    "    def _forward_cross_attn(\n",
    "        self,\n",
    "        x: torch.FloatTensor,\n",
    "        past_key_values,\n",
    "        key_padding_mask: Optional[torch.BoolTensor],\n",
    "    ) -> torch.FloatTensor:\n",
    "        batch_size = x.shape[0]\n",
    "        qkv = self.Wqkv(x)\n",
    "        q = qkv[..., : self.n_head * self.head_dim]\n",
    "        # print(\"q1:\", q)\n",
    "        q = rearrange(q, \"... (h d) -> ... h d\", d=self.head_dim)\n",
    "        # print(\"q2:\", q)\n",
    "        kv = qkv[..., self.n_head * self.head_dim :]\n",
    "        # print(\"kv1:\", kv)\n",
    "        kv = rearrange(kv, \"... (two hkv d) -> ... two hkv d\", two=2, d=self.head_dim)\n",
    "        # print(\"kv2:\", kv)\n",
    "        seqlen_offset = past_key_values.seqlen_offset if past_key_values is not None else 0\n",
    "        causal = None if seqlen_offset == 0 else False\n",
    "        print(\"q SHAPE:\", q.shape)\n",
    "        print(\"kv SHAPE:\", kv.shape)\n",
    "        if self.rotary_dim > 0:\n",
    "            q, kv = self.rotary_emb(q, kv=kv, seqlen_offset=seqlen_offset)\n",
    "        if past_key_values is not None:\n",
    "            kv = _update_kv_cache(kv, past_key_values, self.layer_idx)\n",
    "        if self.flash_attn:\n",
    "            batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "            seqlen_k = kv.shape[1]\n",
    "            cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k = (\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "            if key_padding_mask is not None:\n",
    "                kv, _, cu_seqlens_k, max_seqlen_k = unpad_input(kv, key_padding_mask)\n",
    "                if seqlen_q == 1:\n",
    "                    key_padding_mask = torch.ones(batch_size, 1, device=q.device)\n",
    "                elif seqlen_q != seqlen_k:\n",
    "                    key_padding_mask = key_padding_mask[:, -seqlen_q:]\n",
    "                q, indices_q, cu_seqlens_q, max_seqlen_q = unpad_input(q, key_padding_mask)\n",
    "            if self.checkpointing:\n",
    "                attn_output = torch.utils.checkpoint.checkpoint(\n",
    "                    self.inner_cross_attn,\n",
    "                    q,\n",
    "                    kv,\n",
    "                    causal=causal,\n",
    "                    cu_seqlens=cu_seqlens_q,\n",
    "                    max_seqlen=max_seqlen_q,\n",
    "                    cu_seqlens_k=cu_seqlens_k,\n",
    "                    max_seqlen_k=max_seqlen_k,\n",
    "                )\n",
    "            else:\n",
    "                attn_output = self.inner_cross_attn(\n",
    "                    q,\n",
    "                    kv,\n",
    "                    causal=causal,\n",
    "                    cu_seqlens=cu_seqlens_q,\n",
    "                    max_seqlen=max_seqlen_q,\n",
    "                    cu_seqlens_k=cu_seqlens_k,\n",
    "                    max_seqlen_k=max_seqlen_k,\n",
    "                )\n",
    "            return (\n",
    "                pad_input(attn_output, indices_q, batch_size, max_seqlen_q)\n",
    "                if key_padding_mask is not None\n",
    "                else attn_output\n",
    "            )\n",
    "        if self.checkpointing:\n",
    "            return torch.utils.checkpoint.checkpoint(\n",
    "                self.inner_cross_attn,\n",
    "                q,\n",
    "                kv,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                causal=causal,\n",
    "            )\n",
    "        # print(\"q:\", q)\n",
    "        # print(\"kv:\", kv)\n",
    "        return self.inner_cross_attn(q, kv, key_padding_mask=key_padding_mask, causal=causal)\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.FloatTensor,\n",
    "        past_key_values=None,\n",
    "        attention_mask: Optional[Union[torch.LongTensor, torch.BoolTensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.bool()\n",
    "        else:\n",
    "            attention_mask = None\n",
    "        # MHA\n",
    "        if self.n_head == self.n_head_kv:\n",
    "            if past_key_values is None:\n",
    "                # If `past_key_values` are not supplied, we run self-attention\n",
    "                attn_output = self._forward_self_attn(x, attention_mask)\n",
    "            else:\n",
    "                # If `past_key_values` are supplied, it means that we might have cached values and\n",
    "                # could take advantage of cross-attention\n",
    "                attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        # MQA / GQA\n",
    "        else:\n",
    "            # Regardless of `past_key_values` being supplied or not, it always use cross-attention\n",
    "            # because `q` and `kv` lengths might be different\n",
    "            attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        # print(\"attn_out:\", attn_output)\n",
    "        output = rearrange(attn_output, \"... h d -> ... (h d)\")\n",
    "        # print(\"output:\", output)\n",
    "        output = self.out_proj(output)\n",
    "        # print(\"output:\", output)\n",
    "        return output if not self.return_residual else (output, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op_size: 120\n",
      "rotary cls: <class '__main__.RotaryEmbedding'>\n",
      "{'Wqkv_weights': <tf.Tensor: shape=(60, 120), dtype=float32, numpy=\n",
      "array([[ 0.07678058, -0.07007174,  0.05891553, ...,  0.00992144,\n",
      "        -0.00672298, -0.02630917],\n",
      "       [-0.07346928,  0.02912661,  0.10963711, ...,  0.04504179,\n",
      "        -0.06488528,  0.04778528],\n",
      "       [ 0.10964601, -0.01543122, -0.1091646 , ..., -0.11678492,\n",
      "        -0.10441844, -0.04532752],\n",
      "       ...,\n",
      "       [ 0.09467129,  0.00772457,  0.0470764 , ...,  0.10836332,\n",
      "        -0.08057441,  0.11006826],\n",
      "       [-0.079307  , -0.06699201, -0.09228462, ..., -0.00885021,\n",
      "        -0.11001061, -0.0755408 ],\n",
      "       [-0.10742576,  0.12564814,  0.05507148, ..., -0.0084542 ,\n",
      "         0.07647138, -0.06237472]], dtype=float32)>, 'out_proj_weights': <tf.Tensor: shape=(60, 60), dtype=float32, numpy=\n",
      "array([[ 0.02931975, -0.05140317,  0.03600217, ..., -0.04946491,\n",
      "         0.12587316, -0.0936538 ],\n",
      "       [-0.03755412, -0.09317454, -0.12040319, ..., -0.01674604,\n",
      "        -0.09559858, -0.0400452 ],\n",
      "       [-0.01469262,  0.01829587,  0.02986021, ..., -0.0581398 ,\n",
      "         0.04939298, -0.00098173],\n",
      "       ...,\n",
      "       [ 0.01439585,  0.00893813,  0.07273296, ..., -0.09806082,\n",
      "        -0.11424731, -0.02887627],\n",
      "       [ 0.07707062,  0.07106394, -0.08210291, ...,  0.11117604,\n",
      "         0.05031563,  0.03285577],\n",
      "       [-0.11362642,  0.08820596,  0.08718416, ..., -0.09126077,\n",
      "        -0.05987509, -0.06197001]], dtype=float32)>}\n",
      "q SHAPE: torch.Size([1, 11, 10, 6])\n",
      "kv SHAPE: torch.Size([1, 11, 2, 5, 6])\n",
      "case 2\n",
      "x.shape: torch.Size([1, 11, 10, 6])\n",
      "torch.Size([1, 11, 10, 3])\n",
      "torch.Size([1, 11, 10, 3])\n",
      "torch.Size([11, 1, 3])\n",
      "torch.Size([11, 1, 3])\n",
      "torch.Size([1, 11, 10, 6])\n",
      "torch.Size([1, 11, 2, 10, 6])\n",
      "out: tensor([[[-1.2670e-01, -2.4824e-01, -1.0379e-01, -3.2112e-01,  6.3877e-02,\n",
      "          -6.9395e-01,  1.5525e-01, -6.7439e-01,  5.7392e-01, -2.2811e-02,\n",
      "           3.0432e-01,  1.1236e-01,  1.8112e-01, -4.0803e-01, -4.0827e-01,\n",
      "           3.9172e-01,  2.0429e-01, -6.2238e-01,  6.7955e-02,  2.2444e-01,\n",
      "          -1.6077e-01, -2.7203e-01, -4.2832e-02,  3.8606e-02,  2.0873e-01,\n",
      "           1.1376e-01, -1.5046e-01,  4.7906e-01,  1.7989e-01, -7.7502e-01,\n",
      "          -9.0072e-02,  3.3518e-01, -7.0424e-01, -2.3071e-01, -4.6973e-01,\n",
      "          -1.2649e-01,  5.7536e-02, -7.6752e-02, -6.2408e-01, -1.5415e-01,\n",
      "           2.4274e-01,  1.2282e-01, -2.0309e-01, -5.9496e-01,  2.6962e-01,\n",
      "           2.0813e-01, -1.7408e-01, -3.7258e-01,  3.8721e-01, -1.0274e-02,\n",
      "           1.7039e-02,  7.1254e-02,  1.6884e-01,  2.0377e-01, -6.3864e-01,\n",
      "           7.1859e-01, -3.6252e-02,  3.4622e-02,  8.7831e-01,  7.9714e-02],\n",
      "         [-2.0204e-01, -2.2650e-01, -9.8941e-01,  3.0606e-02,  2.4381e-01,\n",
      "          -3.9707e-01, -1.8448e-01, -6.9769e-01, -2.8540e-01, -1.8450e-01,\n",
      "           8.2486e-01,  1.2034e-01, -5.0195e-01,  6.8133e-01, -1.5953e-01,\n",
      "          -1.2923e-01, -1.9325e-01, -1.2199e-01, -2.3351e-01,  3.8160e-01,\n",
      "           1.3390e-01,  5.6049e-02, -1.5045e-01,  6.0321e-01, -8.2502e-03,\n",
      "          -1.3034e-01, -2.5484e-02,  7.7419e-01,  2.0798e-01, -3.1418e-01,\n",
      "          -4.0864e-01,  1.2491e-01, -4.3754e-01, -1.6803e-01, -1.9910e-01,\n",
      "          -1.4706e-01, -3.6171e-02,  6.0602e-01, -9.4104e-02,  2.9479e-01,\n",
      "           5.6704e-01,  3.7619e-01, -3.4227e-01, -5.2888e-01,  8.7932e-02,\n",
      "           5.6161e-01, -1.7983e-01, -1.8024e-02,  5.0079e-01,  6.8799e-02,\n",
      "           2.1627e-02,  1.2896e-01,  2.3695e-01,  6.7287e-01, -2.9428e-01,\n",
      "           8.6235e-01, -1.8544e-01,  2.0267e-01,  3.5039e-01,  3.1597e-03],\n",
      "         [-2.3052e-01, -4.0319e-01, -6.1243e-01,  1.7308e-01,  1.3544e-01,\n",
      "          -1.7048e-01, -1.6197e-01, -5.4818e-01, -1.2836e-01, -1.5259e-01,\n",
      "           2.0827e-01,  1.8962e-01,  1.4469e-01,  2.3903e-01, -1.3379e-01,\n",
      "           1.9099e-01, -5.3400e-02, -1.3364e-02, -2.7705e-02,  2.0933e-01,\n",
      "          -1.5469e-01,  4.5734e-03,  2.6679e-02,  4.4927e-01, -6.3124e-02,\n",
      "           5.4026e-02, -2.8405e-01,  7.3940e-01,  1.5860e-01, -7.0800e-01,\n",
      "          -4.0566e-01,  2.0705e-01, -2.4807e-01, -2.2375e-01, -4.7448e-01,\n",
      "          -3.1465e-01, -2.4941e-01,  3.9949e-01, -2.5113e-01,  1.1999e-01,\n",
      "           4.8409e-01,  1.5385e-01, -2.2925e-01, -2.5623e-01,  2.0346e-01,\n",
      "           5.3327e-01,  6.8946e-02, -3.4326e-02,  4.4326e-01, -1.4642e-01,\n",
      "           5.5231e-02,  1.8962e-01,  6.6226e-01,  2.1606e-01, -1.1055e-01,\n",
      "           3.2210e-01,  2.3405e-01, -7.0151e-02,  4.2722e-01, -1.6268e-01],\n",
      "         [-4.3660e-01, -2.1421e-01, -7.6134e-01,  2.6542e-01,  4.1462e-01,\n",
      "          -2.5106e-01,  6.2619e-02, -5.1425e-01, -4.3557e-01, -1.2213e-01,\n",
      "           1.8812e-01,  2.9379e-01,  9.1710e-02,  5.4154e-01, -7.3245e-02,\n",
      "           2.8400e-01, -4.7074e-02, -1.9270e-01,  1.8023e-01,  3.6527e-01,\n",
      "           1.1155e-01, -1.9610e-01, -1.5411e-01,  7.0913e-01, -1.6268e-01,\n",
      "          -1.6129e-01, -3.4942e-01,  8.1704e-01,  1.2970e-01, -5.1690e-01,\n",
      "          -6.0063e-01,  1.4671e-01, -3.9452e-01, -2.2326e-01, -5.7247e-01,\n",
      "          -8.1430e-02, -3.6293e-01,  4.0360e-01, -4.9302e-01,  5.1540e-01,\n",
      "           4.7506e-01,  3.4914e-01, -3.7477e-02, -4.6021e-01,  8.8113e-02,\n",
      "           6.5256e-01,  3.1708e-01,  9.1900e-02,  3.6380e-01,  1.3275e-01,\n",
      "           2.6158e-01, -1.3910e-02,  7.3115e-01,  3.5822e-01, -3.0597e-01,\n",
      "           4.9795e-01,  3.8062e-01,  3.3678e-01,  5.1448e-01, -3.9024e-01],\n",
      "         [-3.5767e-01, -5.6883e-01, -5.9926e-01,  1.8799e-01,  2.4905e-01,\n",
      "          -3.0674e-01, -1.3578e-01, -4.9164e-01, -3.9050e-01, -4.6797e-02,\n",
      "           5.2744e-02,  3.5399e-01,  1.1200e-01,  6.8107e-01, -1.0683e-01,\n",
      "           2.2134e-01,  8.2066e-02, -9.5579e-02,  7.4342e-02, -1.7565e-02,\n",
      "           4.1721e-02,  1.7286e-01, -1.6653e-01,  4.9427e-01, -3.9377e-01,\n",
      "          -1.9823e-01, -1.7572e-01,  8.8042e-01,  6.4136e-02, -7.7763e-01,\n",
      "          -2.9625e-01,  2.5474e-01, -2.2640e-01, -3.4654e-01, -5.3607e-01,\n",
      "          -3.7286e-01, -4.7698e-01,  3.1023e-01, -8.1542e-02,  2.4265e-01,\n",
      "           6.3430e-01, -1.7206e-01, -3.4154e-01, -5.2978e-02,  3.6190e-01,\n",
      "           8.2324e-01,  2.7872e-01,  8.1731e-03,  2.2455e-01,  2.4829e-02,\n",
      "          -2.3680e-03, -6.6587e-02,  8.5860e-01,  4.9690e-01, -9.6889e-02,\n",
      "           3.7749e-01,  7.8653e-02,  1.4888e-01,  5.1296e-01,  9.5958e-02],\n",
      "         [-1.2317e-01, -1.8794e-01, -3.2911e-01,  8.1824e-02,  1.5725e-01,\n",
      "          -4.4863e-01, -1.3793e-01, -6.3611e-01, -1.4657e-01, -3.3360e-01,\n",
      "           1.9667e-01,  3.2326e-01, -1.3650e-01,  5.0600e-01, -2.6966e-01,\n",
      "           2.6218e-01, -1.4367e-01, -1.4137e-01,  1.3091e-02,  1.5194e-01,\n",
      "          -4.5997e-02, -9.5156e-02, -8.1432e-02,  2.5529e-01, -9.9771e-02,\n",
      "           6.7518e-02, -4.1073e-01,  4.5618e-01, -3.0794e-02, -5.0177e-01,\n",
      "          -1.5351e-01,  4.4290e-01, -3.0697e-01, -1.4737e-01, -2.4109e-01,\n",
      "          -3.3600e-01, -1.8473e-01, -1.3954e-01, -4.2270e-01, -1.9055e-02,\n",
      "           3.4103e-01, -2.9381e-02, -2.7487e-01, -6.9375e-01,  2.2621e-01,\n",
      "           2.9010e-01,  1.7742e-01,  2.1381e-02,  4.9380e-01,  9.8205e-03,\n",
      "           1.7030e-01,  1.0595e-01,  1.6247e-01,  5.3804e-01, -1.8621e-01,\n",
      "           7.3091e-01, -1.4874e-01,  1.0203e-01,  6.6244e-01, -6.5604e-02],\n",
      "         [-2.0377e-01,  7.5571e-02, -4.0125e-01, -4.9909e-02,  6.7775e-02,\n",
      "          -4.4500e-01, -3.6239e-01, -7.7063e-01, -2.3866e-01, -4.2290e-01,\n",
      "           5.2952e-01,  4.5980e-01, -2.4346e-01,  7.4970e-01, -2.5363e-01,\n",
      "           1.3620e-01,  9.5170e-02,  2.8533e-02,  7.8374e-02,  1.5035e-01,\n",
      "           8.6976e-02, -7.6348e-02, -5.0084e-02,  3.8803e-01, -2.1890e-01,\n",
      "           2.0852e-01, -6.7451e-01,  6.3761e-01, -1.1652e-01, -5.7073e-01,\n",
      "          -4.6956e-01,  9.0511e-02, -6.6071e-01,  8.4418e-02, -2.5795e-01,\n",
      "           1.6911e-01, -6.5944e-02,  3.3650e-01, -4.8768e-01,  1.3068e-01,\n",
      "           7.4833e-01,  2.3321e-01, -4.3294e-01, -8.0489e-01,  3.8203e-01,\n",
      "           5.2641e-01,  7.0361e-03,  2.1797e-01,  1.8405e-01,  7.4223e-02,\n",
      "           1.5883e-01, -2.2982e-01,  1.6075e-01,  3.9106e-01, -3.1870e-01,\n",
      "           6.2253e-01,  7.7685e-04,  4.2570e-01,  6.3244e-01, -1.2705e-01],\n",
      "         [-3.5552e-01, -1.0441e-01, -7.6128e-01,  2.0604e-01,  2.5355e-01,\n",
      "          -2.2459e-01,  1.0232e-01, -6.7618e-01, -2.1399e-01, -1.2865e-01,\n",
      "           6.1867e-01, -1.9307e-05, -1.8981e-01,  2.4178e-01,  6.5246e-03,\n",
      "           1.3088e-01, -2.4710e-01, -2.0641e-01, -2.0611e-02,  6.4992e-01,\n",
      "           7.2619e-02, -4.8067e-01, -6.6218e-02,  7.6286e-01,  1.6107e-01,\n",
      "          -1.3773e-01, -2.7418e-01,  6.7271e-01,  3.1354e-01, -3.4575e-01,\n",
      "          -5.6090e-01,  4.0014e-02, -5.4829e-01, -5.0164e-02, -2.8997e-01,\n",
      "          -3.4686e-02, -1.7623e-01,  5.2159e-01, -5.3626e-01,  5.7574e-01,\n",
      "           3.2673e-01,  6.8617e-01,  3.0607e-02, -6.4682e-01, -9.0659e-02,\n",
      "           3.2855e-01,  1.5959e-03,  5.1404e-02,  4.8325e-01,  1.1934e-01,\n",
      "           2.2936e-01,  8.0189e-02,  4.7251e-01,  4.0113e-01, -3.8873e-01,\n",
      "           7.6101e-01,  3.4051e-01,  2.7487e-01,  3.0770e-01, -3.7476e-01],\n",
      "         [-2.3441e-01, -4.1497e-01, -8.5001e-01, -8.0489e-02,  3.5077e-01,\n",
      "          -5.9369e-01,  1.5802e-01, -6.0485e-01, -1.6626e-01,  1.3474e-01,\n",
      "           6.2750e-01,  1.7781e-01, -3.1854e-01,  4.8877e-01, -1.9127e-01,\n",
      "           2.7915e-04, -1.4315e-01, -4.8932e-01, -1.5750e-01,  3.2640e-01,\n",
      "           2.8618e-02,  5.8248e-02, -2.8305e-01,  4.6863e-01, -6.7283e-02,\n",
      "          -2.3585e-01,  2.1488e-01,  8.8163e-01,  2.5119e-01, -4.4938e-01,\n",
      "          -1.7301e-01,  2.7820e-01, -3.5152e-01, -3.9689e-01, -4.5546e-01,\n",
      "          -3.1621e-01, -1.0851e-01,  3.6160e-01, -1.5165e-01,  3.2586e-01,\n",
      "           3.2704e-01,  1.8293e-01, -1.9985e-01, -4.4000e-01,  8.2821e-02,\n",
      "           6.0165e-01, -4.9153e-02, -1.7662e-01,  5.5274e-01,  1.6292e-01,\n",
      "          -8.4185e-02,  7.6626e-02,  3.6829e-01,  7.9023e-01, -4.4415e-01,\n",
      "           8.8321e-01, -2.2839e-01,  1.4016e-01,  5.3988e-01,  1.9768e-01],\n",
      "         [-1.7390e-01,  5.3897e-02, -5.8459e-01,  2.4746e-01,  3.1560e-02,\n",
      "          -2.2043e-01, -4.2275e-01, -7.6691e-01, -2.6976e-01, -1.3056e-01,\n",
      "           6.1950e-01,  1.7316e-01, -2.2050e-01,  6.8593e-01, -1.5545e-01,\n",
      "          -2.0239e-01,  6.7447e-03,  2.8755e-02, -2.2254e-01,  2.4314e-01,\n",
      "          -2.1036e-02, -4.3752e-02,  3.2991e-01,  4.9706e-01,  2.9063e-03,\n",
      "           2.0161e-01, -4.2394e-01,  4.2131e-01, -1.8830e-01, -4.6655e-01,\n",
      "          -1.8698e-01, -3.2970e-02, -3.6316e-01,  6.1194e-02, -2.4709e-01,\n",
      "          -1.7696e-01, -5.5531e-02,  2.5450e-01, -4.5995e-01, -8.3779e-02,\n",
      "           5.7088e-01,  3.5115e-01, -2.9992e-01, -8.1811e-01,  1.2363e-01,\n",
      "           3.9101e-01, -2.7515e-01,  1.4656e-01,  3.6353e-01, -1.4362e-01,\n",
      "           1.1648e-01,  2.8498e-01,  1.9161e-01,  2.5293e-01, -1.5459e-01,\n",
      "           3.5429e-01,  1.8742e-01,  2.4860e-01,  4.0239e-01, -1.6540e-01],\n",
      "         [-2.3027e-01, -6.5560e-01, -3.4989e-01,  3.1072e-01, -4.1595e-02,\n",
      "          -2.8232e-01, -1.8051e-01, -6.4033e-01, -3.0934e-01,  1.1799e-01,\n",
      "           3.0175e-01,  4.4339e-02, -2.5777e-01,  6.8660e-01,  2.6363e-02,\n",
      "          -5.4444e-02, -1.3585e-01, -8.0810e-02, -2.4550e-01,  3.3355e-02,\n",
      "          -1.1068e-03, -3.4513e-03,  1.7695e-02,  4.4750e-01, -2.3553e-01,\n",
      "          -3.2126e-01,  1.8815e-02,  5.2177e-01,  3.8610e-02, -6.5263e-01,\n",
      "           2.5374e-01,  2.6808e-01, -1.0954e-01, -2.0922e-01, -1.2327e-01,\n",
      "          -7.5243e-01, -4.9903e-01, -2.3159e-02,  3.4430e-02,  1.0909e-01,\n",
      "           3.7901e-01, -2.7584e-01, -3.5384e-01, -1.3404e-01,  2.3498e-01,\n",
      "           4.5388e-01,  1.4867e-02, -7.0563e-03,  2.8942e-01, -2.4282e-02,\n",
      "          -1.4684e-01,  1.0733e-01,  6.4730e-01,  7.9729e-01,  6.8506e-02,\n",
      "           5.8960e-01, -1.4090e-01,  4.7805e-02,  2.5119e-01,  4.7595e-01]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "x.shape: (1, 11, 60)\n",
      "q inital shape: (1, 11, 60)\n",
      "h: 10\n",
      "q final shape: (1, 11, 10, 6)\n",
      "kv 1st shape: (1, 11, 60)\n",
      "kv second shape: (1, 11, 2, 5, 6)\n",
      "case 2\n",
      "x.shape: (1, 11, 10, 6)\n",
      "(1, 11, 10, 3)\n",
      "(1, 11, 10, 3)\n",
      "(11, 1, 3)\n",
      "(11, 1, 3)\n",
      "q (1, 11, 10, 6)\n",
      "kv (1, 11, 2, 10, 6)\n",
      "k (1, 11, 10, 6)\n",
      "padding_mask: (1, 11)\n",
      "key_padding_mask: (1, 11)\n",
      "out: tf.Tensor(\n",
      "[[[-1.26699015e-01 -2.48235643e-01 -1.03785276e-01 -3.21122080e-01\n",
      "    6.38770536e-02 -6.93947792e-01  1.55249685e-01 -6.74385011e-01\n",
      "    5.73920131e-01 -2.28109341e-02  3.04323375e-01  1.12363882e-01\n",
      "    1.81124002e-01 -4.08033639e-01 -4.08270061e-01  3.91719759e-01\n",
      "    2.04285786e-01 -6.22382700e-01  6.79552332e-02  2.24444807e-01\n",
      "   -1.60770193e-01 -2.72028208e-01 -4.28321548e-02  3.86055224e-02\n",
      "    2.08725989e-01  1.13763019e-01 -1.50461555e-01  4.79057789e-01\n",
      "    1.79886758e-01 -7.75016785e-01 -9.00715962e-02  3.35175872e-01\n",
      "   -7.04239428e-01 -2.30713651e-01 -4.69734877e-01 -1.26489252e-01\n",
      "    5.75359799e-02 -7.67520592e-02 -6.24075413e-01 -1.54146343e-01\n",
      "    2.42738053e-01  1.22824281e-01 -2.03091443e-01 -5.94958186e-01\n",
      "    2.69620985e-01  2.08134547e-01 -1.74081206e-01 -3.72580111e-01\n",
      "    3.87206316e-01 -1.02738244e-02  1.70389432e-02  7.12544769e-02\n",
      "    1.68837845e-01  2.03772008e-01 -6.38637960e-01  7.18591094e-01\n",
      "   -3.62523012e-02  3.46217640e-02  8.78314376e-01  7.97141194e-02]\n",
      "  [-2.02041224e-01 -2.26504236e-01 -9.89413738e-01  3.06064859e-02\n",
      "    2.43808717e-01 -3.97065043e-01 -1.84483320e-01 -6.97693944e-01\n",
      "   -2.85395861e-01 -1.84495747e-01  8.24857712e-01  1.20339669e-01\n",
      "   -5.01953781e-01  6.81327581e-01 -1.59527540e-01 -1.29226521e-01\n",
      "   -1.93246260e-01 -1.21993512e-01 -2.33505785e-01  3.81597728e-01\n",
      "    1.33899465e-01  5.60485423e-02 -1.50450885e-01  6.03210032e-01\n",
      "   -8.25023185e-03 -1.30337954e-01 -2.54843496e-02  7.74194896e-01\n",
      "    2.07977369e-01 -3.14177424e-01 -4.08644170e-01  1.24909341e-01\n",
      "   -4.37537730e-01 -1.68029472e-01 -1.99102521e-01 -1.47064805e-01\n",
      "   -3.61708663e-02  6.06015563e-01 -9.41038057e-02  2.94785917e-01\n",
      "    5.67036390e-01  3.76191765e-01 -3.42271268e-01 -5.28880298e-01\n",
      "    8.79324302e-02  5.61609268e-01 -1.79831356e-01 -1.80239882e-02\n",
      "    5.00786722e-01  6.87988028e-02  2.16273069e-02  1.28961995e-01\n",
      "    2.36945748e-01  6.72868073e-01 -2.94278830e-01  8.62346947e-01\n",
      "   -1.85443386e-01  2.02665284e-01  3.50390822e-01  3.15966969e-03]\n",
      "  [-2.30518073e-01 -4.03193027e-01 -6.12430573e-01  1.73075706e-01\n",
      "    1.35442793e-01 -1.70476913e-01 -1.61968410e-01 -5.48180521e-01\n",
      "   -1.28355771e-01 -1.52588069e-01  2.08273605e-01  1.89618990e-01\n",
      "    1.44686386e-01  2.39033461e-01 -1.33787215e-01  1.90992042e-01\n",
      "   -5.34003414e-02 -1.33642275e-02 -2.77046971e-02  2.09329009e-01\n",
      "   -1.54693842e-01  4.57335310e-03  2.66791731e-02  4.49269623e-01\n",
      "   -6.31240979e-02  5.40261492e-02 -2.84047425e-01  7.39395499e-01\n",
      "    1.58597857e-01 -7.07996964e-01 -4.05656457e-01  2.07046166e-01\n",
      "   -2.48071447e-01 -2.23752707e-01 -4.74479258e-01 -3.14647585e-01\n",
      "   -2.49409303e-01  3.99493754e-01 -2.51129538e-01  1.19993128e-01\n",
      "    4.84087497e-01  1.53852731e-01 -2.29254395e-01 -2.56231248e-01\n",
      "    2.03455523e-01  5.33265591e-01  6.89459518e-02 -3.43259312e-02\n",
      "    4.43262160e-01 -1.46415070e-01  5.52312210e-02  1.89622372e-01\n",
      "    6.62259400e-01  2.16061503e-01 -1.10553622e-01  3.22095573e-01\n",
      "    2.34045014e-01 -7.01511279e-02  4.27224278e-01 -1.62677810e-01]\n",
      "  [-4.36599582e-01 -2.14210838e-01 -7.61341274e-01  2.65417427e-01\n",
      "    4.14619625e-01 -2.51063496e-01  6.26192987e-02 -5.14251709e-01\n",
      "   -4.35566932e-01 -1.22133039e-01  1.88124150e-01  2.93785334e-01\n",
      "    9.17102918e-02  5.41536927e-01 -7.32449368e-02  2.84004748e-01\n",
      "   -4.70740758e-02 -1.92698717e-01  1.80230260e-01  3.65272462e-01\n",
      "    1.11545280e-01 -1.96095869e-01 -1.54109120e-01  7.09129989e-01\n",
      "   -1.62677824e-01 -1.61292210e-01 -3.49416375e-01  8.17037582e-01\n",
      "    1.29700899e-01 -5.16901672e-01 -6.00634158e-01  1.46707505e-01\n",
      "   -3.94522756e-01 -2.23259613e-01 -5.72465479e-01 -8.14296380e-02\n",
      "   -3.62932891e-01  4.03603435e-01 -4.93021041e-01  5.15400827e-01\n",
      "    4.75056887e-01  3.49141598e-01 -3.74765731e-02 -4.60208833e-01\n",
      "    8.81129578e-02  6.52563632e-01  3.17076743e-01  9.18999389e-02\n",
      "    3.63798141e-01  1.32747814e-01  2.61580467e-01 -1.39102340e-02\n",
      "    7.31151998e-01  3.58216554e-01 -3.05968821e-01  4.97950673e-01\n",
      "    3.80617380e-01  3.36778015e-01  5.14479518e-01 -3.90242100e-01]\n",
      "  [-3.57665002e-01 -5.68831086e-01 -5.99262118e-01  1.87988877e-01\n",
      "    2.49046117e-01 -3.06738436e-01 -1.35780334e-01 -4.91640359e-01\n",
      "   -3.90499651e-01 -4.67968248e-02  5.27442843e-02  3.53985965e-01\n",
      "    1.11999385e-01  6.81065261e-01 -1.06828615e-01  2.21339002e-01\n",
      "    8.20654854e-02 -9.55785438e-02  7.43423998e-02 -1.75654497e-02\n",
      "    4.17207591e-02  1.72864959e-01 -1.66534498e-01  4.94269907e-01\n",
      "   -3.93769562e-01 -1.98228493e-01 -1.75718814e-01  8.80424261e-01\n",
      "    6.41361251e-02 -7.77634323e-01 -2.96251804e-01  2.54736841e-01\n",
      "   -2.26400644e-01 -3.46536994e-01 -5.36069512e-01 -3.72864306e-01\n",
      "   -4.76981163e-01  3.10225487e-01 -8.15419108e-02  2.42651135e-01\n",
      "    6.34304762e-01 -1.72059372e-01 -3.41542661e-01 -5.29775694e-02\n",
      "    3.61901700e-01  8.23238969e-01  2.78723061e-01  8.17301124e-03\n",
      "    2.24547505e-01  2.48287786e-02 -2.36801873e-03 -6.65865839e-02\n",
      "    8.58600080e-01  4.96903241e-01 -9.68885198e-02  3.77490878e-01\n",
      "    7.86530226e-02  1.48878127e-01  5.12956738e-01  9.59575325e-02]\n",
      "  [-1.23170361e-01 -1.87938899e-01 -3.29111069e-01  8.18244442e-02\n",
      "    1.57251298e-01 -4.48625714e-01 -1.37926608e-01 -6.36110604e-01\n",
      "   -1.46567568e-01 -3.33602160e-01  1.96665809e-01  3.23255479e-01\n",
      "   -1.36496499e-01  5.06003797e-01 -2.69663930e-01  2.62179375e-01\n",
      "   -1.43669397e-01 -1.41370445e-01  1.30907176e-02  1.51944757e-01\n",
      "   -4.59973030e-02 -9.51556265e-02 -8.14319625e-02  2.55287677e-01\n",
      "   -9.97707248e-02  6.75175041e-02 -4.10734266e-01  4.56184685e-01\n",
      "   -3.07935383e-02 -5.01770258e-01 -1.53511599e-01  4.42901909e-01\n",
      "   -3.06966305e-01 -1.47368193e-01 -2.41089076e-01 -3.36000890e-01\n",
      "   -1.84725493e-01 -1.39540151e-01 -4.22702283e-01 -1.90552268e-02\n",
      "    3.41025651e-01 -2.93805748e-02 -2.74873674e-01 -6.93752706e-01\n",
      "    2.26212770e-01  2.90102273e-01  1.77422538e-01  2.13807747e-02\n",
      "    4.93801951e-01  9.82047804e-03  1.70301035e-01  1.05948888e-01\n",
      "    1.62473083e-01  5.38037121e-01 -1.86206326e-01  7.30910242e-01\n",
      "   -1.48735851e-01  1.02030486e-01  6.62442744e-01 -6.56041875e-02]\n",
      "  [-2.03769758e-01  7.55712166e-02 -4.01247621e-01 -4.99088652e-02\n",
      "    6.77752942e-02 -4.45003510e-01 -3.62388968e-01 -7.70633399e-01\n",
      "   -2.38660485e-01 -4.22901541e-01  5.29515803e-01  4.59801883e-01\n",
      "   -2.43463337e-01  7.49699414e-01 -2.53628939e-01  1.36203781e-01\n",
      "    9.51701775e-02  2.85329111e-02  7.83744603e-02  1.50350809e-01\n",
      "    8.69756639e-02 -7.63475001e-02 -5.00839762e-02  3.88032198e-01\n",
      "   -2.18898892e-01  2.08515197e-01 -6.74514234e-01  6.37605906e-01\n",
      "   -1.16516821e-01 -5.70729673e-01 -4.69560772e-01  9.05109718e-02\n",
      "   -6.60709202e-01  8.44181851e-02 -2.57949084e-01  1.69107035e-01\n",
      "   -6.59443140e-02  3.36495370e-01 -4.87677127e-01  1.30676374e-01\n",
      "    7.48330653e-01  2.33214647e-01 -4.32936013e-01 -8.04892361e-01\n",
      "    3.82027984e-01  5.26412070e-01  7.03615556e-03  2.17970625e-01\n",
      "    1.84051692e-01  7.42231533e-02  1.58828139e-01 -2.29815826e-01\n",
      "    1.60749748e-01  3.91064286e-01 -3.18698645e-01  6.22525930e-01\n",
      "    7.76844390e-04  4.25703406e-01  6.32442057e-01 -1.27045095e-01]\n",
      "  [-3.55523646e-01 -1.04408093e-01 -7.61280656e-01  2.06036836e-01\n",
      "    2.53545403e-01 -2.24588841e-01  1.02320924e-01 -6.76178575e-01\n",
      "   -2.13992059e-01 -1.28650889e-01  6.18667066e-01 -1.92925381e-05\n",
      "   -1.89806789e-01  2.41784617e-01  6.52460754e-03  1.30876139e-01\n",
      "   -2.47100607e-01 -2.06408694e-01 -2.06114873e-02  6.49919689e-01\n",
      "    7.26193711e-02 -4.80666518e-01 -6.62178919e-02  7.62862325e-01\n",
      "    1.61071613e-01 -1.37727454e-01 -2.74176300e-01  6.72706425e-01\n",
      "    3.13539505e-01 -3.45754415e-01 -5.60895503e-01  4.00137752e-02\n",
      "   -5.48294842e-01 -5.01638539e-02 -2.89973497e-01 -3.46855149e-02\n",
      "   -1.76228553e-01  5.21591723e-01 -5.36256850e-01  5.75743377e-01\n",
      "    3.26727927e-01  6.86173797e-01  3.06070652e-02 -6.46817684e-01\n",
      "   -9.06586051e-02  3.28553885e-01  1.59590517e-03  5.14039397e-02\n",
      "    4.83245641e-01  1.19341999e-01  2.29361877e-01  8.01888332e-02\n",
      "    4.72510815e-01  4.01134729e-01 -3.88732046e-01  7.61014402e-01\n",
      "    3.40511233e-01  2.74867684e-01  3.07699054e-01 -3.74756575e-01]\n",
      "  [-2.34413117e-01 -4.14972603e-01 -8.50006580e-01 -8.04887339e-02\n",
      "    3.50773484e-01 -5.93691766e-01  1.58019066e-01 -6.04845822e-01\n",
      "   -1.66256964e-01  1.34738624e-01  6.27497017e-01  1.77811384e-01\n",
      "   -3.18536460e-01  4.88769531e-01 -1.91265166e-01  2.79165979e-04\n",
      "   -1.43153653e-01 -4.89319742e-01 -1.57497525e-01  3.26401293e-01\n",
      "    2.86184810e-02  5.82482554e-02 -2.83049822e-01  4.68629986e-01\n",
      "   -6.72832653e-02 -2.35854626e-01  2.14884520e-01  8.81632984e-01\n",
      "    2.51194656e-01 -4.49384809e-01 -1.73014447e-01  2.78196186e-01\n",
      "   -3.51523131e-01 -3.96893233e-01 -4.55459267e-01 -3.16206634e-01\n",
      "   -1.08509175e-01  3.61596018e-01 -1.51647076e-01  3.25862110e-01\n",
      "    3.27035755e-01  1.82928815e-01 -1.99845389e-01 -4.39996779e-01\n",
      "    8.28212127e-02  6.01650774e-01 -4.91530262e-02 -1.76621974e-01\n",
      "    5.52738547e-01  1.62921041e-01 -8.41844827e-02  7.66258389e-02\n",
      "    3.68286282e-01  7.90230155e-01 -4.44152147e-01  8.83210838e-01\n",
      "   -2.28393689e-01  1.40160874e-01  5.39878428e-01  1.97682574e-01]\n",
      "  [-1.73903689e-01  5.38971573e-02 -5.84594011e-01  2.47455195e-01\n",
      "    3.15601118e-02 -2.20432833e-01 -4.22745585e-01 -7.66912401e-01\n",
      "   -2.69761205e-01 -1.30559534e-01  6.19499147e-01  1.73159465e-01\n",
      "   -2.20495760e-01  6.85927570e-01 -1.55453905e-01 -2.02389494e-01\n",
      "    6.74468884e-03  2.87550874e-02 -2.22538725e-01  2.43137911e-01\n",
      "   -2.10361723e-02 -4.37521078e-02  3.29911590e-01  4.97062743e-01\n",
      "    2.90630595e-03  2.01614320e-01 -4.23941374e-01  4.21310782e-01\n",
      "   -1.88303903e-01 -4.66551036e-01 -1.86983526e-01 -3.29702795e-02\n",
      "   -3.63156289e-01  6.11942448e-02 -2.47085154e-01 -1.76958725e-01\n",
      "   -5.55314422e-02  2.54497170e-01 -4.59947944e-01 -8.37790370e-02\n",
      "    5.70883751e-01  3.51147264e-01 -2.99917281e-01 -8.18105638e-01\n",
      "    1.23631082e-01  3.91008824e-01 -2.75151610e-01  1.46559045e-01\n",
      "    3.63526970e-01 -1.43622011e-01  1.16478257e-01  2.84979939e-01\n",
      "    1.91611946e-01  2.52929240e-01 -1.54589236e-01  3.54288012e-01\n",
      "    1.87423125e-01  2.48600587e-01  4.02385652e-01 -1.65399134e-01]\n",
      "  [-2.30271265e-01 -6.55600965e-01 -3.49886447e-01  3.10723364e-01\n",
      "   -4.15955074e-02 -2.82323450e-01 -1.80505350e-01 -6.40331030e-01\n",
      "   -3.09338719e-01  1.17991857e-01  3.01745087e-01  4.43384796e-02\n",
      "   -2.57765770e-01  6.86600506e-01  2.63625998e-02 -5.44444397e-02\n",
      "   -1.35848954e-01 -8.08103904e-02 -2.45498866e-01  3.33553366e-02\n",
      "   -1.10673427e-03 -3.45129357e-03  1.76947061e-02  4.47496325e-01\n",
      "   -2.35530511e-01 -3.21263522e-01  1.88153591e-02  5.21772742e-01\n",
      "    3.86097170e-02 -6.52628839e-01  2.53742844e-01  2.68075109e-01\n",
      "   -1.09538682e-01 -2.09223151e-01 -1.23272412e-01 -7.52432466e-01\n",
      "   -4.99034375e-01 -2.31585950e-02  3.44296172e-02  1.09089091e-01\n",
      "    3.79005045e-01 -2.75843292e-01 -3.53838146e-01 -1.34041101e-01\n",
      "    2.34980062e-01  4.53876942e-01  1.48665532e-02 -7.05624931e-03\n",
      "    2.89420336e-01 -2.42824908e-02 -1.46840900e-01  1.07327208e-01\n",
      "    6.47299349e-01  7.97294319e-01  6.85055107e-02  5.89598596e-01\n",
      "   -1.40898690e-01  4.78047356e-02  2.51192719e-01  4.75951463e-01]]], shape=(1, 11, 60), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# looks good!\n",
    "\n",
    "# for now\n",
    "FlashRotaryEmbedding = None\n",
    "FusedDense = None\n",
    "FlashSelfAttention = None\n",
    "FlashCrossAttention = None\n",
    "\n",
    "\n",
    "class MyMHA(tf.Module):\n",
    "    def __init__(self, rotary_dim, rotary_base, rotary_scale_base,\n",
    "                 n_head, n_head_kv, head_dim, bias:bool, softmax_scale:float,\n",
    "                 layer_idx:int, return_residual:bool,\n",
    "                 flash_rot_emb:bool, n_positions, n_embd,\n",
    "                 fused_dense:bool,\n",
    "                 flash_attn:bool, weights,\n",
    "                 name=None):\n",
    "        super().__init__(name)\n",
    "        # Rotary embedding\n",
    "        self.rotary_dim = rotary_dim\n",
    "        if self.rotary_dim > 0:\n",
    "            rotary_cls = FlashRotaryEmbedding if flash_rot_emb else MyRotaryEmbedding\n",
    "            rotary_kwargs = {}\n",
    "            if rotary_cls is MyRotaryEmbedding:\n",
    "                rotary_kwargs[\"max_position_embeddings\"] = n_positions\n",
    "            self.rotary_emb =  rotary_cls(\n",
    "                self.rotary_dim,\n",
    "                base=rotary_base,\n",
    "                scale_base=rotary_scale_base,\n",
    "                pos_idx_in_fp32=True, # try the other way aswell\n",
    "                max_position_embeddings=2048\n",
    "            )\n",
    "        # MLP\n",
    "        self.n_head = n_head\n",
    "        self.n_head_kv = n_head_kv\n",
    "        self.head_dim = head_dim\n",
    "        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n",
    "        hidden_size = n_embd\n",
    "        linear_cls = FusedDense if fused_dense else bert.Dense_v2\n",
    "        self.Wqkv = linear_cls(hidden_size, op_size, weights=weights['Wqkv_weights'], bias=None)\n",
    "        self.out_proj = linear_cls(hidden_size, hidden_size, weights=weights['out_proj_weights'], bias=None)\n",
    "        # Attention\n",
    "        attn_cls = FlashSelfAttention if flash_attn else MySelfAttention\n",
    "        cross_attn_cls = FlashCrossAttention if flash_attn else MyCrossAttention\n",
    "        self.inner_attn = attn_cls(softmax_scale=softmax_scale)\n",
    "        self.inner_cross_attn = cross_attn_cls(softmax_scale=softmax_scale)\n",
    "        self.flash_attn = flash_attn and attn_cls is FlashSelfAttention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.return_residual = return_residual\n",
    "\n",
    "    def _forward_self_attn(self, x, key_padding_mask):\n",
    "        qkv = self.Wqkv(x)\n",
    "        h = qkv.shape[-1] // (3 * self.head_dim)\n",
    "        assert(h * 3 * self.head_dim == qkv.shape[-1])\n",
    "        qkv = tf.reshape(qkv, qkv.shape[:-1] + [3, h, self.head_dim])\n",
    "        if self.rotary_dim > 0:\n",
    "            qkv = self.rotary_emb(qkv)\n",
    "        return self.inner_attn(qkv, key_padding_mask=key_padding_mask)\n",
    "    \n",
    "    def _forward_cross_attn(self, x, past_key_values, key_padding_mask):\n",
    "        batch_size = x.shape[0]\n",
    "        print(\"x.shape:\", x.shape)\n",
    "        qkv = self.Wqkv(x)\n",
    "        q = qkv[..., : self.n_head * self.head_dim]\n",
    "        # print(\"q1:\", q)\n",
    "        print(\"q inital shape:\", q.shape)\n",
    "        h = q.shape[-1] // self.head_dim\n",
    "        print(\"h:\", h)\n",
    "        assert(h * self.head_dim == q.shape[-1])\n",
    "        q = tf.reshape(q, q.shape[:-1] + [h, self.head_dim])\n",
    "        # print(\"q2:\", q)\n",
    "        print(\"q final shape:\", q.shape)\n",
    "        kv = qkv[..., self.n_head * self.head_dim :]\n",
    "        # print(\"kv1:\", kv)\n",
    "        print(\"kv 1st shape:\", kv.shape)\n",
    "        hkv = kv.shape[-1] // (2 * self.head_dim)\n",
    "        assert(hkv * 2 * self.head_dim == kv.shape[-1])\n",
    "        kv = tf.reshape(kv, kv.shape[:-1] + [2, hkv, self.head_dim])\n",
    "        # print(\"kv2:\", kv)\n",
    "        print(\"kv second shape:\", kv.shape)\n",
    "        seqlen_offset = past_key_values.seqlen_offset if past_key_values is not None else 0\n",
    "        causal = None if seqlen_offset == 0 else False\n",
    "        # print(\"q:\", q)\n",
    "        # print(\"kv:\", kv)\n",
    "        if self.rotary_dim > 0:\n",
    "            q, kv = self.rotary_emb(q, kv=kv, seqlen_offset=seqlen_offset)\n",
    "        if past_key_values is not None:\n",
    "            kv = MY_update_kv_cache(kv, past_key_values, self.layer_idx)\n",
    "        # print(\"q:\", q)\n",
    "        # print(\"kv:\", kv)\n",
    "        return self.inner_cross_attn(q, kv, key_padding_mask=key_padding_mask)\n",
    "    def __call__(self, x, past_key_values, attention_mask):\n",
    "        # assert(attention_mask is not None)\n",
    "        # attention_mask = tf.cast(attention_mask, dtype=tf.bool)\n",
    "        # MHA\n",
    "        if self.n_head == self.n_head_kv:\n",
    "            if past_key_values is None:\n",
    "                # If `past_key_values` are not supplied, we run self-attention\n",
    "                attn_output = self._forward_self_attn(x, attention_mask)\n",
    "            else:\n",
    "                # If `past_key_values` are supplied, it means that we might have cached values and\n",
    "                # could take advantage of cross-attention\n",
    "                attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        # MQA / GQA\n",
    "        else:\n",
    "            # Regardless of `past_key_values` being supplied or not, it always use cross-attention\n",
    "            # because `q` and `kv` lengths might be different\n",
    "            attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        # print(\"attn_out:\", attn_output)\n",
    "        output = tf.reshape(attn_output, attn_output.shape[:-2] + [attn_output.shape[-1] * attn_output.shape[-2]])\n",
    "        output = self.out_proj(output)\n",
    "        # print(\"output:\", output)\n",
    "        return output if not self.return_residual else (output, x)\n",
    "\n",
    "rotary_dim = 5\n",
    "rotary_base = 10000.0\n",
    "rotary_scale_base = None # this seemsm neccesary\n",
    "n_head = 10\n",
    "n_head_kv = 5\n",
    "head_dim = 6\n",
    "bias = False\n",
    "softmax_scale = 2.3\n",
    "layer_idx = 2\n",
    "return_residual = False\n",
    "flash_rot_emb = False\n",
    "n_positions = 30 # idk what this should be\n",
    "n_embd = 60 # i think n_head * head_dim is required\n",
    "fused_dense = False\n",
    "flash_attn = False\n",
    "\n",
    "op_size = head_dim * (n_head + 2 * n_head_kv)\n",
    "print(\"op_size:\", op_size)\n",
    "\n",
    "# for their class\n",
    "class PhiConfig():\n",
    "    def __init__(self):\n",
    "        self.flash_rotary = False\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.fused_dense = False\n",
    "        self.flash_attn= False\n",
    "        self.attn_pdrop = 0\n",
    "\n",
    "\n",
    "config = PhiConfig()\n",
    "mha = MHA(config, torch.float32, 'cpu', rotary_dim, rotary_base, rotary_scale_base, \n",
    "          n_head, n_head_kv, head_dim, False, True, softmax_scale, layer_idx, False, False)\n",
    "\n",
    "weights = {}\n",
    "# weights['Wqkv_weights'] = tf.random.uniform((n_embd, op_size))\n",
    "# weights['out_proj_weights'] = tf.random.uniform((n_embd, n_embd))\n",
    "weights['Wqkv_weights'] = tf.constant(mha.Wqkv.weight.clone().detach().numpy())\n",
    "weights['Wqkv_weights'] = tf.transpose(weights['Wqkv_weights'])\n",
    "weights['out_proj_weights'] = tf.constant(mha.out_proj.weight.clone().detach().numpy())\n",
    "weights['out_proj_weights'] = tf.transpose(weights['out_proj_weights'])\n",
    "\n",
    "print(weights)\n",
    "\n",
    "my_mha = MyMHA(rotary_dim, rotary_base, rotary_scale_base,\n",
    "                 n_head, n_head_kv, head_dim, bias, softmax_scale,\n",
    "                 layer_idx, return_residual,\n",
    "                 flash_rot_emb, n_positions, n_embd,\n",
    "                 fused_dense,\n",
    "                 flash_attn, weights)\n",
    "\n",
    "key_padding_mask = torch.zeros((batch_size, seqlen_k), dtype=torch.float32)\n",
    "key_padding_mask[0,0] += 1\n",
    "key_padding_mask[0,1] += 1\n",
    "# key_padding_mask = key_padding_mask.to(bool)\n",
    "# key_padding_mask = tf.constant(key_padding_mask.numpy())\n",
    "\n",
    "# input = torch.randn([3,7,11, 13, n_embd])\n",
    "input = torch.randn([batch_size, seqlen_k, n_embd])\n",
    "\n",
    "out = mha(input, None, key_padding_mask)\n",
    "print(\"out:\", out)\n",
    "\n",
    "\n",
    "input = tf.constant(input.numpy())\n",
    "key_padding_mask = tf.constant(key_padding_mask.numpy())\n",
    "\n",
    "out = my_mha(x=input, past_key_values=None, attention_mask=key_padding_mask)\n",
    "print(\"out:\", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyParallelBlock(tf.Module):\n",
    "    def __init__(self, block_idx, name=None):\n",
    "        super().__init__(name)\n",
    "        self.ln = bert.LayerNorm(weights=None, biases=None, eps=None)\n",
    "        self.block_idx = block_idx\n",
    "        self.mixer = MyMHA()\n",
    "        self.mlp = MyMLP()\n",
    "    \n",
    "    def __call__(self, hidden_states, past_key_values, attention_mask):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        attn_outputs = self.mixer(\n",
    "            hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        if isinstance(attn_outputs, tuple):\n",
    "            attn_outputs = attn_outputs[0]\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCausalLMHead(tf.Module):\n",
    "    def __init__(self, n_embd, layer_norm_eps, vocab_size, name=None):\n",
    "        super().__init__(name)\n",
    "        self.ln = nn.LayerNorm(n_embd, eps=layer_norm_eps)\n",
    "        self.linear = bert.Dense_v2(n_embd, vocab_size)\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        logits = tf.cast(self.linear(hidden_states), dtype=tf.float32)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyPhiPreTrainedModel(tf.Module):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
