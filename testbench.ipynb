{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to microsoft phi-2 code (put something like this in final piece of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Embedding dimensions and calculations (done)\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Token embedding with dropout.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        hidden_states = self.wte(input_ids)\n",
    "        return hidden_states\n",
    "\n",
    "# testing py\n",
    "py_layer = Embedding(vocab_size=3000, n_embd=768)\n",
    "input = torch.randint(0, 3000, (4,))\n",
    "# print(py_layer(input))\n",
    "# testing tf\n",
    "matrix = py_layer.wte._parameters['weight'].data.numpy()\n",
    "matrix = tf.convert_to_tensor(matrix)\n",
    "tf_layer = bert.Embedding(4, 768, matrix)\n",
    "# print(tf_layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb() (done)\n",
    "def MY_apply_rotary_emb(x, cos, sin): # code mostly taken from original\n",
    "    assert(len(x.shape) == 4)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = tf.split(x_rot, 2, axis=-1) # assumption that tf.split behaves the same as nn.chunk\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    x1, x2, c, s = [tf.cast(t, tf.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = tf.concat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1)\n",
    "    x_rot = tf.cast(x_rot, x.dtype) # might fail due to passing in x.dtype\n",
    "    return tf.concat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    ") -> torch.FloatTensor:\n",
    "    # print(\"x.shape:\", x.shape)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = x_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    x1, x2, c, s = [t.to(dtype=torch.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = torch.cat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1).to(x.dtype)\n",
    "    return torch.cat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "last_dim = 5 # it seems this is neccesary\n",
    "x = torch.randn([2,3,2,last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "# print(_apply_rotary_emb(x, cos, sin))\n",
    "# print(\"-----\")\n",
    "x = tf.constant(x.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "# print(MY_apply_rotary_emb(x, cos, sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_kv (done)\n",
    "def MY_apply_rotary_emb_kv(kv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(kv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    k1, k2, c, s = [tf.cast(t, tf.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, kv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        kv[:, :, 1:2, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_kv(\n",
    "    kv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k= None,\n",
    "    sin_k= None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    k1, k2, c, s = [t.to(dtype=torch.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(kv.dtype)\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            kv[:, :, 1:2, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "kv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "kv = tf.constant(kv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "\n",
    "# print(MY_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_qkv (done)\n",
    "\n",
    "def MY_apply_rotary_emb_qkv(qkv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(qkv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = tf.split(q_rot, 2, axis=-1)\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    q1, q2, k1, k2, c, s = [tf.cast(t, tf.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = tf.concat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1)\n",
    "    q_rot = tf.cast(q_rot, qkv.dtype)\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, qkv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([q_rot, q_pass], axis=-1), axis=2),\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        qkv[:, :, 2:3, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_qkv(\n",
    "    qkv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k=None,\n",
    "    sin_k=None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    q1, q2, k1, k2, c, s = [t.to(dtype=torch.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = torch.cat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).to(qkv.dtype)\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(qkv.dtype)\n",
    "    return torch.cat(\n",
    "        [   torch.cat([q_rot, q_pass], axis=-1).unsqueeze(2),\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            qkv[:, :, 2:3, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "# print(MY_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing rotary embedding layer (done)\n",
    "class MyRotaryEmbedding(tf.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base: float = None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        name=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        self.inv_freq = self._compute_inv_freq()\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        self.scale = (\n",
    "            (tf.range(0, dim, 2, dtype=tf.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None)\n",
    "        self._update_cos_sin_cache(max_position_embeddings, dtype=tf.float32)\n",
    "\n",
    "    def _compute_inv_freq(self):\n",
    "        return 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(self, seqlen:int, dtype):\n",
    "        self._seq_len_cached = seqlen\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = tf.range(seqlen, dtype=tf.float32)\n",
    "            if self.inv_freq.dtype != tf.float32:\n",
    "                inv_freq = self._compute_inv_freq()\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = tf.range(seqlen, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        # tensorflow does not appear to do what the top comment states\n",
    "        freqs = tf.einsum('i,j->ij', t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = tf.cast(tf.cos(freqs), dtype=dtype)\n",
    "            self._sin_cached = tf.cast(tf.sin(freqs), dtype=dtype)\n",
    "        else:\n",
    "            power = (tf.range(seqlen, dtype=self.scale.dtype) - seqlen // 2) / self.scale_base\n",
    "            scale = self.scale ** tf.expand_dims(power, axis=1)\n",
    "            # Force the scale multiplciation to happen in fp32\n",
    "            self._cos_cached = tf.cast((tf.cos(freqs) * scale), dtype=dtype)\n",
    "            self._sin_cached = tf.cast((tf.sin(freqs) * scale), dtype=dtype)\n",
    "            self._cos_k_cached = tf.cast((tf.cos(freqs) / scale), dtype=dtype)\n",
    "            self._sin_k_cached = tf.cast((tf.sin(freqs) / scale), dtype=dtype)\n",
    "    \n",
    "    def __call__(self, qkv, kv, seqlen_offset, **kwargs):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            # or (self.training and self._cos_cached.is_inference()) # look into this\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return MY_apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = MY_apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            kv = MY_apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            return q, kv\n",
    "    ################################\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Rotary positional embedding (RoPE).\n",
    "    Reference:\n",
    "        RoFormer: Enhanced Transformer with Rotary Position Embedding.\n",
    "        https://arxiv.org/pdf/2104.09864.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base= None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        device = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if scale_base is not None:\n",
    "            raise NotImplementedError\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.device = device\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        inv_freq = self._compute_inv_freq(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        scale = (\n",
    "            (torch.arange(0, dim, 2, device=device, dtype=torch.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None\n",
    "        )\n",
    "        self.register_buffer(\"scale\", scale, persistent=False)\n",
    "\n",
    "        # Initialize cached attributes since ONNX can't rely on dynamic initialization\n",
    "        self._update_cos_sin_cache(max_position_embeddings, device=device, dtype=torch.float32)\n",
    "\n",
    "    def _compute_inv_freq(self, device: Optional[str] = None) -> torch.FloatTensor:\n",
    "        return 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> None:\n",
    "        self._seq_len_cached = seqlen\n",
    "\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
    "            if self.inv_freq.dtype != torch.float32:\n",
    "                inv_freq = self._compute_inv_freq(device=device)\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = torch.cos(freqs).to(dtype)\n",
    "            self._sin_cached = torch.sin(freqs).to(dtype)\n",
    "        else:\n",
    "            power = (\n",
    "                torch.arange(seqlen, dtype=self.scale.dtype, device=self.scale.device) - seqlen // 2\n",
    "            ) / self.scale_base\n",
    "            scale = self.scale.to(device=power.device) ** rearrange(power, \"s -> s 1\")\n",
    "\n",
    "            # Force the scale multiplication to happen in fp32\n",
    "            self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
    "            self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
    "            self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
    "            self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.Tensor,\n",
    "        kv: Optional[torch.Tensor] = None,\n",
    "        seqlen_offset: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            or (self.training and self._cos_cached.is_inference())\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, device=qkv.device, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return _apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = _apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "            kv = _apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "\n",
    "            return q, kv\n",
    "\n",
    "\n",
    "dim = 5\n",
    "base = 5000\n",
    "scale_base = None\n",
    "pos_idx_in_fp32 = False\n",
    "max_position_embeddings = 2048\n",
    "device = 'cpu'\n",
    "\n",
    "rot_emb = RotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings, device)\n",
    "tf_rot_emb = MyRotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings)\n",
    "\n",
    "# test with kv being something, and being None\n",
    "\n",
    "# it appears that if kv will be none, then qkv must be 4d rather than 5d\n",
    "\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "kv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "seqlen_offset = 4\n",
    "print(rot_emb(qkv, kv, seqlen_offset))\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "kv = tf.constant(kv.numpy())\n",
    "print(tf_rot_emb(qkv, kv, seqlen_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Linear Layer\n",
    "layer = torch.nn.Linear(3, 5)\n",
    "input = torch.randn([7,3])\n",
    "print(layer(input))\n",
    "weights = tf.cast(torch.detach(layer.weight).numpy(), dtype=tf.float32)\n",
    "bias = tf.cast(torch.detach(layer.bias).numpy(), dtype=tf.float32)\n",
    "linear = bert.Dense(3, 5, weights, is_biased=True, bias=bias)\n",
    "input = tf.cast(input.numpy(), dtype=tf.float32)\n",
    "print(linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP (need to fill in the weight, bias, and activation)\n",
    "\n",
    "class MyMLP(tf.Module):\n",
    "    def __init__(self, n_inner:int, n_embd:int, name=None): # manually written in act-fn()\n",
    "        super().__init__(name)\n",
    "        self.fc1 = bert.Dense(n_embd, n_inner, weights=None, is_biased=True, bias=None)\n",
    "        self.fc2 = bert.Dense(n_inner, n_embd, weights=None, is_biased=True, bias=None)\n",
    "        self.act = None # find out later\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: I dont know the notation of the bool mask (does \"true\" mean to mask or not)\n",
    "\n",
    "# assumption that must be checked: assume that True means to replace\n",
    "def mask_fill(matrix, mask, value):\n",
    "    negmask = 1 - mask\n",
    "    return (matrix * mask) + (negmask * value)\n",
    "\n",
    "class MySelfAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale:float, name=None):\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, qkv, key_padding_mask=None):\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = tf.unstack(qkv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        softmax_scale = self.softmax_scale\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen), -10000.0, dtype=scores.dtype)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 0.0)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # i am assuming this to be causal\n",
    "        causal_mask = tf.experimental.numpy.triu(tf.fill((seqlen, seqlen), -10000.0), 1) # might need to be replaced\n",
    "        scores = scores + tf.cast(causal_mask, dtype=scores.dtype)\n",
    "        attention = tf.cast(tf.softmax(scores, dim=-1), dtype=v.dtype)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCrossAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale, name=None): # assume to be causal\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, q, kv, key_padding_mask):\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        if kv.shape[3] != kv.shape[2]:\n",
    "            kv = tf.repeat(x, repeats=q.shape[2] // kv.shape[3], axis=-2)\n",
    "        k, v = tf.unstack(kv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * self.softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen_k), -10000.0, dtype=scores.dtype)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 0.0)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # causal stuff\n",
    "        rows = tf.expand_dims(tf.range(seqlen_q, dtype=tf.long), axis=1) # if this fails, do axis=-1\n",
    "        cols = tf.range(seqlen_k, dtype=tf.long)\n",
    "        causal_mask = cols > rows + seqlen_k - seqlen_q\n",
    "        scores = mask_fill(scores, causal_mask, -10000.0)\n",
    "        # end of causal stuff\n",
    "        attention = tf.softmax(scores, axis=-1, dtype=v.dtype)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MY_update_kv_cache(kv, inference_params, layer_idx:int):\n",
    "    num_heads, head_dim = kv.shape[-2:]\n",
    "    if layer_idx not in inference_params.key_value_memory_dict:\n",
    "        # during the UDO desgin process, could use uninitalized memory rather than zeros\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.zeros(\n",
    "            [\n",
    "                inference_params.max_batch_size,\n",
    "                inference_params.max_seqlen,\n",
    "                2,\n",
    "                num_heads,\n",
    "                head_dim\n",
    "            ],\n",
    "            dtype=kv.dtype\n",
    "        )\n",
    "    batch_start = inference_params.batch_size_offset\n",
    "    batch_end = batch_start + kv.shape[0]\n",
    "    sequence_start = inference_params.seqlen_offset\n",
    "    sequence_end = sequence_start + kv.shape[1]\n",
    "    # When the current sequence length is equal to or larger than the maximum sequence length,\n",
    "    # we need to concatenate the current `kv` with the cached `kv` to expand its length\n",
    "    if sequence_end >= inference_params.max_seqlen:\n",
    "        # the line below might fail due to the tuple\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.concat((inference_params.key_value_memory_dict[layer_idx], kv), axis=1)\n",
    "    inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, sequence_start:sequence_end, ...] = kv\n",
    "    kv = inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, :sequence_end, ...]\n",
    "    return kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4191062565.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class MyMHA(tf.Module)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class MyMHA(tf.Module):\n",
    "    def __init__(self, dtype, rotary_dim, rotary_base, rotary_scale_base,\n",
    "                 n_head, n_head_kv, head_dim, bias:bool, softmax_scale:float,\n",
    "                 layer_idx:int, return_residual:bool, checkpointing:bool, \n",
    "                 flash_rot_emb:bool, n_positions, n_embd,\n",
    "                 fused_dense:bool,\n",
    "                 flash_attn:bool,\n",
    "                 name=None):\n",
    "        super().__init__(name)\n",
    "        # Rotary embedding\n",
    "        self.rotary_dim = rotary_dim\n",
    "        if self.rotary_dim > 0:\n",
    "            rotary_cls = FlashRotaryEmbedding if flash_rot_emb else MyRotaryEmbedding\n",
    "            rotary_kwargs = {}\n",
    "            if rotary_cls is RotaryEmbedding:\n",
    "                rotary_kwargs[\"max_position_embeddings\"] = n_positions\n",
    "            self.rotary_emb =  rotary_cls(\n",
    "                self.rotary_dim,\n",
    "                base=rotary_base,\n",
    "                scale_base=rotary_scale_base,\n",
    "                kwargs= **rotary_kwargs,\n",
    "            )\n",
    "        # MLP\n",
    "        self.n_head = n_head\n",
    "        self.n_head_kv = n_head_kv\n",
    "        self.head_dim = head_dim\n",
    "        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n",
    "        hidden_size = n_embd\n",
    "        linear_cls = FusedDense if fused_dense else bert.Dense\n",
    "        self.Wqkv = linear_cls(hidden_size, op_size, weights=None,  \n",
    "                               is_biased=True, bias=None)\n",
    "        self.out_proj = linear_cls(hidden_size, hidden_size, weights=None,  \n",
    "                               is_biased=True, bias=None)\n",
    "        # Attention\n",
    "        attn_cls = FlashSelfAttention if flash_attn else MySelfAttention\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
