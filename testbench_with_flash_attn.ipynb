{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 22:14:45.594009: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 22:14:45.628960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 22:14:46.356910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to microsoft phi-2 code (put something like this in final piece of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 22:14:47.647145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-29 22:14:47.647685: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# testing Embedding dimensions and calculations (done)\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Token embedding with dropout.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        hidden_states = self.wte(input_ids)\n",
    "        return hidden_states\n",
    "\n",
    "# testing py\n",
    "py_layer = Embedding(vocab_size=3000, n_embd=768)\n",
    "input = torch.randint(0, 3000, (4,))\n",
    "# print(py_layer(input))\n",
    "# testing tf\n",
    "matrix = py_layer.wte._parameters['weight'].data.numpy()\n",
    "matrix = tf.convert_to_tensor(matrix)\n",
    "tf_layer = bert.Embedding(4, 768, matrix)\n",
    "# print(tf_layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb() (done)\n",
    "def MY_apply_rotary_emb(x, cos, sin): # code mostly taken from original\n",
    "    print(\"x.shape:\", x.shape)\n",
    "    assert(len(x.shape) == 4)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = tf.split(x_rot, 2, axis=-1) # assumption that tf.split behaves the same as nn.chunk\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    x1, x2, c, s = [tf.cast(t, tf.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = tf.concat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1)\n",
    "    x_rot = tf.cast(x_rot, x.dtype) # might fail due to passing in x.dtype\n",
    "    return tf.concat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    ") -> torch.FloatTensor:\n",
    "    # print(\"x.shape:\", x.shape)\n",
    "    _, seqlen, _, _ = x.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    x_rot = x[:, :, :, :rotary_dim]\n",
    "    # print(\"x_rot\", x_rot.shape)\n",
    "    x_pass = x[:, :, :, rotary_dim:]\n",
    "    x1, x2 = x_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    x1, x2, c, s = [t.to(dtype=torch.float32) for t in [x1, x2, c, s]]\n",
    "    for i in [x1, x2, c, s]: print(i.shape)\n",
    "    x_rot = torch.cat([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1).to(x.dtype)\n",
    "    return torch.cat([x_rot, x_pass], axis=-1)\n",
    "\n",
    "last_dim = 5 # it seems this is neccesary\n",
    "x = torch.randn([2,3,2,last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "# print(_apply_rotary_emb(x, cos, sin))\n",
    "# print(\"-----\")\n",
    "x = tf.constant(x.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "# print(MY_apply_rotary_emb(x, cos, sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_kv (done)\n",
    "def MY_apply_rotary_emb_kv(kv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(kv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    k1, k2, c, s = [tf.cast(t, tf.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, kv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        kv[:, :, 1:2, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_kv(\n",
    "    kv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k= None,\n",
    "    sin_k= None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = kv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    k_rot = kv[:, :, 0, :, :rotary_dim]\n",
    "    k_pass = kv[:, :, 0, :, rotary_dim:]\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    k1, k2, c, s = [t.to(dtype=torch.float32) for t in [k1, k2, c, s]]\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(kv.dtype)\n",
    "    return torch.cat(\n",
    "        [\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            kv[:, :, 1:2, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "kv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "kv = tf.constant(kv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "\n",
    "# print(MY_apply_rotary_emb_kv(kv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing _apply_rotary_emb_qkv (done)\n",
    "\n",
    "def MY_apply_rotary_emb_qkv(qkv, cos, sin, cos_k=None, sin_k=None):\n",
    "    assert(len(qkv.shape) == 5)\n",
    "    assert(len(cos.shape) == 2)\n",
    "    if cos_k is not None: assert(len(cos_k.shape) == 4)\n",
    "    if cos_k is not None: assert(cos_k.shape == sin_k.shape)\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = tf.split(q_rot, 2, axis=-1)\n",
    "    k1, k2 = tf.split(k_rot, 2, axis=-1)\n",
    "    c = tf.expand_dims(cos[:seqlen], axis=1)\n",
    "    s = tf.expand_dims(sin[:seqlen], axis=1)\n",
    "    q1, q2, k1, k2, c, s = [tf.cast(t, tf.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = tf.concat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1)\n",
    "    q_rot = tf.cast(q_rot, qkv.dtype)\n",
    "    k_rot = tf.concat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1)\n",
    "    k_rot = tf.cast(k_rot, qkv.dtype)\n",
    "    return tf.concat([\n",
    "        tf.expand_dims(tf.concat([q_rot, q_pass], axis=-1), axis=2),\n",
    "        tf.expand_dims(tf.concat([k_rot, k_pass], axis=-1), axis=2),\n",
    "        qkv[:, :, 2:3, :, :],\n",
    "    ], axis=2)\n",
    "\n",
    "def _apply_rotary_emb_qkv(\n",
    "    qkv: torch.FloatTensor,\n",
    "    cos: torch.FloatTensor,\n",
    "    sin: torch.FloatTensor,\n",
    "    cos_k=None,\n",
    "    sin_k=None,\n",
    ") -> torch.FloatTensor:\n",
    "    _, seqlen, _, _, _ = qkv.shape\n",
    "    _, rotary_dim = cos.shape\n",
    "    rotary_dim *= 2\n",
    "    q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "    q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "    k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "    k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "    q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "    k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "    c, s = rearrange(cos[:seqlen], \"s d -> s 1 d\"), rearrange(sin[:seqlen], \"s d -> s 1 d\")\n",
    "    q1, q2, k1, k2, c, s = [t.to(dtype=torch.float32) for t in [q1, q2, k1, k2, c, s]]\n",
    "    q_rot = torch.cat([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).to(qkv.dtype)\n",
    "    k_rot = torch.cat([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).to(qkv.dtype)\n",
    "    return torch.cat(\n",
    "        [   torch.cat([q_rot, q_pass], axis=-1).unsqueeze(2),\n",
    "            torch.cat([k_rot, k_pass], axis=-1).unsqueeze(2),\n",
    "            qkv[:, :, 2:3, :, :],\n",
    "        ],\n",
    "        axis=2,\n",
    "    )\n",
    "\n",
    "last_dim = 5 \n",
    "qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "cos = torch.randn([4,last_dim])\n",
    "sin = torch.randn([4,last_dim])\n",
    "cos_k = torch.randn([4, 5, 6, last_dim])\n",
    "sin_k = torch.randn([4, 5, 6, last_dim])\n",
    "# print(_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))\n",
    "# print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "cos = tf.constant(cos.numpy())\n",
    "sin = tf.constant(sin.numpy())\n",
    "cos_k = tf.constant(cos_k.numpy())\n",
    "sin_k = tf.constant(sin_k.numpy())\n",
    "# print(MY_apply_rotary_emb_kv(qkv, cos, sin, cos_k, sin_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kernal1/my_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 2\n",
      "torch.Size([2, 3, 8, 3])\n",
      "torch.Size([2, 3, 8, 3])\n",
      "torch.Size([3, 1, 3])\n",
      "torch.Size([3, 1, 3])\n",
      "(tensor([[[[-1.7900, -0.7851, -0.2578,  0.3732, -1.2666, -0.0702, -1.6105,\n",
      "            0.6977, -2.3031,  1.2280],\n",
      "          [ 0.9764,  1.0402, -0.6257, -1.9324,  1.6229,  0.8316,  0.7740,\n",
      "            0.5860,  0.2650,  0.8752],\n",
      "          [ 0.7895, -0.3337,  0.4225, -0.3911,  0.6514, -0.2907, -0.4883,\n",
      "            0.3681,  0.1783,  1.7626],\n",
      "          [-1.3311,  0.4135,  0.9900,  0.0073,  2.2774,  1.5734,  0.7218,\n",
      "           -1.0520, -0.7684,  1.5304],\n",
      "          [-0.4609, -0.0834,  0.0286,  0.7884,  1.3756, -1.2021,  0.0081,\n",
      "           -0.2050,  1.0963, -0.1612],\n",
      "          [-0.0833, -0.6115, -0.2220,  2.2724,  0.5622, -1.3402, -0.0409,\n",
      "            1.5951, -0.5891, -0.3403],\n",
      "          [ 0.0319, -1.2722, -0.1970,  0.5765, -1.1703,  1.0729, -0.2438,\n",
      "            1.0372,  3.3739,  0.0157],\n",
      "          [-0.1301,  1.1609, -0.2562,  0.6764,  0.0621,  1.0037,  1.2254,\n",
      "            0.1864,  1.4947, -1.3250]],\n",
      "\n",
      "         [[-0.0198, -0.0520,  1.7987, -1.4198,  2.4551, -0.4790, -0.7629,\n",
      "            0.1062,  1.8963,  0.9034],\n",
      "          [ 0.2052,  0.4063, -1.3941, -0.7557, -2.0030, -0.1812,  0.7116,\n",
      "           -0.0421, -0.5404,  1.9608],\n",
      "          [ 0.6809, -0.7048,  0.7469,  2.4521, -1.4035,  0.8383, -1.4716,\n",
      "            0.3919,  0.4698, -0.9241],\n",
      "          [ 0.3043,  0.5259, -0.5492,  0.1427,  0.3225, -0.5511,  1.4986,\n",
      "            0.3449,  1.7364, -0.4868],\n",
      "          [ 1.2371, -1.7818,  0.4990, -0.0052,  0.1866,  0.2836,  1.0265,\n",
      "            2.4461,  0.7718, -0.0132],\n",
      "          [ 0.9976,  0.1792,  0.1466,  1.1009, -0.8758, -0.9096, -0.9719,\n",
      "           -1.5163,  0.8105,  0.4971],\n",
      "          [ 1.2322, -1.5211, -0.7678,  0.7375, -0.0388, -0.2063,  0.1138,\n",
      "           -0.3520, -0.3117, -1.8612],\n",
      "          [-0.6266,  0.6872,  0.8070, -0.9634,  0.3022, -0.3698,  0.4961,\n",
      "           -0.8231, -0.7070,  2.4077]],\n",
      "\n",
      "         [[-0.2973, -0.6732, -1.5734, -1.6208, -0.0769, -0.2059,  0.4818,\n",
      "           -0.9399, -0.0743, -0.9496],\n",
      "          [-1.2213, -0.5117,  0.0203,  0.8394,  0.7829, -0.0937,  0.2887,\n",
      "           -0.1345,  1.4160, -1.1560],\n",
      "          [-0.9081, -0.7749, -0.8837, -2.0130, -0.8777,  1.0946,  0.2996,\n",
      "           -0.4259, -0.6221, -0.2435],\n",
      "          [ 1.2004, -1.0232, -1.5494, -0.7811, -0.0340, -0.8527,  0.9800,\n",
      "           -0.2280, -0.4405, -0.5604],\n",
      "          [-1.2664,  1.5334, -0.6225, -0.9634,  0.7781, -0.9500,  1.9630,\n",
      "            2.5674,  0.5124, -1.1503],\n",
      "          [-0.9824,  0.5518, -0.3365, -0.6780, -0.9656,  0.3253,  0.1181,\n",
      "            1.6996,  1.2724, -0.1180],\n",
      "          [ 0.4514, -0.9982, -0.6907, -1.3417, -0.9755, -0.4850,  0.8938,\n",
      "            0.1501,  1.0308, -1.3876],\n",
      "          [ 0.4214,  1.5899,  0.2340, -0.5349, -1.0062, -0.1996,  0.2899,\n",
      "            0.0235,  1.4108, -2.4048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0489, -0.0065, -0.0496, -0.1869,  1.6754, -0.2317, -0.5344,\n",
      "           -1.1790, -1.8004, -0.5399],\n",
      "          [-0.5884,  0.1550, -0.3384, -1.7056, -0.1641,  0.0612, -2.3714,\n",
      "           -0.8875, -2.2114,  0.1818],\n",
      "          [-0.2718,  1.2595, -0.7524, -0.3492,  1.1456, -0.3672,  0.9983,\n",
      "            0.4643, -0.6188, -0.9003],\n",
      "          [-1.0872, -0.2247, -1.1108,  1.1025,  0.6469,  0.1198,  0.6914,\n",
      "           -0.6877,  0.2419, -0.5705],\n",
      "          [-0.2879,  0.6446, -0.5352,  1.3227,  0.0423, -0.0862, -0.8671,\n",
      "           -1.1183, -2.3597, -0.5745],\n",
      "          [ 0.7637, -0.5342,  0.0095, -0.6412, -1.3186,  1.3232, -2.2136,\n",
      "           -1.9101, -0.0816, -0.7608],\n",
      "          [-0.2188,  0.3538,  0.9828, -0.7178,  0.7359, -0.5351, -0.4929,\n",
      "            1.0866, -1.1744, -0.5394],\n",
      "          [-1.0267,  0.2662, -0.5837, -1.0132, -0.3003, -1.2964, -0.8422,\n",
      "            0.5628, -1.4887,  0.5176]],\n",
      "\n",
      "         [[ 1.7902, -0.7419,  0.3756, -0.8423, -0.9441, -1.4751, -0.9029,\n",
      "            2.0755, -1.1494, -0.0651],\n",
      "          [ 0.7926, -0.7244, -0.3825,  1.7372,  1.5301, -0.5841,  0.8706,\n",
      "            0.5814, -0.9609,  2.3411],\n",
      "          [ 2.1009, -0.5190,  0.2884, -1.1749, -0.1102, -0.0638,  0.0235,\n",
      "            0.5657, -1.4749,  1.3723],\n",
      "          [-1.1174,  0.3470, -1.7696, -0.9990,  0.3884, -0.8974, -0.2956,\n",
      "            1.5135, -1.7674,  0.5799],\n",
      "          [ 0.4579, -1.2583,  0.0089,  0.1486, -2.2226,  0.4786,  2.5153,\n",
      "           -1.1326, -0.0374, -1.9075],\n",
      "          [ 0.4196, -1.4803, -0.9345, -0.5218, -0.5769,  0.1500, -0.5977,\n",
      "           -1.4519,  0.1254, -2.4961],\n",
      "          [ 1.9090,  0.6071, -0.8159,  1.7759,  0.9492,  1.7767, -1.1542,\n",
      "           -0.6704,  1.5430, -0.1471],\n",
      "          [ 0.7146, -0.7457,  0.6684,  1.3074,  0.3498,  0.2907,  0.7028,\n",
      "            0.1356, -0.0381, -0.2684]],\n",
      "\n",
      "         [[-1.6529,  0.5631, -0.9408,  0.3208,  0.6952,  0.5373, -0.1230,\n",
      "            0.1863, -0.0692,  0.5975],\n",
      "          [ 1.2834, -1.1599, -0.2468,  0.3322,  0.0633, -0.9493,  0.2932,\n",
      "           -0.5703, -0.0945,  0.2778],\n",
      "          [-0.6316,  0.6260, -1.6503,  0.5673, -0.8486, -0.5158,  0.9095,\n",
      "           -0.7123,  0.9317, -1.0609],\n",
      "          [-0.0197,  0.3150, -0.3962, -0.5109, -0.2108,  1.8089,  1.3569,\n",
      "            0.1271,  0.2128,  0.3412],\n",
      "          [-0.1821, -0.2636,  1.2732, -0.9557, -2.4428, -0.6416, -1.0359,\n",
      "           -0.0129,  0.4843, -0.8070],\n",
      "          [ 1.3370, -0.8098, -2.2075, -1.1507, -1.7048, -0.0792, -0.5289,\n",
      "            0.6580,  0.0711,  0.0601],\n",
      "          [ 1.1052,  0.1325,  0.8747, -0.7089,  0.3680, -0.2541, -0.0128,\n",
      "            0.4910,  0.7944,  0.0697],\n",
      "          [-0.4268, -0.9952, -0.5605,  0.5880, -0.4743,  0.8925,  0.0309,\n",
      "           -2.1619,  0.1057, -1.6146]]]]), tensor([[[[[-1.5141, -0.5371,  0.3328, -0.8269, -0.4914, -0.6907, -0.9422,\n",
      "             1.7612,  1.3763, -0.0525],\n",
      "           [-1.2917, -1.2310, -1.0652,  0.7717, -1.0233,  0.0348,  1.1597,\n",
      "            -1.0584, -1.8114, -0.6160],\n",
      "           [-0.0243,  1.2752, -0.1849,  0.4256,  2.1595,  1.2542, -0.4819,\n",
      "             0.3618,  0.6781, -0.7912],\n",
      "           [ 1.5617, -0.4763,  0.7299, -0.7849,  1.0119, -0.0900,  1.0130,\n",
      "            -2.9864,  1.1855, -2.0264],\n",
      "           [ 0.5759, -1.5551, -0.9217,  0.2548, -1.6918,  0.5947, -1.1621,\n",
      "             1.5926, -0.6021, -0.8925],\n",
      "           [ 1.4730,  1.1024, -1.5023,  0.4560,  0.1606,  0.0199,  0.9853,\n",
      "            -0.6396, -0.4595, -0.9795],\n",
      "           [ 0.6193, -1.0172, -0.5228, -1.1356,  0.0508,  0.0504, -0.3081,\n",
      "            -0.4202, -0.0558, -0.1012],\n",
      "           [-1.1282, -0.9864, -0.2501,  0.7154, -0.9900, -0.9008,  0.7092,\n",
      "             0.7820,  1.3778,  0.0114]],\n",
      "\n",
      "          [[ 0.5117, -0.1737,  1.1511, -0.0126, -0.5271,  0.8508, -1.3610,\n",
      "            -2.0762, -1.0542, -0.8182],\n",
      "           [ 0.2084,  0.4908, -0.1667, -0.4492, -0.6213, -0.4322,  0.7219,\n",
      "             2.8600,  1.0068, -1.7260],\n",
      "           [-1.9933,  0.6158,  0.2153,  0.2611, -0.4418,  0.5406, -0.6553,\n",
      "             0.7854,  0.1910, -1.2008],\n",
      "           [ 0.3864, -1.7844, -0.1037,  0.5754, -0.9217, -0.6144, -1.0048,\n",
      "            -1.7077, -0.0541, -1.2577],\n",
      "           [-0.1764, -1.4257,  0.6778,  1.3853,  1.2959,  1.0507,  1.1812,\n",
      "            -0.9611, -0.0232, -1.4692],\n",
      "           [-0.4795,  0.3366, -2.6631,  0.3454, -0.5111,  0.5909,  0.9888,\n",
      "             1.5879, -1.4838,  0.1958],\n",
      "           [-0.8750, -0.4536,  0.2103,  0.2052,  1.0839, -1.5121,  0.8559,\n",
      "            -0.8731,  0.5113,  1.1682],\n",
      "           [ 1.8372,  0.0827,  0.6131, -2.4975,  1.6381,  0.0744,  0.4960,\n",
      "             0.5767,  0.3010,  0.9474]]],\n",
      "\n",
      "\n",
      "         [[[-1.4517, -1.1179, -0.9721,  0.8386,  1.3399,  0.3278,  1.5768,\n",
      "            -0.8342,  0.5690, -1.5919],\n",
      "           [ 0.4919, -0.4742, -0.3582,  0.2876,  0.8352,  0.7330,  1.0204,\n",
      "            -0.7722, -1.2779, -0.0758],\n",
      "           [-1.3480,  0.3704, -1.1193,  0.8637,  0.5257,  1.0181,  0.0445,\n",
      "            -0.2769,  1.2722,  0.3760],\n",
      "           [ 1.7284, -0.6489, -0.1753, -0.2746, -1.2372,  0.6514,  0.7967,\n",
      "             0.2439, -0.1443,  0.7985],\n",
      "           [ 0.2986, -1.1464, -1.1980,  1.2540,  0.7605,  1.6205, -1.3684,\n",
      "             1.8272, -1.0625,  0.0196],\n",
      "           [-0.9976,  0.1077, -0.4736,  1.0891,  0.0991, -1.1957, -0.1601,\n",
      "            -1.3582,  0.0451, -1.0559],\n",
      "           [-0.2046, -1.3078,  0.1874,  1.2168,  0.0241,  0.3762, -0.2527,\n",
      "             0.6111, -1.5212,  0.6594],\n",
      "           [ 1.6948, -0.7111, -1.2053,  0.6468,  0.7714, -0.9213, -0.3191,\n",
      "            -0.2231,  0.6347,  0.0728]],\n",
      "\n",
      "          [[ 0.3819, -1.7515,  0.5743,  0.7274, -0.6179, -1.0192,  0.8111,\n",
      "            -2.0654,  0.4858,  0.0676],\n",
      "           [-0.2012, -0.0111,  0.7983,  0.4453,  1.4188,  0.1856,  0.9401,\n",
      "            -2.5209,  0.9113, -0.2316],\n",
      "           [ 2.3733, -0.7623,  1.1311,  1.3681, -0.7969, -2.1796, -0.8735,\n",
      "            -1.4220,  0.9129,  1.0935],\n",
      "           [-1.0600,  2.6020,  0.5035, -1.5135, -1.5322,  0.1308,  0.3693,\n",
      "            -0.0370, -0.9073, -0.4555],\n",
      "           [ 2.6198,  0.4368, -0.1959, -0.6331, -0.3154,  0.4582,  0.9115,\n",
      "             0.5505,  2.4171, -0.2171],\n",
      "           [-0.2546,  0.7257, -0.3125, -0.0183,  0.5276, -1.2994, -0.1207,\n",
      "            -0.5548,  1.4821, -0.4235],\n",
      "           [ 0.2230,  1.0512, -2.1188,  0.5653, -0.0053, -0.0475,  0.4583,\n",
      "            -0.9669,  1.2316, -0.4282],\n",
      "           [ 0.3329,  1.8610,  1.0565, -0.7926,  1.1546,  0.5699,  0.3949,\n",
      "            -1.2187, -1.1805, -1.5118]]],\n",
      "\n",
      "\n",
      "         [[[ 0.7888, -1.0254, -0.5202,  0.6278,  0.0174, -0.6010,  1.0435,\n",
      "            -0.5158, -0.3038,  0.8365],\n",
      "           [ 1.0053, -1.0080,  0.4824, -1.3080, -3.5381, -0.6231,  0.1451,\n",
      "            -0.1876,  1.1017, -0.5896],\n",
      "           [-1.0958,  1.1880, -1.2789,  0.5178,  0.5471,  0.6695,  0.2699,\n",
      "            -0.4156,  0.3757,  0.4165],\n",
      "           [ 0.3677,  0.9691,  0.6558, -0.2831, -0.1199,  1.2052, -0.1473,\n",
      "             0.7986,  0.5306,  0.5850],\n",
      "           [ 1.2919, -0.6886,  0.2898, -1.2847, -0.4825,  0.6796, -1.1805,\n",
      "             0.5986, -0.6435,  0.7381],\n",
      "           [-0.0591,  0.4972,  2.0445, -0.2344,  1.4242, -1.3366, -0.4380,\n",
      "            -0.3632,  0.6123, -0.8894],\n",
      "           [ 0.2621, -0.1271,  1.0752,  0.7101,  0.6298, -0.1851, -1.1916,\n",
      "            -0.2989, -1.6676,  0.6987],\n",
      "           [ 0.0581, -0.0158,  0.7328, -0.1764,  0.5759, -2.4529, -1.3306,\n",
      "             0.5552,  1.2263, -0.2666]],\n",
      "\n",
      "          [[-0.4439,  1.6161,  1.3892,  2.9432, -1.3078,  0.9753,  0.0515,\n",
      "             0.0884,  1.4268, -1.6870],\n",
      "           [-1.1941,  1.9096,  0.4578,  0.7191,  1.7402, -0.9447,  1.0777,\n",
      "             0.7036,  0.6971,  1.0157],\n",
      "           [ 1.0831, -1.1877, -1.8027, -0.9669, -1.2880,  0.0316, -0.3508,\n",
      "            -0.4701, -0.5118,  0.6847],\n",
      "           [-1.3703, -0.5922, -0.4295,  1.1670, -0.3976, -2.1387,  0.0901,\n",
      "             0.9999, -1.2405,  0.6571],\n",
      "           [-0.8135, -0.3056,  0.6205, -1.0283, -0.8782,  0.6478,  0.5201,\n",
      "            -1.4810, -0.7023, -0.7526],\n",
      "           [ 0.0677,  1.4225, -2.5180, -0.0072,  0.7294,  0.1364,  0.8883,\n",
      "             0.1903, -0.6870, -0.2250],\n",
      "           [ 0.3993, -1.3255, -1.4625,  0.3196,  2.0483, -0.5732,  0.7701,\n",
      "             0.0574,  1.4920,  0.4871],\n",
      "           [-0.2596, -1.5536, -0.2551, -1.5074, -0.4128,  0.0883, -1.2232,\n",
      "            -2.0905,  0.8347,  0.2949]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0105, -0.3792, -0.7709,  0.5774, -0.4286, -0.2890,  0.2111,\n",
      "             0.4701, -0.0929,  0.3592],\n",
      "           [-0.2852, -1.0646, -1.0175,  0.7870,  0.0243, -0.0617, -1.0101,\n",
      "             1.8153, -0.7219,  0.7081],\n",
      "           [ 0.7807, -0.2774, -1.8425, -0.3518, -0.1792, -1.0977,  0.6349,\n",
      "            -0.2323,  0.5619, -0.1918],\n",
      "           [-0.9961,  0.5704,  0.8904,  0.6142,  1.4005, -0.0439, -0.7670,\n",
      "             1.6205,  1.5168,  1.0320],\n",
      "           [-1.0439, -0.1449, -1.0145, -1.2055, -0.4208,  0.4384,  1.0067,\n",
      "             0.1843,  0.4359, -1.5802],\n",
      "           [-0.7370,  0.3789, -0.0244, -0.9691,  0.1888,  1.5331, -0.2040,\n",
      "             1.3108, -0.3881, -0.9269],\n",
      "           [-0.6496,  1.4512, -0.5904,  0.1537, -0.9209, -0.1713, -3.3821,\n",
      "             0.0429,  0.3328,  1.1320],\n",
      "           [ 0.7852,  0.8688,  0.2122, -1.0754, -2.2697,  0.5790, -0.5360,\n",
      "             0.2810, -0.8950,  0.1330]],\n",
      "\n",
      "          [[ 0.0080,  1.2723,  0.8407, -2.7797, -0.8056,  0.4865,  0.5961,\n",
      "            -1.1682,  0.2755, -0.7731],\n",
      "           [ 0.3969, -1.3586, -1.1081,  2.9909,  0.1598, -0.5971, -0.5107,\n",
      "             0.7343,  0.0845,  0.5136],\n",
      "           [-0.3524,  0.3871,  0.4404, -0.6902, -0.4594, -0.0175,  1.0811,\n",
      "             0.6273, -0.7042, -0.0753],\n",
      "           [-0.5788, -0.2020,  1.0279, -1.0539, -0.5407,  0.5560,  1.1380,\n",
      "            -2.0152,  1.7609,  0.2772],\n",
      "           [-1.4186, -0.2335, -0.3973, -2.8567,  0.1037, -0.4584,  0.2017,\n",
      "            -0.4662,  1.2773, -1.6452],\n",
      "           [ 0.8366,  2.0962,  1.0919,  0.6803, -0.2735,  2.0951,  0.9689,\n",
      "             1.1210,  0.3731, -1.3066],\n",
      "           [ 0.9610,  0.5294, -0.4317, -1.5118, -0.5855,  0.0625,  0.4784,\n",
      "            -0.0059, -0.5497,  0.4948],\n",
      "           [ 0.6885, -0.5823, -2.1467, -1.6563, -0.0792, -0.2631,  0.0218,\n",
      "            -2.1007, -0.5056,  0.5936]]],\n",
      "\n",
      "\n",
      "         [[[ 0.2978,  1.1460,  1.0229, -0.3123, -0.6202, -1.2700, -0.4014,\n",
      "             0.9343,  1.4109, -0.0169],\n",
      "           [ 0.6892,  0.6768,  0.2972,  0.0973, -0.6435,  1.8278, -1.4087,\n",
      "             0.6342, -0.9263, -0.3365],\n",
      "           [ 0.7121, -0.1962,  0.5197, -0.3787, -1.8679, -0.6296, -0.0564,\n",
      "            -0.4596, -0.7369,  0.7031],\n",
      "           [ 0.0423, -2.1637,  0.3597,  0.2759, -0.4304,  1.1652, -0.2618,\n",
      "             2.7570, -0.0801,  1.9362],\n",
      "           [ 0.8374,  0.0217, -1.4381, -0.8192,  0.0590,  0.9148, -0.8756,\n",
      "            -0.4839, -0.8845, -2.6472],\n",
      "           [-0.0729, -1.1938,  0.3571,  0.8712,  0.0301,  0.1262,  0.2725,\n",
      "             1.3803, -0.3303,  0.6580],\n",
      "           [-1.3911, -0.4612, -1.5567,  0.0927,  0.5894, -0.0157, -0.7709,\n",
      "            -0.8460,  1.3396, -0.6163],\n",
      "           [-1.0864, -0.9680, -0.4497,  0.1618,  0.7455,  1.1769, -0.0437,\n",
      "             0.2090, -0.3135,  0.0841]],\n",
      "\n",
      "          [[-0.5229,  0.2043,  2.1883, -1.1481,  0.9409,  0.5438,  0.8599,\n",
      "            -0.4252, -0.9843,  0.5407],\n",
      "           [-0.1442, -0.1011, -1.2651, -1.2141, -1.6656, -1.1118, -0.2445,\n",
      "             0.1211,  0.6049, -0.7725],\n",
      "           [ 0.1341, -0.0644,  0.6435, -0.3141,  0.8378, -0.3199, -1.2804,\n",
      "            -0.8594,  1.4116,  1.6225],\n",
      "           [-0.1209,  2.2441,  0.1888,  1.0451, -0.2337,  0.0914,  1.4699,\n",
      "            -0.3806, -0.6886, -1.2529],\n",
      "           [-2.2671, -0.1288,  1.5659, -1.5220, -0.8840,  0.7409, -0.3513,\n",
      "            -1.3149,  2.3147, -1.0011],\n",
      "           [-0.5579,  0.4638,  1.8544, -1.3977,  0.1835, -1.0902, -0.3810,\n",
      "             0.8708, -0.5045,  0.8800],\n",
      "           [-1.2291,  0.1289,  0.8977,  0.9349, -0.5696, -0.0369, -1.3667,\n",
      "            -1.1829,  0.8171,  0.5872],\n",
      "           [-0.4122, -1.9831,  0.7677, -0.0743,  0.2266, -0.2323, -0.0688,\n",
      "             0.1568, -0.0581,  0.1914]]],\n",
      "\n",
      "\n",
      "         [[[ 0.1546, -0.4409, -0.0850, -0.2759,  1.2867,  0.5688, -0.1012,\n",
      "            -1.4852,  1.0334,  0.6426],\n",
      "           [-1.5067, -0.6982, -0.2360, -0.7648,  0.5485,  0.2797,  0.2358,\n",
      "             0.2645, -0.1988, -0.5244],\n",
      "           [ 0.0693, -0.1837, -0.6301,  1.6662,  0.6577, -0.3054,  0.4358,\n",
      "            -0.0068,  0.0154, -0.1377],\n",
      "           [-0.4797,  0.1107,  0.0464,  1.4440,  0.6261,  0.3937, -0.8578,\n",
      "            -0.4839,  0.1002, -0.6535],\n",
      "           [-0.6804,  0.8260, -0.1286, -0.6753,  2.2390,  2.1160, -1.2857,\n",
      "             1.9339,  0.4278,  1.7321],\n",
      "           [ 0.2752,  1.8982, -0.5433, -2.0016,  1.1467, -0.5133,  1.3124,\n",
      "             0.3249, -1.4564, -0.6741],\n",
      "           [ 0.6246,  0.4422,  0.3722, -0.1912, -1.5909, -0.2997, -0.9688,\n",
      "             1.6370, -1.9015, -0.1506],\n",
      "           [-0.5817,  0.2304, -1.2704,  0.9747, -1.1340,  0.0629,  1.1889,\n",
      "            -1.0165,  1.1362,  1.8777]],\n",
      "\n",
      "          [[ 0.1172,  0.1780,  0.6920, -1.1570,  0.3746,  0.4381, -0.1081,\n",
      "            -1.2945, -1.3200,  0.6479],\n",
      "           [-0.8748,  0.4197,  2.4915,  0.1101,  1.5405, -1.2698, -2.8038,\n",
      "             0.6708, -0.0955,  0.4972],\n",
      "           [ 0.1642, -0.8139, -0.2912, -0.4833, -0.4741, -1.5365, -1.0850,\n",
      "             1.1553,  1.3391, -1.0066],\n",
      "           [-0.7647,  0.9329, -0.0715, -0.1563, -0.8400,  0.4329,  0.3691,\n",
      "            -0.3205, -0.1196,  0.0297],\n",
      "           [-0.3022, -0.4582, -0.0101, -0.1825,  2.1378, -0.1814,  1.2546,\n",
      "             0.5838,  0.2997, -0.4240],\n",
      "           [-0.2413,  0.6440,  0.4782, -1.1549, -0.6922, -1.1910,  0.0655,\n",
      "            -1.4004,  0.5689,  1.2571],\n",
      "           [ 0.1634,  0.4024, -0.2030,  0.1038, -1.1347, -0.2535, -0.4154,\n",
      "            -0.8938, -0.3098, -1.0669],\n",
      "           [ 0.5402,  0.4277,  0.1115,  0.0174,  1.8681,  1.2682,  1.1942,\n",
      "             1.9648, -0.0697, -0.5998]]]]]))\n",
      "-----\n",
      "case 2\n",
      "x.shape: (2, 3, 8, 10)\n",
      "(2, 3, 8, 3)\n",
      "(2, 3, 8, 3)\n",
      "(3, 1, 3)\n",
      "(3, 1, 3)\n",
      "(<tf.Tensor: shape=(2, 3, 8, 10), dtype=float32, numpy=\n",
      "array([[[[-1.7899659 , -0.78511924, -0.25779927,  0.37316513,\n",
      "          -1.2666459 , -0.07021647, -1.6105175 ,  0.6976891 ,\n",
      "          -2.3031056 ,  1.2279682 ],\n",
      "         [ 0.97642803,  1.0401855 , -0.62574726, -1.9323689 ,\n",
      "           1.6229227 ,  0.8315524 ,  0.77401   ,  0.58595157,\n",
      "           0.2649647 ,  0.87521917],\n",
      "         [ 0.78946877, -0.33368063,  0.4225115 , -0.3910988 ,\n",
      "           0.6513746 , -0.29069152, -0.4883207 ,  0.3681069 ,\n",
      "           0.17834172,  1.7626281 ],\n",
      "         [-1.3310809 ,  0.4135496 ,  0.9899511 ,  0.00729275,\n",
      "           2.2774065 ,  1.5733899 ,  0.7217782 , -1.0520257 ,\n",
      "          -0.7683959 ,  1.5303556 ],\n",
      "         [-0.46093166, -0.08342054,  0.02859988,  0.788386  ,\n",
      "           1.3756275 , -1.2021313 ,  0.00805058, -0.20496096,\n",
      "           1.0962852 , -0.1612082 ],\n",
      "         [-0.08330333, -0.6115222 , -0.2219885 ,  2.272401  ,\n",
      "           0.56219316, -1.3401934 , -0.04092553,  1.5951481 ,\n",
      "          -0.5891229 , -0.34029764],\n",
      "         [ 0.03186387, -1.2721951 , -0.19702902,  0.5765143 ,\n",
      "          -1.1703072 ,  1.0728683 , -0.24376173,  1.0371739 ,\n",
      "           3.3739486 ,  0.0157368 ],\n",
      "         [-0.13014391,  1.1608894 , -0.2561602 ,  0.6764114 ,\n",
      "           0.06211773,  1.0037022 ,  1.2254181 ,  0.18637356,\n",
      "           1.4947404 , -1.3249881 ]],\n",
      "\n",
      "        [[-0.01983628, -0.05196097,  1.7986516 , -1.4198207 ,\n",
      "           2.4550655 , -0.47901952, -0.7629271 ,  0.10623689,\n",
      "           1.896251  ,  0.9034426 ],\n",
      "         [ 0.20517798,  0.4063177 , -1.3940878 , -0.75572723,\n",
      "          -2.0029955 , -0.18122348,  0.71156496, -0.04210787,\n",
      "          -0.540352  ,  1.9608463 ],\n",
      "         [ 0.68092465, -0.7047564 ,  0.74691194,  2.4521258 ,\n",
      "          -1.4035311 ,  0.8382775 , -1.4716073 ,  0.39192715,\n",
      "           0.4698165 , -0.9240592 ],\n",
      "         [ 0.30428207,  0.52593994, -0.5492419 ,  0.14266728,\n",
      "           0.322488  , -0.5511024 ,  1.4985917 ,  0.34486124,\n",
      "           1.7363902 , -0.48677808],\n",
      "         [ 1.2370566 , -1.781794  ,  0.49903905, -0.00522166,\n",
      "           0.18661812,  0.2835931 ,  1.0265207 ,  2.446054  ,\n",
      "           0.77182335, -0.0131974 ],\n",
      "         [ 0.9976192 ,  0.17924862,  0.14664139,  1.1008754 ,\n",
      "          -0.8758399 , -0.90963584, -0.97189593, -1.5163435 ,\n",
      "           0.81048214,  0.49711868],\n",
      "         [ 1.2321562 , -1.5211362 , -0.767838  ,  0.7375215 ,\n",
      "          -0.03876765, -0.20632486,  0.11379927, -0.35197592,\n",
      "          -0.3116659 , -1.8611556 ],\n",
      "         [-0.6266235 ,  0.6871821 ,  0.80703294, -0.9634419 ,\n",
      "           0.3021742 , -0.36981675,  0.49607128, -0.8231155 ,\n",
      "          -0.70699507,  2.4077065 ]],\n",
      "\n",
      "        [[-0.2973482 , -0.67317545, -1.5734371 , -1.620842  ,\n",
      "          -0.07693527, -0.20586371,  0.48183268, -0.9398846 ,\n",
      "          -0.07428367, -0.94960743],\n",
      "         [-1.221278  , -0.51166964,  0.02030594,  0.8394387 ,\n",
      "           0.78288275, -0.09371379,  0.2886642 , -0.13445385,\n",
      "           1.4160125 , -1.1560428 ],\n",
      "         [-0.9081429 , -0.77485394, -0.8836597 , -2.0130417 ,\n",
      "          -0.87774324,  1.0945846 ,  0.29958212, -0.42588624,\n",
      "          -0.6221182 , -0.24353129],\n",
      "         [ 1.2003881 , -1.0232377 , -1.5493815 , -0.7811262 ,\n",
      "          -0.03402016, -0.85269815,  0.97998196, -0.22804065,\n",
      "          -0.4404938 , -0.5604243 ],\n",
      "         [-1.266439  ,  1.5334046 , -0.6224525 , -0.9634254 ,\n",
      "           0.7780806 , -0.95001787,  1.9630488 ,  2.5674295 ,\n",
      "           0.5124412 , -1.150337  ],\n",
      "         [-0.98241   ,  0.551834  , -0.33648083, -0.67801857,\n",
      "          -0.96560043,  0.32528564,  0.11811054,  1.6996119 ,\n",
      "           1.2724439 , -0.11804518],\n",
      "         [ 0.45135078, -0.998248  , -0.6907417 , -1.3417037 ,\n",
      "          -0.9755    , -0.48503673,  0.89381766,  0.15014331,\n",
      "           1.0308174 , -1.3875792 ],\n",
      "         [ 0.42135248,  1.5898929 ,  0.23397644, -0.534862  ,\n",
      "          -1.0062164 , -0.19955842,  0.28994924,  0.02349729,\n",
      "           1.4107839 , -2.4048312 ]]],\n",
      "\n",
      "\n",
      "       [[[ 0.04886749, -0.00651892, -0.04964413, -0.18687454,\n",
      "           1.6753978 , -0.2316804 , -0.534413  , -1.1790043 ,\n",
      "          -1.8003898 , -0.5398874 ],\n",
      "         [-0.58837265,  0.15496215, -0.3383894 , -1.7056057 ,\n",
      "          -0.16408452,  0.06123887, -2.371423  , -0.8875372 ,\n",
      "          -2.2114222 ,  0.18177903],\n",
      "         [-0.2718449 ,  1.2595357 , -0.7523816 , -0.34915996,\n",
      "           1.1455802 , -0.3672215 ,  0.9982976 ,  0.46427697,\n",
      "          -0.61881006, -0.90034   ],\n",
      "         [-1.0871639 , -0.22472602, -1.1108382 ,  1.1025342 ,\n",
      "           0.64694786,  0.11975032,  0.6913915 , -0.68769604,\n",
      "           0.24193677, -0.5705415 ],\n",
      "         [-0.2878964 ,  0.6446215 , -0.535187  ,  1.3226849 ,\n",
      "           0.0423391 , -0.08624492, -0.86708266, -1.118338  ,\n",
      "          -2.3596878 , -0.57451034],\n",
      "         [ 0.76365757, -0.5342433 ,  0.00952831, -0.6412129 ,\n",
      "          -1.3186458 ,  1.3231748 , -2.2135997 , -1.9100536 ,\n",
      "          -0.08155985, -0.7607755 ],\n",
      "         [-0.21884488,  0.3538209 ,  0.9828345 , -0.71784306,\n",
      "           0.73592794, -0.53505784, -0.49292237,  1.0866362 ,\n",
      "          -1.174383  , -0.53935343],\n",
      "         [-1.0267318 ,  0.2662207 , -0.58373195, -1.0131809 ,\n",
      "          -0.30032805, -1.2964112 , -0.84223473,  0.56282336,\n",
      "          -1.4887264 ,  0.51759344]],\n",
      "\n",
      "        [[ 1.7902482 , -0.74194306,  0.37564412, -0.8423412 ,\n",
      "          -0.9440688 , -1.4750642 , -0.9028552 ,  2.0755498 ,\n",
      "          -1.1493511 , -0.06514744],\n",
      "         [ 0.79256356, -0.7244139 , -0.38248125,  1.7371963 ,\n",
      "           1.5301404 , -0.58408123,  0.8706175 ,  0.5814366 ,\n",
      "          -0.9608693 ,  2.3410919 ],\n",
      "         [ 2.1008754 , -0.51898146,  0.28839153, -1.1748949 ,\n",
      "          -0.11020596, -0.06382707,  0.02348141,  0.5656673 ,\n",
      "          -1.4749235 ,  1.3723443 ],\n",
      "         [-1.1173952 ,  0.34702498, -1.7696382 , -0.9989834 ,\n",
      "           0.38838878, -0.8973561 , -0.2955988 ,  1.5135318 ,\n",
      "          -1.7673696 ,  0.57986623],\n",
      "         [ 0.4578565 , -1.2582507 ,  0.00892013,  0.14859128,\n",
      "          -2.2226214 ,  0.47862482,  2.5153463 , -1.132573  ,\n",
      "          -0.03742332, -1.9074528 ],\n",
      "         [ 0.4195575 , -1.4803466 , -0.93445706, -0.52179337,\n",
      "          -0.57694244,  0.14996828, -0.59773743, -1.4518528 ,\n",
      "           0.12541418, -2.496086  ],\n",
      "         [ 1.9090198 ,  0.6071046 , -0.8158576 ,  1.7758896 ,\n",
      "           0.9491878 ,  1.7766764 , -1.154186  , -0.67043275,\n",
      "           1.543023  , -0.14711265],\n",
      "         [ 0.71455127, -0.7456904 ,  0.668417  ,  1.3073747 ,\n",
      "           0.3498495 ,  0.29071084,  0.7027588 ,  0.13563469,\n",
      "          -0.03813383, -0.26842773]],\n",
      "\n",
      "        [[-1.6529464 ,  0.56308573, -0.94080216,  0.3208431 ,\n",
      "           0.69522536,  0.53734225, -0.12298729,  0.18630946,\n",
      "          -0.06915361,  0.5975232 ],\n",
      "         [ 1.2833699 , -1.1599041 , -0.2468094 ,  0.33222196,\n",
      "           0.06330538, -0.94928086,  0.2931615 , -0.57030535,\n",
      "          -0.09453171,  0.27782962],\n",
      "         [-0.6315515 ,  0.62595975, -1.6503109 ,  0.5672882 ,\n",
      "          -0.84855384, -0.51577693,  0.909477  , -0.712288  ,\n",
      "           0.93168074, -1.06093   ],\n",
      "         [-0.01970296,  0.31495756, -0.3962437 , -0.510914  ,\n",
      "          -0.2107868 ,  1.8088975 ,  1.3569258 ,  0.12711085,\n",
      "           0.21275492,  0.34123075],\n",
      "         [-0.18214798, -0.2635705 ,  1.2731931 , -0.95569354,\n",
      "          -2.4428294 , -0.6415509 , -1.0358833 , -0.01289005,\n",
      "           0.4842973 , -0.8069625 ],\n",
      "         [ 1.3369907 , -0.80983317, -2.2075396 , -1.1507343 ,\n",
      "          -1.7047939 , -0.0792193 , -0.5288852 ,  0.65796757,\n",
      "           0.07113189,  0.06007399],\n",
      "         [ 1.1051519 ,  0.13248244,  0.87468576, -0.7089371 ,\n",
      "           0.36796248, -0.25407416, -0.01284131,  0.49095973,\n",
      "           0.79436696,  0.06966531],\n",
      "         [-0.4267567 , -0.9951813 , -0.5604736 ,  0.58795935,\n",
      "          -0.47434402,  0.89251024,  0.03089386, -2.1618826 ,\n",
      "           0.10572209, -1.6146281 ]]]], dtype=float32)>, <tf.Tensor: shape=(2, 3, 2, 8, 10), dtype=float32, numpy=\n",
      "array([[[[[-1.5141124 , -0.53706723,  0.33277664, -0.8268813 ,\n",
      "           -0.49139288, -0.6906722 , -0.94223285,  1.7612472 ,\n",
      "            1.3763301 , -0.05250169],\n",
      "          [-1.2916911 , -1.2310182 , -1.0651675 ,  0.77170295,\n",
      "           -1.0232545 ,  0.03484299,  1.1597395 , -1.0583752 ,\n",
      "           -1.8114177 , -0.6160324 ],\n",
      "          [-0.02430291,  1.2751508 , -0.18487152,  0.4256174 ,\n",
      "            2.1595466 ,  1.2541827 , -0.48188925,  0.36176404,\n",
      "            0.6781406 , -0.7912167 ],\n",
      "          [ 1.5616626 , -0.47631425,  0.72994775, -0.7848855 ,\n",
      "            1.01188   , -0.09000015,  1.0129875 , -2.9863584 ,\n",
      "            1.18552   , -2.0263636 ],\n",
      "          [ 0.5759401 , -1.5551305 , -0.9217436 ,  0.2548108 ,\n",
      "           -1.6917506 ,  0.5947131 , -1.1621088 ,  1.5925863 ,\n",
      "           -0.6021439 , -0.8925073 ],\n",
      "          [ 1.4730375 ,  1.1024412 , -1.5023005 ,  0.455997  ,\n",
      "            0.1606223 ,  0.01989844,  0.98526627, -0.6396445 ,\n",
      "           -0.45951977, -0.97950494],\n",
      "          [ 0.61933124, -1.0171676 , -0.5228248 , -1.1356441 ,\n",
      "            0.05075778,  0.05041151, -0.30812255, -0.4201837 ,\n",
      "           -0.05583012, -0.10117821],\n",
      "          [-1.1282314 , -0.98641074, -0.25014755,  0.71538633,\n",
      "           -0.9900276 , -0.9007889 ,  0.70915484,  0.78199583,\n",
      "            1.3778149 ,  0.01135642]],\n",
      "\n",
      "         [[ 0.51167965, -0.17369705,  1.1510603 , -0.01263376,\n",
      "           -0.52708435,  0.85076934, -1.3609568 , -2.076208  ,\n",
      "           -1.0541811 , -0.8181693 ],\n",
      "          [ 0.2083626 ,  0.49079415, -0.16668299, -0.449217  ,\n",
      "           -0.6213116 , -0.43223798,  0.7218613 ,  2.8600445 ,\n",
      "            1.0068297 , -1.7260026 ],\n",
      "          [-1.9933413 ,  0.6158    ,  0.21530947,  0.2610723 ,\n",
      "           -0.44181153,  0.5405895 , -0.65526474,  0.785385  ,\n",
      "            0.19097398, -1.200833  ],\n",
      "          [ 0.38636354, -1.784393  , -0.10369799,  0.5754217 ,\n",
      "           -0.92174125, -0.61442167, -1.0048366 , -1.7076836 ,\n",
      "           -0.05414506, -1.2577466 ],\n",
      "          [-0.17639208, -1.4256972 ,  0.67776537,  1.3853387 ,\n",
      "            1.2958767 ,  1.05071   ,  1.1812071 , -0.9610618 ,\n",
      "           -0.02316179, -1.4691842 ],\n",
      "          [-0.47946572,  0.3365669 , -2.6630929 ,  0.34538755,\n",
      "           -0.51106906,  0.59094673,  0.9887981 ,  1.5879092 ,\n",
      "           -1.4838098 ,  0.19583869],\n",
      "          [-0.87495565, -0.453553  ,  0.21027304,  0.20524046,\n",
      "            1.083889  , -1.5120683 ,  0.855892  , -0.8730615 ,\n",
      "            0.51130044,  1.1681699 ],\n",
      "          [ 1.8371636 ,  0.08273038,  0.6130808 , -2.497452  ,\n",
      "            1.6380836 ,  0.07439467,  0.49602368,  0.5767268 ,\n",
      "            0.30096477,  0.9473939 ]]],\n",
      "\n",
      "\n",
      "        [[[-1.4517189 , -1.1178933 , -0.9720615 ,  0.83861613,\n",
      "            1.3399131 ,  0.32778725,  1.576764  , -0.83420056,\n",
      "            0.5690442 , -1.5919216 ],\n",
      "          [ 0.49187878, -0.47421622, -0.3581561 ,  0.2876271 ,\n",
      "            0.8351684 ,  0.7329732 ,  1.0204303 , -0.772222  ,\n",
      "           -1.2779471 , -0.07580852],\n",
      "          [-1.3480358 ,  0.3703941 , -1.1193253 ,  0.8636682 ,\n",
      "            0.525651  ,  1.0180924 ,  0.04452372, -0.27693275,\n",
      "            1.2722301 ,  0.37599668],\n",
      "          [ 1.7284225 , -0.6489245 , -0.17532215, -0.27463996,\n",
      "           -1.2371991 ,  0.65142566,  0.79669   ,  0.24387032,\n",
      "           -0.1443297 ,  0.7985445 ],\n",
      "          [ 0.2986159 , -1.1463796 , -1.1980138 ,  1.2539684 ,\n",
      "            0.76045257,  1.620474  , -1.3683552 ,  1.8271708 ,\n",
      "           -1.0625162 ,  0.01959864],\n",
      "          [-0.9975538 ,  0.10765847, -0.4735786 ,  1.0890989 ,\n",
      "            0.09914063, -1.1956861 , -0.16005397, -1.3582219 ,\n",
      "            0.04509829, -1.0558809 ],\n",
      "          [-0.20463935, -1.307798  ,  0.18738614,  1.2168295 ,\n",
      "            0.0240844 ,  0.37619275, -0.25265887,  0.6111068 ,\n",
      "           -1.5211718 ,  0.6593669 ],\n",
      "          [ 1.6948363 , -0.7110859 , -1.2053155 ,  0.6467709 ,\n",
      "            0.77142835, -0.9212748 , -0.3191425 , -0.22309801,\n",
      "            0.63465846,  0.07283082]],\n",
      "\n",
      "         [[ 0.38191962, -1.7515166 ,  0.5743309 ,  0.7273995 ,\n",
      "           -0.6178845 , -1.0191675 ,  0.81112   , -2.0654008 ,\n",
      "            0.48584118,  0.0676344 ],\n",
      "          [-0.20116256, -0.0110906 ,  0.7983049 ,  0.44530573,\n",
      "            1.4188025 ,  0.1856034 ,  0.94011474, -2.5208647 ,\n",
      "            0.91131204, -0.23159704],\n",
      "          [ 2.373338  , -0.762345  ,  1.1311167 ,  1.3680613 ,\n",
      "           -0.79692435, -2.1796165 , -0.87354565, -1.4220097 ,\n",
      "            0.9129101 ,  1.093526  ],\n",
      "          [-1.0600034 ,  2.6020372 ,  0.50348747, -1.5135351 ,\n",
      "           -1.5322114 ,  0.13076748,  0.36933908, -0.03698306,\n",
      "           -0.9073326 , -0.45553038],\n",
      "          [ 2.6197608 ,  0.43684644, -0.19588323, -0.6330907 ,\n",
      "           -0.31540036,  0.45815054,  0.91147506,  0.55045   ,\n",
      "            2.4171169 , -0.21705407],\n",
      "          [-0.2546273 ,  0.72573566, -0.31245357, -0.01831206,\n",
      "            0.52758527, -1.2994436 , -0.12073369, -0.55479264,\n",
      "            1.4821057 , -0.42348507],\n",
      "          [ 0.22303466,  1.0511762 , -2.118826  ,  0.56531364,\n",
      "           -0.00534819, -0.0474793 ,  0.4583294 , -0.966874  ,\n",
      "            1.231595  , -0.42817155],\n",
      "          [ 0.33287814,  1.8609507 ,  1.056546  , -0.79256994,\n",
      "            1.1545876 ,  0.5699137 ,  0.39489686, -1.218655  ,\n",
      "           -1.180499  , -1.5118104 ]]],\n",
      "\n",
      "\n",
      "        [[[ 0.78875846, -1.0253599 , -0.52024496,  0.62781715,\n",
      "            0.01740077, -0.6009565 ,  1.0434992 , -0.51576036,\n",
      "           -0.30375618,  0.836524  ],\n",
      "          [ 1.0052986 , -1.0079627 ,  0.48243633, -1.3080375 ,\n",
      "           -3.5380845 , -0.6231422 ,  0.14514379, -0.18755576,\n",
      "            1.101677  , -0.589553  ],\n",
      "          [-1.0957747 ,  1.1879561 , -1.2789469 ,  0.5177522 ,\n",
      "            0.5470575 ,  0.66946304,  0.26989067, -0.4156056 ,\n",
      "            0.37570816,  0.4165083 ],\n",
      "          [ 0.36771148,  0.9691125 ,  0.6557692 , -0.2831283 ,\n",
      "           -0.11991705,  1.2052491 , -0.14726645,  0.7986082 ,\n",
      "            0.5306339 ,  0.5850334 ],\n",
      "          [ 1.2918799 , -0.68859065,  0.28978774, -1.2847111 ,\n",
      "           -0.48251066,  0.67959607, -1.1805145 ,  0.5985549 ,\n",
      "           -0.6435253 ,  0.73812485],\n",
      "          [-0.05911439,  0.49721104,  2.0444884 , -0.2343583 ,\n",
      "            1.4242257 , -1.3365653 , -0.43804246, -0.36317787,\n",
      "            0.6123414 , -0.889375  ],\n",
      "          [ 0.26210043, -0.12712464,  1.0751737 ,  0.71010697,\n",
      "            0.62976825, -0.1850655 , -1.1916095 , -0.298856  ,\n",
      "           -1.6676189 ,  0.69867605],\n",
      "          [ 0.05807326, -0.0158288 ,  0.7327965 , -0.17637283,\n",
      "            0.57588273, -2.4528503 , -1.3305565 ,  0.55520326,\n",
      "            1.2262795 , -0.2666026 ]],\n",
      "\n",
      "         [[-0.44390455,  1.616113  ,  1.3891503 ,  2.9432316 ,\n",
      "           -1.3077675 ,  0.97530276,  0.05151065,  0.08838998,\n",
      "            1.4267837 , -1.6869918 ],\n",
      "          [-1.1940856 ,  1.9096423 ,  0.45782998,  0.7190609 ,\n",
      "            1.7401664 , -0.94465846,  1.0777282 ,  0.70360166,\n",
      "            0.6971344 ,  1.0156928 ],\n",
      "          [ 1.0831362 , -1.1876563 , -1.8026549 , -0.96686834,\n",
      "           -1.2879834 ,  0.03160163, -0.35078606, -0.4701337 ,\n",
      "           -0.51179636,  0.68472457],\n",
      "          [-1.3702639 , -0.59224665, -0.42952612,  1.1670045 ,\n",
      "           -0.3975991 , -2.1387482 ,  0.09006158,  0.9998535 ,\n",
      "           -1.240506  ,  0.65713865],\n",
      "          [-0.81351984, -0.30557936,  0.620521  , -1.028255  ,\n",
      "           -0.87818706,  0.6477875 ,  0.5201241 , -1.4810275 ,\n",
      "           -0.7022577 , -0.7525926 ],\n",
      "          [ 0.06770881,  1.4224943 , -2.5179777 , -0.00717841,\n",
      "            0.7294483 ,  0.13640147,  0.88828474,  0.1902924 ,\n",
      "           -0.68696004, -0.22495247],\n",
      "          [ 0.39929706, -1.3255343 , -1.4625326 ,  0.31956333,\n",
      "            2.0483365 , -0.5731926 ,  0.7700973 ,  0.05737338,\n",
      "            1.4920447 ,  0.48708346],\n",
      "          [-0.2596384 , -1.5535723 , -0.25510404, -1.5073743 ,\n",
      "           -0.41281968,  0.08831391, -1.2231798 , -2.090465  ,\n",
      "            0.8347241 ,  0.29489744]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[-0.01046887, -0.37922144, -0.7709357 ,  0.57737267,\n",
      "           -0.4286331 , -0.28903997,  0.21107149,  0.47014025,\n",
      "           -0.09286179,  0.35915717],\n",
      "          [-0.2852455 , -1.0645547 , -1.017527  ,  0.7869526 ,\n",
      "            0.02432916, -0.06167112, -1.0100768 ,  1.8152823 ,\n",
      "           -0.7218833 ,  0.7081207 ],\n",
      "          [ 0.7806748 , -0.27737734, -1.8425014 , -0.35183078,\n",
      "           -0.17916581, -1.0976552 ,  0.63487995, -0.23233174,\n",
      "            0.5619418 , -0.19175392],\n",
      "          [-0.9960802 ,  0.5704383 ,  0.89041   ,  0.61415046,\n",
      "            1.4004679 , -0.04385191, -0.76704544,  1.6204566 ,\n",
      "            1.5167929 ,  1.0319821 ],\n",
      "          [-1.0438584 , -0.14490868, -1.0145164 , -1.2054738 ,\n",
      "           -0.42080742,  0.43836728,  1.0066969 ,  0.18426502,\n",
      "            0.4358787 , -1.580209  ],\n",
      "          [-0.7370467 ,  0.37885463, -0.02435167, -0.9691074 ,\n",
      "            0.18877661,  1.5331312 , -0.20403308,  1.3107731 ,\n",
      "           -0.3880924 , -0.9268645 ],\n",
      "          [-0.6495583 ,  1.4511545 , -0.59036094,  0.15374027,\n",
      "           -0.9208681 , -0.17127894, -3.382053  ,  0.04290456,\n",
      "            0.33279008,  1.1320286 ],\n",
      "          [ 0.7852118 ,  0.86877084,  0.21217506, -1.0753597 ,\n",
      "           -2.2697334 ,  0.57895786, -0.536023  ,  0.2810263 ,\n",
      "           -0.8950161 ,  0.13297933]],\n",
      "\n",
      "         [[ 0.00804594,  1.2722945 ,  0.8406508 , -2.7796535 ,\n",
      "           -0.80556357,  0.48649418,  0.59614646, -1.1682051 ,\n",
      "            0.27554598, -0.77306086],\n",
      "          [ 0.3968538 , -1.3585899 , -1.1080623 ,  2.990874  ,\n",
      "            0.15983085, -0.5971184 , -0.5107036 ,  0.7342639 ,\n",
      "            0.08454425,  0.51356006],\n",
      "          [-0.3523813 ,  0.38705313,  0.4404289 , -0.69022995,\n",
      "           -0.4594063 , -0.01747542,  1.0810592 ,  0.627256  ,\n",
      "           -0.704234  , -0.0752961 ],\n",
      "          [-0.5788051 , -0.20199   ,  1.0278615 , -1.0538846 ,\n",
      "           -0.5406918 ,  0.556036  ,  1.1380458 , -2.0151794 ,\n",
      "            1.7608732 ,  0.27718282],\n",
      "          [-1.4186018 , -0.23348697, -0.39729968, -2.8566608 ,\n",
      "            0.10372346, -0.45841298,  0.20167774, -0.466221  ,\n",
      "            1.2773327 , -1.6452066 ],\n",
      "          [ 0.83657557,  2.0962155 ,  1.0918875 ,  0.6802816 ,\n",
      "           -0.27351454,  2.0950735 ,  0.9689356 ,  1.1210102 ,\n",
      "            0.3730844 , -1.3066177 ],\n",
      "          [ 0.9610323 ,  0.5294383 , -0.43166327, -1.5117841 ,\n",
      "           -0.58554125,  0.06250795,  0.47844455, -0.00592823,\n",
      "           -0.5496911 ,  0.49484408],\n",
      "          [ 0.68847424, -0.58229893, -2.1466537 , -1.6562562 ,\n",
      "           -0.07920032, -0.2630862 ,  0.02182591, -2.100707  ,\n",
      "           -0.5055928 ,  0.59361875]]],\n",
      "\n",
      "\n",
      "        [[[ 0.29781303,  1.146021  ,  1.0228579 , -0.3122927 ,\n",
      "           -0.62015384, -1.2699896 , -0.40142578,  0.93434095,\n",
      "            1.4108748 , -0.01693862],\n",
      "          [ 0.68924266,  0.67675126,  0.29715878,  0.09732552,\n",
      "           -0.64345866,  1.8278317 , -1.4086531 ,  0.63423383,\n",
      "           -0.9262587 , -0.33654317],\n",
      "          [ 0.71207047, -0.19623688,  0.51972765, -0.37867254,\n",
      "           -1.8679172 , -0.629616  , -0.05642142, -0.4595547 ,\n",
      "           -0.7369183 ,  0.7030807 ],\n",
      "          [ 0.0422879 , -2.1636674 ,  0.35972005,  0.2758864 ,\n",
      "           -0.4304244 ,  1.1652234 , -0.2617987 ,  2.757035  ,\n",
      "           -0.08009732,  1.9361638 ],\n",
      "          [ 0.83736575,  0.02174247, -1.4381447 , -0.81916785,\n",
      "            0.05903789,  0.9148391 , -0.87561786, -0.48390666,\n",
      "           -0.8845306 , -2.6471722 ],\n",
      "          [-0.0729043 , -1.1938437 ,  0.35705256,  0.871151  ,\n",
      "            0.0301225 ,  0.12616128,  0.27252468,  1.3803185 ,\n",
      "           -0.33028987,  0.65800905],\n",
      "          [-1.3910738 , -0.46124363, -1.5567282 ,  0.09273469,\n",
      "            0.58941275, -0.01570354, -0.7708506 , -0.846034  ,\n",
      "            1.3395685 , -0.6162915 ],\n",
      "          [-1.0864019 , -0.96801156, -0.4496935 ,  0.1618497 ,\n",
      "            0.7455205 ,  1.1769301 , -0.04367996,  0.20901006,\n",
      "           -0.31351346,  0.084053  ]],\n",
      "\n",
      "         [[-0.52293783,  0.20426051,  2.1883023 , -1.1481036 ,\n",
      "            0.94089353,  0.54382896,  0.8599048 , -0.42521527,\n",
      "           -0.98433036,  0.54068416],\n",
      "          [-0.14419955, -0.1010858 , -1.2650882 , -1.214086  ,\n",
      "           -1.6656244 , -1.1117741 , -0.24450207,  0.12109713,\n",
      "            0.6049139 , -0.7724656 ],\n",
      "          [ 0.13413322, -0.06439549,  0.6434799 , -0.3141382 ,\n",
      "            0.83776337, -0.31988952, -1.2804383 , -0.8593639 ,\n",
      "            1.4116367 ,  1.6225334 ],\n",
      "          [-0.12091447,  2.2440624 ,  0.18878224,  1.0451208 ,\n",
      "           -0.23365508,  0.09135025,  1.4699137 , -0.3805641 ,\n",
      "           -0.6886439 , -1.252881  ],\n",
      "          [-2.2670822 , -0.12878819,  1.5659237 , -1.5219874 ,\n",
      "           -0.8839548 ,  0.74091965, -0.35128313, -1.3149291 ,\n",
      "            2.314707  , -1.0010685 ],\n",
      "          [-0.55793047,  0.46380025,  1.8543903 , -1.3977184 ,\n",
      "            0.18351388, -1.0902435 , -0.38100547,  0.8707641 ,\n",
      "           -0.5044502 ,  0.88000774],\n",
      "          [-1.2291112 ,  0.12892646,  0.8977292 ,  0.93487805,\n",
      "           -0.56963503, -0.03691282, -1.3667164 , -1.1829306 ,\n",
      "            0.81711835,  0.58720034],\n",
      "          [-0.41219616, -1.9830893 ,  0.76766896, -0.07434592,\n",
      "            0.22663745, -0.23232321, -0.06879312,  0.15679066,\n",
      "           -0.05813726,  0.19135748]]],\n",
      "\n",
      "\n",
      "        [[[ 0.15458816, -0.4408637 , -0.08498794, -0.27587634,\n",
      "            1.2866762 ,  0.56876594, -0.10123458, -1.4851799 ,\n",
      "            1.0334402 ,  0.6425968 ],\n",
      "          [-1.5066575 , -0.69815105, -0.23601517, -0.7648119 ,\n",
      "            0.54851365,  0.27969584,  0.23584275,  0.26451665,\n",
      "           -0.19882646, -0.5243675 ],\n",
      "          [ 0.06926358, -0.18373126, -0.63005954,  1.6661546 ,\n",
      "            0.6577319 , -0.3053534 ,  0.43575007, -0.0067802 ,\n",
      "            0.01544132, -0.13765338],\n",
      "          [-0.47973454,  0.11066066,  0.04635619,  1.4439946 ,\n",
      "            0.62613505,  0.393707  , -0.8577733 , -0.48388866,\n",
      "            0.10023551, -0.65351367],\n",
      "          [-0.6803605 ,  0.8259877 , -0.12864615, -0.6753117 ,\n",
      "            2.2390032 ,  2.115964  , -1.2857214 ,  1.9339017 ,\n",
      "            0.42776784,  1.7321168 ],\n",
      "          [ 0.27518886,  1.8981961 , -0.54332846, -2.0015697 ,\n",
      "            1.1467388 , -0.51325685,  1.3123865 ,  0.3248996 ,\n",
      "           -1.4564089 , -0.6740627 ],\n",
      "          [ 0.6246328 ,  0.4422217 ,  0.3721956 , -0.1911812 ,\n",
      "           -1.5909165 , -0.29966858, -0.9688467 ,  1.6369519 ,\n",
      "           -1.9014877 , -0.15056331],\n",
      "          [-0.58172673,  0.2304027 , -1.2703819 ,  0.97474277,\n",
      "           -1.1339761 ,  0.0628878 ,  1.1889389 , -1.0165288 ,\n",
      "            1.1362196 ,  1.8776748 ]],\n",
      "\n",
      "         [[ 0.11715409,  0.17800005,  0.6920104 , -1.157004  ,\n",
      "            0.37461665,  0.43809643, -0.10812657, -1.2944669 ,\n",
      "           -1.3200163 ,  0.64785683],\n",
      "          [-0.8747908 ,  0.4197333 ,  2.4915457 ,  0.11010927,\n",
      "            1.5405074 , -1.2698298 , -2.8037555 ,  0.67081064,\n",
      "           -0.09545311,  0.49718517],\n",
      "          [ 0.16417551, -0.8139071 , -0.29122177, -0.4833086 ,\n",
      "           -0.47408772, -1.5365335 , -1.0849861 ,  1.1553259 ,\n",
      "            1.3391414 , -1.0065638 ],\n",
      "          [-0.7646766 ,  0.9328956 , -0.07153141, -0.15628208,\n",
      "           -0.84001464,  0.43292218,  0.36905712, -0.32045397,\n",
      "           -0.11964099,  0.02967008],\n",
      "          [-0.30220997, -0.4581756 , -0.01013829, -0.18251966,\n",
      "            2.1378493 , -0.1814386 ,  1.2546366 ,  0.5838393 ,\n",
      "            0.29965612, -0.4240004 ],\n",
      "          [-0.24133365,  0.6439907 ,  0.4781707 , -1.154884  ,\n",
      "           -0.6922279 , -1.1909893 ,  0.06554071, -1.4003936 ,\n",
      "            0.5689122 ,  1.2571404 ],\n",
      "          [ 0.16342202,  0.4023525 , -0.20295979,  0.10375216,\n",
      "           -1.1347218 , -0.25345314, -0.41535953, -0.8938465 ,\n",
      "           -0.3097657 , -1.0668873 ],\n",
      "          [ 0.5401964 ,  0.4276653 ,  0.11154477,  0.01738023,\n",
      "            1.8680953 ,  1.2682009 ,  1.1942446 ,  1.96478   ,\n",
      "           -0.06974422, -0.59984076]]]]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# testing rotary embedding layer (done)\n",
    "class MyRotaryEmbedding(tf.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base: float = None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        name=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        self.inv_freq = self._compute_inv_freq()\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        self.scale = (\n",
    "            (tf.range(0, dim, 2, dtype=tf.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None)\n",
    "        self._update_cos_sin_cache(max_position_embeddings, dtype=tf.float32)\n",
    "\n",
    "    def _compute_inv_freq(self):\n",
    "        return 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(self, seqlen:int, dtype):\n",
    "        self._seq_len_cached = seqlen\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = tf.range(seqlen, dtype=tf.float32)\n",
    "            if self.inv_freq.dtype != tf.float32:\n",
    "                inv_freq = self._compute_inv_freq()\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = tf.range(seqlen, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        # tensorflow does not appear to do what the top comment states\n",
    "        freqs = tf.einsum('i,j->ij', t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = tf.cast(tf.cos(freqs), dtype=dtype)\n",
    "            self._sin_cached = tf.cast(tf.sin(freqs), dtype=dtype)\n",
    "        else:\n",
    "            power = (tf.range(seqlen, dtype=self.scale.dtype) - seqlen // 2) / self.scale_base\n",
    "            scale = self.scale ** tf.expand_dims(power, axis=1)\n",
    "            # Force the scale multiplciation to happen in fp32\n",
    "            self._cos_cached = tf.cast((tf.cos(freqs) * scale), dtype=dtype)\n",
    "            self._sin_cached = tf.cast((tf.sin(freqs) * scale), dtype=dtype)\n",
    "            self._cos_k_cached = tf.cast((tf.cos(freqs) / scale), dtype=dtype)\n",
    "            self._sin_k_cached = tf.cast((tf.sin(freqs) / scale), dtype=dtype)\n",
    "    \n",
    "    def __call__(self, qkv, kv, seqlen_offset, **kwargs):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            # or (self.training and self._cos_cached.is_inference()) # look into this\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return MY_apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = MY_apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            kv = MY_apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],)\n",
    "            return q, kv\n",
    "    ################################\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        base: int = 10000,\n",
    "        scale_base= None,\n",
    "        pos_idx_in_fp32: bool = True,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        device = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if scale_base is not None:\n",
    "            raise NotImplementedError\n",
    "        self.dim = dim\n",
    "        self.base = float(base)\n",
    "        self.scale_base = scale_base\n",
    "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.device = device\n",
    "        # Generate and save the inverse frequency buffer (non-trainable)\n",
    "        inv_freq = self._compute_inv_freq(device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Generate and save the scale buffer (non-trainable)\n",
    "        scale = (\n",
    "            (torch.arange(0, dim, 2, device=device, dtype=torch.float32) + 0.4 * dim) / (1.4 * dim)\n",
    "            if scale_base is not None\n",
    "            else None\n",
    "        )\n",
    "        self.register_buffer(\"scale\", scale, persistent=False)\n",
    "\n",
    "        # Initialize cached attributes since ONNX can't rely on dynamic initialization\n",
    "        self._update_cos_sin_cache(max_position_embeddings, device=device, dtype=torch.float32)\n",
    "\n",
    "    def _compute_inv_freq(self, device: Optional[str] = None) -> torch.FloatTensor:\n",
    "        return 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=torch.float32) / self.dim))\n",
    "\n",
    "    def _update_cos_sin_cache(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> None:\n",
    "        self._seq_len_cached = seqlen\n",
    "\n",
    "        # fp32 is preferred since the output of `torch.arange` can be quite large\n",
    "        # and bf16 would lose a lot of precision\n",
    "        if self.pos_idx_in_fp32:\n",
    "            t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
    "            if self.inv_freq.dtype != torch.float32:\n",
    "                inv_freq = self._compute_inv_freq(device=device)\n",
    "            else:\n",
    "                inv_freq = self.inv_freq\n",
    "        else:\n",
    "            t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
    "            inv_freq = self.inv_freq\n",
    "\n",
    "        # `torch.outer` is preferred since `torch.einsum` converts from fp32 to fp16 if used with AMP\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        if self.scale is None:\n",
    "            self._cos_cached = torch.cos(freqs).to(dtype)\n",
    "            self._sin_cached = torch.sin(freqs).to(dtype)\n",
    "        else:\n",
    "            power = (\n",
    "                torch.arange(seqlen, dtype=self.scale.dtype, device=self.scale.device) - seqlen // 2\n",
    "            ) / self.scale_base\n",
    "            scale = self.scale.to(device=power.device) ** rearrange(power, \"s -> s 1\")\n",
    "\n",
    "            # Force the scale multiplication to happen in fp32\n",
    "            self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
    "            self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
    "            self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
    "            self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.Tensor,\n",
    "        kv: Optional[torch.Tensor] = None,\n",
    "        seqlen_offset: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (\n",
    "            self._seq_len_cached < qkv.shape[1] + seqlen_offset\n",
    "            or self._cos_cached.device != qkv.device\n",
    "            or self._cos_cached.dtype != qkv.dtype\n",
    "            or (self.training and self._cos_cached.is_inference())\n",
    "        ):\n",
    "            self._update_cos_sin_cache(qkv.shape[1] + seqlen_offset, device=qkv.device, dtype=qkv.dtype)\n",
    "            print(\"case 1\")\n",
    "        else:\n",
    "            print(\"case 2\")\n",
    "        if kv is None:\n",
    "            return _apply_rotary_emb_qkv(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "        else:\n",
    "            # print(\"qkv:\", qkv.shape)\n",
    "            # print(\"_cos_cached:\", self._cos_cached[seqlen_offset:].shape)\n",
    "            # print(\"_sin_cached:\", self._sin_cached[seqlen_offset:].shape)\n",
    "            q = _apply_rotary_emb(\n",
    "                qkv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "            kv = _apply_rotary_emb_kv(\n",
    "                kv,\n",
    "                self._cos_cached[seqlen_offset:],\n",
    "                self._sin_cached[seqlen_offset:],\n",
    "            )\n",
    "\n",
    "            return q, kv\n",
    "\n",
    "\n",
    "dim = 5\n",
    "base = 5000\n",
    "scale_base = None\n",
    "pos_idx_in_fp32 = False\n",
    "max_position_embeddings = 2048\n",
    "device = 'cpu'\n",
    "\n",
    "rot_emb = RotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings, device)\n",
    "tf_rot_emb = MyRotaryEmbedding(dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings)\n",
    "\n",
    "# test with kv being something, and being None\n",
    "\n",
    "# it appears that if kv will be none, then qkv must be 4d rather than 5d\n",
    "\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2]) \n",
    "qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "kv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "seqlen_offset = 4\n",
    "print(rot_emb(qkv, kv, seqlen_offset))\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "kv = tf.constant(kv.numpy())\n",
    "print(tf_rot_emb(qkv, kv, seqlen_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3835, -0.6626,  0.1680, -0.0848,  1.0117],\n",
      "        [ 0.6447, -1.1747, -0.1887, -0.4882,  1.2400],\n",
      "        [-0.1074, -0.4478,  1.0507,  0.5867,  1.1698],\n",
      "        [-0.1609,  1.1470,  0.5112,  0.6772, -0.2061],\n",
      "        [ 0.3013,  2.0532, -0.5900,  0.2114, -1.1886],\n",
      "        [ 0.6394,  0.2294, -0.4412, -0.2011,  0.2540],\n",
      "        [-0.2194,  0.0963,  0.4387,  0.2138,  0.2543]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tf.Tensor(\n",
      "[[-0.02260414  0.36220938  0.2733536   0.3355272   1.10681   ]\n",
      " [ 0.3904422   0.19216077  0.4791666  -0.08113319  1.4271572 ]\n",
      " [-1.0028046   0.87150764 -0.10148591  1.4295963   1.1488026 ]\n",
      " [-0.03813796  0.17467475 -0.16894978  0.4353561  -0.36198846]\n",
      " [ 1.0383843  -0.5042809   0.16638803 -0.94916064 -1.3186995 ]\n",
      " [ 0.30429593  0.09387419  0.45344672 -0.3244572   0.33560476]\n",
      " [ 1.1846408  -0.38991195 -0.19961894 -0.15549368  0.15859848]], shape=(7, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# testing Linear Layer\n",
    "layer = torch.nn.Linear(3, 5)\n",
    "input = torch.randn([7,3])\n",
    "print(layer(input))\n",
    "weights = tf.cast(torch.detach(layer.weight).numpy(), dtype=tf.float32)\n",
    "bias = tf.cast(torch.detach(layer.bias).numpy(), dtype=tf.float32)\n",
    "linear = bert.Dense_v2(3, 5, weights, bias=bias)\n",
    "input = tf.cast(input.numpy(), dtype=tf.float32)\n",
    "print(linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NewGELU(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name)\n",
    "    def __call__(self, input):\n",
    "        return 0.5 * input * (1.0 + tf.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * tf.pow(input, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP (need to fill in the weight, bias, and activation) (works)\n",
    "\n",
    "class MyMLP(tf.Module):\n",
    "    def __init__(self, n_inner:int, n_embd:int, name=None): # manually written in act-fn()\n",
    "        super().__init__(name)\n",
    "        self.fc1 = bert.Dense_v2(n_embd, n_inner, weights=None, bias=None)\n",
    "        self.fc2 = bert.Dense_v2(n_inner, n_embd, weights=None, bias=None)\n",
    "        self.act = NewGELU\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class NewGELUActivation(nn.Module): # grabbed from hugging face\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "    \n",
    "\n",
    "# input = torch.randn([2,3])\n",
    "# print(input)\n",
    "# layer = NewGELUActivation()\n",
    "# print(layer(input))\n",
    "# input = tf.constant(input.numpy())\n",
    "# layer = NewGELU()\n",
    "# print(layer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Looks good\n",
    "\n",
    "# not sure what key_padding_mask dims are supposed to be in reality\n",
    "\n",
    "def mask_fill(matrix, mask, value):\n",
    "    inv_mask = (-1 * mask) + 1\n",
    "    return inv_mask * matrix + (value * mask)\n",
    "\n",
    "class MySelfAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale:float, name=None):\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, qkv, key_padding_mask=None):\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = tf.unstack(qkv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        softmax_scale = self.softmax_scale\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        # print(scores)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen), -10000.0)\n",
    "            padding_mask = tf.cast(padding_mask, dtype=scores.dtype)\n",
    "            # print(padding_mask)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 3.0)\n",
    "            # print(padding_mask)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # print(scores)\n",
    "        # i am assuming this to be causal\n",
    "        causal_mask = tf.experimental.numpy.triu(tf.fill((seqlen, seqlen), -10000.0), 1) # might need to be replaced\n",
    "        scores = scores + tf.cast(causal_mask, dtype=scores.dtype)\n",
    "        # print(scores)\n",
    "        attention = tf.cast(tf.nn.softmax(scores, axis=-1), dtype=v.dtype)\n",
    "        # print(attention)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        causal: bool = True,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        attention_dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.drop = nn.Dropout(attention_dropout)\n",
    "    @torch.autocast(\"cpu\", enabled=False)\n",
    "    def forward(\n",
    "        self,\n",
    "        qkv: torch.FloatTensor,\n",
    "        causal: bool = None,\n",
    "        key_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.FloatTensor:\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.to(torch.float32)\n",
    "        k = k.to(torch.float32)\n",
    "        causal = self.causal if causal is None else causal\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        # using float16, which might lead to overflow\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        # print(scores)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype, device=scores.device)\n",
    "            # print(padding_mask)\n",
    "            padding_mask.masked_fill_(key_padding_mask, 3.0)\n",
    "            # print(padding_mask)\n",
    "            scores = scores + rearrange(padding_mask, \"b s -> b 1 1 s\")\n",
    "        # print(scores)\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "        # print(scores)\n",
    "        attention = torch.softmax(scores, dim=-1).to(v.dtype)\n",
    "        # print(attention)\n",
    "        attention = self.drop(attention)\n",
    "        output = torch.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "my_self_atten = MySelfAttention(softmax_scale=0.1)\n",
    "self_atten = SelfAttention(softmax_scale=0.1)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 17\n",
    "last_dim = 5\n",
    "# qkv = torch.randn([2,3,2, 8, last_dim*2])\n",
    "# qkv = torch.randn([2,3, 8, last_dim*2]) # making qkv\n",
    "qkv = torch.randn([batch_size, seq_len, 3, 5, 11])\n",
    "key_padding_mask = torch.zeros((batch_size, seq_len), dtype=torch.float32)\n",
    "key_padding_mask[0,0] += 1\n",
    "key_padding_mask[0,1] += 1\n",
    "key_padding_mask = key_padding_mask.to(bool)\n",
    "out = self_atten(qkv, causal=True, key_padding_mask=key_padding_mask)\n",
    "# print(out)\n",
    "print(\"-----\")\n",
    "qkv = tf.constant(qkv.numpy())\n",
    "key_padding_mask = tf.constant(key_padding_mask.to(torch.float).numpy())\n",
    "out = my_self_atten(qkv, key_padding_mask=key_padding_mask)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 20, 7])\n",
      "torch.Size([1, 11, 2, 20, 7])\n",
      "-----\n",
      "(1, 17, 20, 7)\n",
      "(1, 11, 2, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "# looks good\n",
    "\n",
    "class MyCrossAttention(tf.Module):\n",
    "    def __init__(self, softmax_scale, name=None): # assume to be causal\n",
    "        super().__init__(name)\n",
    "        self.softmax_scale = softmax_scale\n",
    "    def __call__(self, q, kv, key_padding_mask):\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        if kv.shape[3] != kv.shape[2]:\n",
    "            kv = tf.repeat(kv, repeats=q.shape[2] // kv.shape[3], axis=-2)\n",
    "        k, v = tf.unstack(kv, axis=2)\n",
    "        q = tf.cast(q, dtype=tf.float32)\n",
    "        k = tf.cast(k, dtype=tf.float32)\n",
    "        print(q.shape)\n",
    "        print(kv.shape)\n",
    "        scores = tf.einsum(\"bthd,bshd->bhts\", q, k * self.softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = tf.fill((batch_size, seqlen_k), -10000.0)\n",
    "            padding_mask = tf.cast(padding_mask, dtype=scores.dtype)\n",
    "            padding_mask = mask_fill(padding_mask, key_padding_mask, 0.0)\n",
    "            scores = scores + tf.expand_dims(tf.expand_dims(padding_mask, axis=1), axis=1)\n",
    "        # causal stuff\n",
    "        rows = tf.expand_dims(tf.range(seqlen_q, dtype=tf.int64), axis=1) # if this fails, do axis=-1\n",
    "        cols = tf.range(seqlen_k, dtype=tf.int64)\n",
    "        causal_mask = cols > rows + seqlen_k - seqlen_q\n",
    "        causal_mask = tf.cast(causal_mask, dtype=tf.float32)\n",
    "        scores = mask_fill(scores, causal_mask, -10000.0)\n",
    "        # end of causal stuff\n",
    "        attention = tf.nn.softmax(scores, axis=-1)\n",
    "        attention = tf.cast(attention, dtype=v.dtype)\n",
    "        output = tf.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, causal: bool = True, softmax_scale: Optional[float] = None,\n",
    "        attention_dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.drop = nn.Dropout(attention_dropout)\n",
    "    def forward(self, q: torch.FloatTensor, kv: torch.FloatTensor,causal: bool = None,\n",
    "        key_padding_mask: Optional[torch.BoolTensor] = None, **kwargs) -> torch.FloatTensor:\n",
    "        batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        seqlen_k = kv.shape[1]\n",
    "        if kv.shape[3] != q.shape[2]:\n",
    "            kv = repeat(kv, \"... hkv d -> ... (hkv g) d\", g=q.shape[2] // kv.shape[3])\n",
    "        k, v = kv.unbind(dim=2)\n",
    "        q = q.to(torch.float32)\n",
    "        k = k.to(torch.float32)\n",
    "        causal = self.causal if causal is None else causal\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        # Autocast is manually disabled to avoid `torch.einsum` performing the operation\n",
    "        # using float16, which might lead to overflow\n",
    "        print(q.shape)\n",
    "        print(kv.shape)\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full(\n",
    "                (batch_size, seqlen_k),\n",
    "                -10000.0,\n",
    "                dtype=scores.dtype,\n",
    "                device=scores.device,\n",
    "            )\n",
    "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
    "            scores = scores + rearrange(padding_mask, \"b s -> b 1 1 s\")\n",
    "        if causal:\n",
    "            rows = rearrange(torch.arange(seqlen_q, device=q.device, dtype=torch.long), \"s -> s 1\")\n",
    "            cols = torch.arange(seqlen_k, device=k.device, dtype=torch.long)\n",
    "            causal_mask = cols > rows + seqlen_k - seqlen_q\n",
    "            scores = scores.masked_fill(causal_mask, -10000.0)\n",
    "        attention = torch.softmax(scores, dim=-1).to(v.dtype)\n",
    "        attention = self.drop(attention)\n",
    "        output = torch.einsum(\"bhts,bshd->bthd\", attention, v)\n",
    "        return output\n",
    "\n",
    "cross_atten = CrossAttention(causal=True, softmax_scale=0.1)\n",
    "my_cross_atten = MyCrossAttention(softmax_scale=0.1)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 17\n",
    "seqlen_k = 11\n",
    "two = 2 # must be two\n",
    "q = torch.randn(batch_size, seq_len, 20, 7)\n",
    "kv = torch.randn(batch_size, seqlen_k, two, 5, 7)\n",
    "key_padding_mask = torch.zeros((batch_size, seqlen_k), dtype=torch.float32)\n",
    "key_padding_mask[0,0] += 1\n",
    "key_padding_mask[0,1] += 1\n",
    "key_padding_mask = key_padding_mask.to(bool)\n",
    "out = cross_atten(q, kv, causal=True, key_padding_mask=key_padding_mask)\n",
    "\n",
    "# print(out)\n",
    "print(\"-----\")\n",
    "q = tf.constant(q.numpy())\n",
    "kv = tf.constant(kv.numpy())\n",
    "key_padding_mask = tf.constant(key_padding_mask.to(torch.float).numpy())\n",
    "out = my_cross_atten(q, kv, key_padding_mask)\n",
    "# print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MY_update_kv_cache(kv, inference_params, layer_idx:int):\n",
    "    num_heads, head_dim = kv.shape[-2:]\n",
    "    if layer_idx not in inference_params.key_value_memory_dict:\n",
    "        # during the UDO desgin process, could use uninitalized memory rather than zeros\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.zeros(\n",
    "            [\n",
    "                inference_params.max_batch_size,\n",
    "                inference_params.max_seqlen,\n",
    "                2,\n",
    "                num_heads,\n",
    "                head_dim\n",
    "            ],\n",
    "            dtype=kv.dtype\n",
    "        )\n",
    "    batch_start = inference_params.batch_size_offset\n",
    "    batch_end = batch_start + kv.shape[0]\n",
    "    sequence_start = inference_params.seqlen_offset\n",
    "    sequence_end = sequence_start + kv.shape[1]\n",
    "    # When the current sequence length is equal to or larger than the maximum sequence length,\n",
    "    # we need to concatenate the current `kv` with the cached `kv` to expand its length\n",
    "    if sequence_end >= inference_params.max_seqlen:\n",
    "        # the line below might fail due to the tuple\n",
    "        inference_params.key_value_memory_dict[layer_idx] = tf.concat((inference_params.key_value_memory_dict[layer_idx], kv), axis=1)\n",
    "    inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, sequence_start:sequence_end, ...] = kv\n",
    "    kv = inference_params.key_value_memory_dict[layer_idx][batch_start:batch_end, :sequence_end, ...]\n",
    "    return kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not my code\n",
    "\n",
    "pad_input, unpad_input = None, None\n",
    "FlashRotaryEmbedding = None\n",
    "FlashSelfAttention, FlashCrossAttention = None, None\n",
    "FusedDense = None\n",
    "\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \"\"\"Multi-head attention layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: PretrainedConfig,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "        device: Optional[str] = None,\n",
    "        rotary_dim: Optional[int] = None,\n",
    "        rotary_base: float = 10000.0,\n",
    "        rotary_scale_base: Optional[float] = None,\n",
    "        n_head: Optional[int] = None,\n",
    "        n_head_kv: Optional[int] = None,\n",
    "        head_dim: Optional[int] = None,\n",
    "        bias: bool = True,\n",
    "        causal: bool = True,\n",
    "        softmax_scale: Optional[float] = None,\n",
    "        layer_idx: Optional[int] = None,\n",
    "        return_residual: bool = False,\n",
    "        checkpointing: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Rotary embedding\n",
    "        self.rotary_dim = rotary_dim if rotary_dim is not None else getattr(config, \"rotary_dim\", 0)\n",
    "        if self.rotary_dim > 0:\n",
    "            rotary_cls = FlashRotaryEmbedding if config.flash_rotary else RotaryEmbedding\n",
    "            if rotary_cls is None:\n",
    "                rotary_cls = RotaryEmbedding\n",
    "\n",
    "            rotary_kwargs = {}\n",
    "            if rotary_cls is RotaryEmbedding:\n",
    "                rotary_kwargs[\"max_position_embeddings\"] = config.n_positions\n",
    "            print(\"rotary cls:\", rotary_cls)\n",
    "            self.rotary_emb = rotary_cls(\n",
    "                self.rotary_dim,\n",
    "                rotary_base,\n",
    "                rotary_scale_base,\n",
    "                 True,\n",
    "                43,\n",
    "                device\n",
    "                # **rotary_kwargs\n",
    "            )\n",
    "\n",
    "        # MLP\n",
    "        self.n_head, self.n_head_kv, self.head_dim = _find_mha_dims(\n",
    "            config, n_head=n_head, n_head_kv=n_head_kv, head_dim=head_dim\n",
    "        )\n",
    "        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n",
    "        hidden_size = config.n_embd\n",
    "\n",
    "        linear_cls = FusedDense if config.fused_dense else nn.Linear\n",
    "        if linear_cls is None:\n",
    "            linear_cls = nn.Linear\n",
    "\n",
    "        self.Wqkv = linear_cls(hidden_size, op_size, bias=bias, device=device, dtype=dtype)\n",
    "        self.out_proj = linear_cls(hidden_size, hidden_size, bias=bias, device=device, dtype=dtype)\n",
    "\n",
    "        # Attention\n",
    "        attn_cls = FlashSelfAttention if config.flash_attn else SelfAttention\n",
    "        if attn_cls is None:\n",
    "            attn_cls = SelfAttention\n",
    "\n",
    "        cross_attn_cls = FlashCrossAttention if config.flash_attn else CrossAttention\n",
    "        if cross_attn_cls is None:\n",
    "            cross_attn_cls = CrossAttention\n",
    "\n",
    "        self.inner_attn = attn_cls(\n",
    "            causal=causal,\n",
    "            softmax_scale=softmax_scale,\n",
    "            attention_dropout=config.attn_pdrop,\n",
    "        )\n",
    "        self.inner_cross_attn = cross_attn_cls(\n",
    "            causal=causal,\n",
    "            softmax_scale=softmax_scale,\n",
    "            attention_dropout=config.attn_pdrop,\n",
    "        )\n",
    "\n",
    "        self.flash_attn = config.flash_attn and attn_cls is FlashSelfAttention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.return_residual = return_residual\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "    def _forward_self_attn(\n",
    "        self, x: torch.FloatTensor, key_padding_mask: Optional[torch.BoolTensor]\n",
    "    ) -> torch.FloatTensor:\n",
    "        qkv = self.Wqkv(x)\n",
    "        qkv = rearrange(qkv, \"... (three h d) -> ... three h d\", three=3, d=self.head_dim)\n",
    "        if self.rotary_dim > 0:\n",
    "            qkv = self.rotary_emb(qkv)\n",
    "        if self.flash_attn:\n",
    "            batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "            cu_seqlens, max_seqlen = None, None\n",
    "            if key_padding_mask is not None:\n",
    "                # If `key_padding_mask` is supplied, we need to unpad the input and retrieve\n",
    "                # the `cu_seqlens` and `max_seqlen` to be used by `flash-attn`\n",
    "                qkv, indices, cu_seqlens, max_seqlen = unpad_input(qkv, key_padding_mask)\n",
    "            if self.checkpointing:\n",
    "                attn_output = torch.utils.checkpoint.checkpoint(\n",
    "                    self.inner_attn, qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen\n",
    "                )\n",
    "            else:\n",
    "                attn_output = self.inner_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen).to(qkv.device)\n",
    "            # If `key_padding_mask` is supplied, we need to pad the output back to the original shape\n",
    "            return pad_input(attn_output, indices, batch_size, seqlen) if key_padding_mask is not None else attn_output\n",
    "        if self.checkpointing:\n",
    "            return torch.utils.checkpoint.checkpoint(self.inner_attn, qkv, key_padding_mask=key_padding_mask)\n",
    "        return self.inner_attn(qkv, key_padding_mask=key_padding_mask)\n",
    "\n",
    "    def _forward_cross_attn(\n",
    "        self,\n",
    "        x: torch.FloatTensor,\n",
    "        past_key_values,\n",
    "        key_padding_mask: Optional[torch.BoolTensor],\n",
    "    ) -> torch.FloatTensor:\n",
    "        batch_size = x.shape[0]\n",
    "        qkv = self.Wqkv(x)\n",
    "        q = qkv[..., : self.n_head * self.head_dim]\n",
    "        q = rearrange(q, \"... (h d) -> ... h d\", d=self.head_dim)\n",
    "        kv = qkv[..., self.n_head * self.head_dim :]\n",
    "        kv = rearrange(kv, \"... (two hkv d) -> ... two hkv d\", two=2, d=self.head_dim)\n",
    "        seqlen_offset = past_key_values.seqlen_offset if past_key_values is not None else 0\n",
    "        causal = None if seqlen_offset == 0 else False\n",
    "        if self.rotary_dim > 0:\n",
    "            q, kv = self.rotary_emb(q, kv=kv, seqlen_offset=seqlen_offset)\n",
    "        if past_key_values is not None:\n",
    "            kv = _update_kv_cache(kv, past_key_values, self.layer_idx)\n",
    "        if self.flash_attn:\n",
    "            batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "            seqlen_k = kv.shape[1]\n",
    "            cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k = (\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "            if key_padding_mask is not None:\n",
    "                kv, _, cu_seqlens_k, max_seqlen_k = unpad_input(kv, key_padding_mask)\n",
    "                if seqlen_q == 1:\n",
    "                    key_padding_mask = torch.ones(batch_size, 1, device=q.device)\n",
    "                elif seqlen_q != seqlen_k:\n",
    "                    key_padding_mask = key_padding_mask[:, -seqlen_q:]\n",
    "                q, indices_q, cu_seqlens_q, max_seqlen_q = unpad_input(q, key_padding_mask)\n",
    "            if self.checkpointing:\n",
    "                attn_output = torch.utils.checkpoint.checkpoint(\n",
    "                    self.inner_cross_attn,\n",
    "                    q,\n",
    "                    kv,\n",
    "                    causal=causal,\n",
    "                    cu_seqlens=cu_seqlens_q,\n",
    "                    max_seqlen=max_seqlen_q,\n",
    "                    cu_seqlens_k=cu_seqlens_k,\n",
    "                    max_seqlen_k=max_seqlen_k,\n",
    "                )\n",
    "            else:\n",
    "                attn_output = self.inner_cross_attn(\n",
    "                    q,\n",
    "                    kv,\n",
    "                    causal=causal,\n",
    "                    cu_seqlens=cu_seqlens_q,\n",
    "                    max_seqlen=max_seqlen_q,\n",
    "                    cu_seqlens_k=cu_seqlens_k,\n",
    "                    max_seqlen_k=max_seqlen_k,\n",
    "                )\n",
    "            return (\n",
    "                pad_input(attn_output, indices_q, batch_size, max_seqlen_q)\n",
    "                if key_padding_mask is not None\n",
    "                else attn_output\n",
    "            )\n",
    "        if self.checkpointing:\n",
    "            return torch.utils.checkpoint.checkpoint(\n",
    "                self.inner_cross_attn,\n",
    "                q,\n",
    "                kv,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "                causal=causal,\n",
    "            )\n",
    "        return self.inner_cross_attn(q, kv, key_padding_mask=key_padding_mask, causal=causal)\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.FloatTensor,\n",
    "        past_key_values=None,\n",
    "        attention_mask: Optional[Union[torch.LongTensor, torch.BoolTensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.bool()\n",
    "        else:\n",
    "            attention_mask = None\n",
    "        # MHA\n",
    "        if self.n_head == self.n_head_kv:\n",
    "            if past_key_values is None:\n",
    "                # If `past_key_values` are not supplied, we run self-attention\n",
    "                attn_output = self._forward_self_attn(x, attention_mask)\n",
    "            else:\n",
    "                # If `past_key_values` are supplied, it means that we might have cached values and\n",
    "                # could take advantage of cross-attention\n",
    "                attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        # MQA / GQA\n",
    "        else:\n",
    "            # Regardless of `past_key_values` being supplied or not, it always use cross-attention\n",
    "            # because `q` and `kv` lengths might be different\n",
    "            attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        output = rearrange(attn_output, \"... h d -> ... (h d)\")\n",
    "        output = self.out_proj(output)\n",
    "        return output if not self.return_residual else (output, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotary cls: <class '__main__.RotaryEmbedding'>\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 177\u001b[0m\n\u001b[1;32m    169\u001b[0m my_mha \u001b[38;5;241m=\u001b[39m MyMHA(rotary_dim, rotary_base, rotary_scale_base,\n\u001b[1;32m    170\u001b[0m                  n_head, n_head_kv, head_dim, bias, softmax_scale,\n\u001b[1;32m    171\u001b[0m                  layer_idx, return_residual,\n\u001b[1;32m    172\u001b[0m                  flash_rot_emb, n_positions, n_embd,\n\u001b[1;32m    173\u001b[0m                  fused_dense,\n\u001b[1;32m    174\u001b[0m                  flash_attn, weights)\n\u001b[1;32m    176\u001b[0m config \u001b[38;5;241m=\u001b[39m PhiConfig()\n\u001b[0;32m--> 177\u001b[0m mha \u001b[38;5;241m=\u001b[39m \u001b[43mMHA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_scale_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m          \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size, seqlen_k), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    181\u001b[0m key_padding_mask[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 46\u001b[0m, in \u001b[0;36mMHA.__init__\u001b[0;34m(self, config, dtype, device, rotary_dim, rotary_base, rotary_scale_base, n_head, n_head_kv, head_dim, bias, causal, softmax_scale, layer_idx, return_residual, checkpointing)\u001b[0m\n\u001b[1;32m     44\u001b[0m         rotary_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_position_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mn_positions\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary cls:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rotary_cls)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m \u001b[43mrotary_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_scale_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m43\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# **rotary_kwargs\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# MLP\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head_kv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m _find_mha_dims(\n\u001b[1;32m     58\u001b[0m     config, n_head\u001b[38;5;241m=\u001b[39mn_head, n_head_kv\u001b[38;5;241m=\u001b[39mn_head_kv, head_dim\u001b[38;5;241m=\u001b[39mhead_dim\n\u001b[1;32m     59\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 102\u001b[0m, in \u001b[0;36mRotaryEmbedding.__init__\u001b[0;34m(self, dim, base, scale_base, pos_idx_in_fp32, max_position_embeddings, device, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(base)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for now\n",
    "FlashRotaryEmbedding = None\n",
    "FusedDense = None\n",
    "FlashSelfAttention = None\n",
    "FlashCrossAttention = None\n",
    "\n",
    "\n",
    "class MyMHA(tf.Module):\n",
    "    def __init__(self, rotary_dim, rotary_base, rotary_scale_base,\n",
    "                 n_head, n_head_kv, head_dim, bias:bool, softmax_scale:float,\n",
    "                 layer_idx:int, return_residual:bool,\n",
    "                 flash_rot_emb:bool, n_positions, n_embd,\n",
    "                 fused_dense:bool,\n",
    "                 flash_attn:bool, weights,\n",
    "                 name=None):\n",
    "        super().__init__(name)\n",
    "        # Rotary embedding\n",
    "        self.rotary_dim = rotary_dim\n",
    "        if self.rotary_dim > 0:\n",
    "            rotary_cls = FlashRotaryEmbedding if flash_rot_emb else MyRotaryEmbedding\n",
    "            rotary_kwargs = {}\n",
    "            if rotary_cls is RotaryEmbedding:\n",
    "                rotary_kwargs[\"max_position_embeddings\"] = n_positions\n",
    "            self.rotary_emb =  rotary_cls(\n",
    "                self.rotary_dim,\n",
    "                base=rotary_base,\n",
    "                scale_base=rotary_scale_base,\n",
    "                **rotary_kwargs\n",
    "            )\n",
    "        # MLP\n",
    "        self.n_head = n_head\n",
    "        self.n_head_kv = n_head_kv\n",
    "        self.head_dim = head_dim\n",
    "        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n",
    "        hidden_size = n_embd\n",
    "        linear_cls = FusedDense if fused_dense else bert.Dense_v2\n",
    "        self.Wqkv = linear_cls(hidden_size, op_size, weights=weights['Wqkv_weights'], bias=None)\n",
    "        self.out_proj = linear_cls(hidden_size, hidden_size, weights=weights['out_proj_weights'], bias=None)\n",
    "        # Attention\n",
    "        attn_cls = FlashSelfAttention if flash_attn else MySelfAttention\n",
    "        cross_attn_cls = FlashCrossAttention if flash_attn else MyCrossAttention\n",
    "        self.inner_attn = attn_cls(softmax_scale=softmax_scale)\n",
    "        self.inner_cross_attn = cross_attn_cls(softmax_scale=softmax_scale)\n",
    "        self.flash_attn = flash_attn and attn_cls is FlashSelfAttention\n",
    "        self.layer_idx = layer_idx\n",
    "        self.return_residual = return_residual\n",
    "\n",
    "    def _forward_self_attn(self, x, key_padding_mask):\n",
    "        qkv = self.Wqkv(x)\n",
    "        h = qkv.shape[-1] // (3 * self.head_dim)\n",
    "        assert(h * 3 * self.head_dim == qkv.shape[-1])\n",
    "        qkv = tf.reshape(qkv, qkv.shape[:-1] + [3, h, self.head_dim])\n",
    "        if self.rotary_dim > 0:\n",
    "            qkv = self.rotary_emb(qkv)\n",
    "        # if self.flash_attn:\n",
    "        #     batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        #     cu_seqlens, max_seqlen = None, None\n",
    "        #     if key_padding_mask is not None:\n",
    "        #         # If `key_padding_mask` is supplied, we need to unpad the input and retrieve\n",
    "        #         # the `cu_seqlens` and `max_seqlen` to be used by `flash-attn`\n",
    "        #         qkv, indices, cu_seqlens, max_seqlen = unpad_input(qkv, key_padding_mask)\n",
    "        #     attn_output = self.inner_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n",
    "        #     # If `key_padding_mask` is supplied, we need to pad the output back to the original shape\n",
    "        #     return pad_input(attn_output, indices, batch_size, seqlen)\n",
    "        return self.inner_attn(qkv, key_padding_mask=key_padding_mask)\n",
    "    \n",
    "    def _forward_cross_attn(self, x, past_key_values, key_padding_mask):\n",
    "        batch_size = x.shape[0]\n",
    "        qkv = self.Wqkv(x)\n",
    "        q = qkv[..., : self.n_head * self.head_dim]\n",
    "        h = q.shape[-1] // self.head_dim\n",
    "        assert(h * self.head_dim == q.shape[-1])\n",
    "        q = tf.reshape(q, q.shape[:-1] + [h, self.head_dim])\n",
    "        kv = qkv[..., self.n_head * self.head_dim :]\n",
    "        hkv = kv.shape[-1] // (2 * self.head_dim)\n",
    "        assert(hkv * 2 * self.head_dim == kv.shape[-1])\n",
    "        kv = tf.reshape(kv, kv.shape[:-1] + [2, hkv, self.head_dim])\n",
    "        seqlen_offset = past_key_values.seqlen_offset if past_key_values is not None else 0\n",
    "        causal = None if seqlen_offset == 0 else False\n",
    "        if self.rotary_dim > 0:\n",
    "            q, kv = self.rotary_emb(q, kv=kv, seqlen_offset=seqlen_offset)\n",
    "        if past_key_values is not None:\n",
    "            kv = MY_update_kv_cache(kv, past_key_values, self.layer_idx)\n",
    "        # assert(self.flash_attn is not None)\n",
    "        # if self.flash_attn:\n",
    "        #     batch_size, seqlen_q = q.shape[0], q.shape[1]\n",
    "        #     seqlen_k = kv.shape[1]\n",
    "        #     cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k = (\n",
    "        #         None,\n",
    "        #         None,\n",
    "        #         None,\n",
    "        #         None,\n",
    "        #     )\n",
    "        #     if key_padding_mask is not None:\n",
    "        #         kv, _, cu_seqlens_k, max_seqlen_k = unpad_input(kv, key_padding_mask)\n",
    "        #         if seqlen_q == 1:\n",
    "        #             key_padding_mask = tf.ones((batch_size, 1))\n",
    "        #         elif seqlen_q != seqlen_k:\n",
    "        #             key_padding_mask = key_padding_mask[:, -seqlen_q:]\n",
    "        #         q, indices_q, cu_seqlens_q, max_seqlen_q = unpad_input(q, key_padding_mask)\n",
    "        #     attn_output = self.inner_cross_attn(\n",
    "        #         q,\n",
    "        #         kv,\n",
    "        #         causal=causal,\n",
    "        #         cu_seqlens=cu_seqlens_q,\n",
    "        #         max_seqlen=max_seqlen_q,\n",
    "        #         cu_seqlens_k=cu_seqlens_k,\n",
    "        #         max_seqlen_k=max_seqlen_k,\n",
    "        #     )\n",
    "        #     return (\n",
    "        #     pad_input(attn_output, indices_q, batch_size, max_seqlen_q)\n",
    "        #     if key_padding_mask is not None\n",
    "        #     else attn_output\n",
    "        #     )\n",
    "        return self.inner_cross_attn(q, kv, key_padding_mask=key_padding_mask, causal=causal)\n",
    "    def __call__(self, x, past_key_values, attention_mask):\n",
    "        # assert(attention_mask is not None)\n",
    "        attention_mask = tf.cast(attention_mask, dtype=tf.bool)\n",
    "        # MHA\n",
    "        if self.n_head == self.n_head_kv:\n",
    "            if past_key_values is None:\n",
    "                # If `past_key_values` are not supplied, we run self-attention\n",
    "                attn_output = self._forward_self_attn(x, attention_mask)\n",
    "            else:\n",
    "                # If `past_key_values` are supplied, it means that we might have cached values and\n",
    "                # could take advantage of cross-attention\n",
    "                attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        # MQA / GQA\n",
    "        else:\n",
    "            # Regardless of `past_key_values` being supplied or not, it always use cross-attention\n",
    "            # because `q` and `kv` lengths might be different\n",
    "            attn_output = self._forward_cross_attn(x, past_key_values, attention_mask)\n",
    "        output = tf.reshape(attn_output, attn_output.shape[:-2] + [attn_output.shape[-1] * attn_output.shape[-2]])\n",
    "        output = self.out_proj(output)\n",
    "        return output if not self.return_residual else (output, x)\n",
    "\n",
    "rotary_dim = 5\n",
    "rotary_base = 10000.0\n",
    "rotary_scale_base = 3.0\n",
    "n_head = 4\n",
    "n_head_kv = 5\n",
    "head_dim = 6\n",
    "bias = False\n",
    "softmax_scale = 2.3\n",
    "layer_idx = 2\n",
    "return_residual = False\n",
    "flash_rot_emb = False\n",
    "n_positions = 30 # idk what this should be\n",
    "n_embd = 20\n",
    "fused_dense = False\n",
    "flash_attn = False\n",
    "\n",
    "op_size = head_dim * (n_head + 2 * n_head_kv)\n",
    "\n",
    "# for their class\n",
    "class PhiConfig():\n",
    "    def __init__(self):\n",
    "        self.flash_rotary = False\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.fused_dense = False\n",
    "        self.flash_attn= False\n",
    "        self.attn_pdrop = 0\n",
    "\n",
    "weights = {}\n",
    "weights['Wqkv_weights'] = torch.randn((n_embd, op_size))\n",
    "weights['out_proj_weights'] = torch.randn((n_embd, n_embd))\n",
    "\n",
    "my_mha = MyMHA(rotary_dim, rotary_base, rotary_scale_base,\n",
    "                 n_head, n_head_kv, head_dim, bias, softmax_scale,\n",
    "                 layer_idx, return_residual,\n",
    "                 flash_rot_emb, n_positions, n_embd,\n",
    "                 fused_dense,\n",
    "                 flash_attn, weights)\n",
    "\n",
    "config = PhiConfig()\n",
    "mha = MHA(config, torch.float32, 'cpu', rotary_dim, rotary_base, rotary_scale_base, \n",
    "          n_head, n_head_kv, head_dim, False, True, softmax_scale, layer_idx, False, False)\n",
    "\n",
    "key_padding_mask = torch.zeros((batch_size, seqlen_k), dtype=torch.float32)\n",
    "key_padding_mask[0,0] += 1\n",
    "key_padding_mask[0,1] += 1\n",
    "key_padding_mask = key_padding_mask.to(bool)\n",
    "# key_padding_mask = tf.constant(key_padding_mask.numpy())\n",
    "\n",
    "input = torch.randn([3,7,11, 13, n_embd])\n",
    "\n",
    "\n",
    "\n",
    "# my_mha(x=input, past_key_values=None, attention_mask=key_padding_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyParallelBlock(tf.Module):\n",
    "    def __init__(self, block_idx, name=None):\n",
    "        super().__init__(name)\n",
    "        self.ln = bert.LayerNorm(weights=None, biases=None, eps=None)\n",
    "        self.block_idx = block_idx\n",
    "        self.mixer = MyMHA()\n",
    "        self.mlp = MyMLP()\n",
    "    \n",
    "    def __call__(self, hidden_states, past_key_values, attention_mask):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        attn_outputs = self.mixer(\n",
    "            hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        if isinstance(attn_outputs, tuple):\n",
    "            attn_outputs = attn_outputs[0]\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCausalLMHead(tf.Module):\n",
    "    def __init__(self, n_embd, layer_norm_eps, vocab_size, name=None):\n",
    "        super().__init__(name)\n",
    "        self.ln = nn.LayerNorm(n_embd, eps=layer_norm_eps)\n",
    "        self.linear = bert.Dense_v2(n_embd, vocab_size)\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.ln(hidden_states)\n",
    "        logits = tf.cast(self.linear(hidden_states), dtype=tf.float32)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyPhiPreTrainedModel(tf.Module):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
